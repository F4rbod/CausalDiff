{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class PropensityNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(PropensityNN, self).__init__()\n",
    "        # self.fc1 = nn.Linear(input_size, 128)\n",
    "        # self.fc2 = nn.Linear(128, 64)\n",
    "        # self.fc3 = nn.Linear(64, output_size)\n",
    "        self.simple_linear = nn.Linear(input_size, output_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = torch.relu(self.fc1(x))\n",
    "        # x = torch.relu(self.fc2(x))\n",
    "        # x = self.fc3(x)\n",
    "        x = self.simple_linear(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MSMPropensityTreatment(CausalDiff):\n",
    "\n",
    "    model_type = 'propensity_treatment'\n",
    "\n",
    "    def __init__(self,\n",
    "                 args: DictConfig,\n",
    "                 dataset_collection: Union[RealDatasetCollection,\n",
    "                                           SyntheticDatasetCollection] = None,\n",
    "                 autoregressive: bool = None, has_vitals: bool = None, **kwargs):\n",
    "        super().__init__(args, dataset_collection, autoregressive, has_vitals)\n",
    "\n",
    "        self.input_size = self.dim_treatments\n",
    "        logger.info(f'Input size of {self.model_type}: {self.input_size}')\n",
    "        self.output_size = self.dim_treatments\n",
    "\n",
    "        self.propensity_treatment = PropensityNN(\n",
    "            self.input_size, self.output_size)\n",
    "        self.optimizer = optim.Adam(\n",
    "            self.propensity_treatment.parameters(), lr=0.001)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.save_hyperparameters(args)\n",
    "\n",
    "    def get_inputs(self, dataset: Dataset) -> np.array:\n",
    "        active_entries = dataset.data['active_entries']\n",
    "        prev_treatments = dataset.data['prev_treatments']\n",
    "        inputs = (prev_treatments * active_entries).sum(1)\n",
    "        return inputs\n",
    "\n",
    "    def fit(self):\n",
    "        self.prepare_data()\n",
    "        train_f = self.get_exploded_dataset(\n",
    "            self.dataset_collection.train_f, min_length=self.lag_features)\n",
    "        active_entries = train_f.data['active_entries']\n",
    "        last_entries = active_entries - \\\n",
    "            np.concatenate([active_entries[:, 1:, :], np.zeros(\n",
    "                (active_entries.shape[0], 1, 1))], axis=1)\n",
    "\n",
    "        # Inputs\n",
    "        inputs = self.get_inputs(train_f)\n",
    "\n",
    "        # Outputs\n",
    "        current_treatments = train_f.data['current_treatments']\n",
    "        outputs = (current_treatments * last_entries).sum(1)\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        inputs = torch.tensor(inputs, dtype=torch.float32)\n",
    "        outputs = torch.tensor(outputs, dtype=torch.float32)\n",
    "\n",
    "        # Create DataLoader\n",
    "        dataset = TensorDataset(inputs, outputs)\n",
    "        dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "        # Training loop\n",
    "        # Training loop\n",
    "        for epoch in range(args.exp.max_epochs):\n",
    "            epoch_loss = 0.0\n",
    "            for batch_inputs, batch_outputs in dataloader:\n",
    "                self.optimizer.zero_grad()\n",
    "                predictions = self.propensity_treatment(batch_inputs)\n",
    "                loss = self.criterion(\n",
    "                    predictions, batch_outputs)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "            epoch_loss /= len(dataloader)\n",
    "            print(\n",
    "                f\"Epoch {epoch+1}/{args.exp.max_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "            # also show accuracy\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            with torch.no_grad():\n",
    "                for batch_inputs, batch_outputs in dataloader:\n",
    "                    outputs = self.propensity_treatment(batch_inputs)\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    total += batch_outputs.size(0)\n",
    "                    # Convert one-hot targets to indices:\n",
    "                    target_labels = batch_outputs.argmax(dim=1)\n",
    "                    correct += (predicted == target_labels).sum().item()\n",
    "            print(f\"Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "\n",
    "class MSMPropensityHistory(CausalDiff):\n",
    "\n",
    "    model_type = 'propensity_history'\n",
    "\n",
    "    def __init__(self,\n",
    "                 args: DictConfig,\n",
    "                 dataset_collection: Union[RealDatasetCollection,\n",
    "                                           SyntheticDatasetCollection] = None,\n",
    "                 autoregressive: bool = None, has_vitals: bool = None, **kwargs):\n",
    "        super().__init__(args, dataset_collection, autoregressive, has_vitals)\n",
    "\n",
    "        self.input_size = self.dim_treatments + \\\n",
    "            self.dim_static_features + self.lag_features\n",
    "        self.input_size += self.dim_vitals if self.has_vitals else 0\n",
    "        self.input_size += self.dim_outcome if self.autoregressive else 0\n",
    "\n",
    "        logger.info(f'Input size of {self.model_type}: {self.input_size}')\n",
    "        self.output_size = self.dim_treatments\n",
    "\n",
    "        self.propensity_history = PropensityNN(\n",
    "            self.input_size, self.output_size)\n",
    "        self.optimizer = optim.Adam(\n",
    "            self.propensity_history.parameters(), lr=0.001)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.save_hyperparameters(args)\n",
    "\n",
    "    def get_inputs(self, dataset: Dataset, projection_horizon=0) -> np.array:\n",
    "        active_entries = dataset.data['active_entries']\n",
    "        lagged_entries = active_entries - \\\n",
    "            np.concatenate([active_entries[:, self.lag_features + 1:, :],\n",
    "                            np.zeros((active_entries.shape[0], self.lag_features + 1, 1))], axis=1)\n",
    "        if projection_horizon > 0:\n",
    "            lagged_entries = np.concatenate([lagged_entries[:, projection_horizon:, :],\n",
    "                                             np.zeros((active_entries.shape[0], projection_horizon, 1))], axis=1)\n",
    "\n",
    "        active_entries_before_projection = np.concatenate([active_entries[:, projection_horizon:, :],\n",
    "                                                          np.zeros((active_entries.shape[0], projection_horizon, 1))], axis=1)\n",
    "\n",
    "        prev_treatments = dataset.data['prev_treatments']\n",
    "        inputs = [(prev_treatments * active_entries_before_projection).sum(1)]\n",
    "        if self.has_vitals:\n",
    "            vitals = dataset.data['vitals']\n",
    "            inputs.append(vitals[np.repeat(lagged_entries, self.dim_vitals, 2) == 1.0].reshape(vitals.shape[0],\n",
    "                                                                                               (self.lag_features + 1) *\n",
    "                                                                                               self.dim_vitals))\n",
    "        if self.autoregressive:\n",
    "            prev_outputs = dataset.data['prev_outputs']\n",
    "            inputs.append(prev_outputs[np.repeat(lagged_entries, self.dim_outcome, 2) == 1.0].reshape(prev_outputs.shape[0],\n",
    "                                                                                                      (self.lag_features + 1) *\n",
    "                                                                                                      self.dim_outcome))\n",
    "        static_features = dataset.data['static_features']\n",
    "        inputs.append(static_features)\n",
    "        return np.concatenate(inputs, axis=1)\n",
    "\n",
    "    def fit(self):\n",
    "        self.prepare_data()\n",
    "        train_f = self.get_exploded_dataset(\n",
    "            self.dataset_collection.train_f, min_length=self.lag_features)\n",
    "        active_entries = train_f.data['active_entries']\n",
    "        last_entries = active_entries - \\\n",
    "            np.concatenate([active_entries[:, 1:, :], np.zeros(\n",
    "                (active_entries.shape[0], 1, 1))], axis=1)\n",
    "\n",
    "        # Inputs\n",
    "        inputs = self.get_inputs(train_f)\n",
    "\n",
    "        # Outputs\n",
    "        current_treatments = train_f.data['current_treatments']\n",
    "        outputs = (current_treatments * last_entries).sum(1)\n",
    "        # Convert to PyTorch tensors\n",
    "        inputs = torch.tensor(inputs, dtype=torch.float32)\n",
    "        outputs = torch.tensor(outputs, dtype=torch.float32)\n",
    "\n",
    "        # Create DataLoader\n",
    "        dataset = TensorDataset(inputs, outputs)\n",
    "        dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(args.exp.max_epochs):\n",
    "            epoch_loss = 0.0\n",
    "            for batch_inputs, batch_outputs in dataloader:\n",
    "                self.optimizer.zero_grad()\n",
    "                predictions = self.propensity_history(batch_inputs)\n",
    "                loss = self.criterion(predictions, batch_outputs)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "            epoch_loss /= len(dataloader)\n",
    "            print(\n",
    "                f\"Epoch {epoch + 1}/{args.exp.max_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "            # also show accuracy\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for batch_inputs, batch_outputs in dataloader:\n",
    "                predictions = self.propensity_history(batch_inputs)\n",
    "                _, predicted = torch.max(predictions.data, 1)\n",
    "                total += batch_outputs.size(0)\n",
    "\n",
    "                target_labels = batch_outputs.argmax(dim=1)\n",
    "                correct += (predicted == target_labels).sum().item()\n",
    "\n",
    "            print(f\"Accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# %matplotlib inline\n",
    "# from IPython.display import display, clear_output\n",
    "# import matplotlib.pyplot as plt\n",
    "# import time\n",
    "# import statistics\n",
    "# from itertools import chain\n",
    "# import numpy as np\n",
    "# from collections import deque\n",
    "# # torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# def train(model, data_loader, data_loader_validation, epochs, lr, loss_func, weighted_loss_func = None,\n",
    "#           batch_embedder = None, gradient_clip = 1.0, amp_scale = True,\n",
    "#           annealing_mode = False, annealing_window=5, annealing_multiplier=1.25, annealing_ratio = 0.5, annealing_minimum = 1e-6,\n",
    "#           explode = False,\n",
    "#           projection_horizon=1,\n",
    "#           device=\"cuda\", num_gpus=1, verbose=False,\n",
    "#           plot_every=10, plot_display_window=100,\n",
    "#           validation_frequency=1, validation_prp=10, moving_avg_window=10):\n",
    "\n",
    "#     # Check for GPU availability\n",
    "#     available_gpus = torch.cuda.device_count()\n",
    "#     if available_gpus < num_gpus:\n",
    "#         print(f\"Requested {num_gpus} GPUs, but only {available_gpus} are available.\")\n",
    "#         num_gpus = available_gpus\n",
    "#     else:\n",
    "#         print(f\"Using {num_gpus} GPUs for training.\")\n",
    "#         #also print gpu model\n",
    "#         print(f\"GPU model: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "#     if num_gpus > 1:\n",
    "#         model = torch.nn.DataParallel(model, device_ids=list(range(num_gpus)))\n",
    "#         if batch_embedder is not None:\n",
    "#             batch_embedder = torch.nn.DataParallel(batch_embedder, device_ids=list(range(num_gpus)))\n",
    "\n",
    "#     if batch_embedder is not None:\n",
    "#         batch_embedder = batch_embedder.to(device)\n",
    "#     model = model.to(device)\n",
    "\n",
    "#     if batch_embedder is not None:\n",
    "#         optimizer = torch.optim.Adam(\n",
    "#             chain(batch_embedder.parameters(), model.parameters()),\n",
    "#             lr=lr\n",
    "#         )\n",
    "#     else:\n",
    "#         optimizer = torch.optim.Adam(\n",
    "#             model.parameters(),\n",
    "#             lr=lr\n",
    "#         )\n",
    "\n",
    "#     model.train()\n",
    "\n",
    "#     # if batch_embedder is not None:\n",
    "#     #     batch_embedder.train()\n",
    "\n",
    "#     loss_list = []\n",
    "#     fig, ax1 = plt.subplots(1, 1, figsize=(10, 6))\n",
    "#     epoch_loss_list = []\n",
    "#     val_loss = 0\n",
    "\n",
    "#     total_time_start = time.time()\n",
    "\n",
    "#     if amp_scale:\n",
    "#         scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "#     for epoch in range(epochs):\n",
    "#         # Annealing for the learning rate\n",
    "#         if annealing_mode and epoch > annealing_window:\n",
    "#             if len(epoch_loss_list) > 0 and epoch_loss_list[-1] >= annealing_multiplier * (statistics.mean(epoch_loss_list[-annealing_window:])):\n",
    "#                 for g in optimizer.param_groups:\n",
    "#                     if g['lr'] * annealing_ratio < annealing_minimum:\n",
    "#                         g['lr'] = annealing_minimum\n",
    "#                     else:\n",
    "#                         g['lr'] *= annealing_ratio\n",
    "#         start = time.time()\n",
    "#         for i, batch in enumerate(data_loader):\n",
    "#             if explode:\n",
    "#                 batch = model.explode_trajectories(batch, projection_horizon=projection_horizon)\n",
    "#                 # convert to pytorch tensor\n",
    "#                 batch = {key: torch.tensor(value).to(device) for key, value in batch.items()}\n",
    "#             sequence_lengths = batch['sequence_lengths']\n",
    "#             model.sequence_length = sequence_lengths\n",
    "#             if 'stabilized_weights' in batch:\n",
    "#                 stabilized_weights = batch['stabilized_weights']\n",
    "#             curr_treatments = batch['current_treatments']\n",
    "#             vitals_or_prev_outputs = []\n",
    "#             # vitals_or_prev_outputs.append(batch['vitals']) if self.has_vitals else None\n",
    "#             # if self.autoregressive else None\n",
    "#             vitals_or_prev_outputs.append(batch['prev_outputs'])\n",
    "#             vitals_or_prev_outputs = torch.cat(vitals_or_prev_outputs, dim=-1)\n",
    "#             static_features = batch['static_features']\n",
    "#             outputs = batch['outputs']\n",
    "\n",
    "#             batch = torch.cat((vitals_or_prev_outputs, curr_treatments), dim=-1)\n",
    "#             if explode:\n",
    "#                 batch = torch.cat((batch, static_features), dim=-1)\n",
    "#             else:\n",
    "#                 batch = torch.cat((batch, static_features.unsqueeze(\n",
    "#                     1).expand(-1, batch.size(1), -1)), dim=-1)\n",
    "#             batch = torch.cat((batch, outputs), dim=-1)\n",
    "#             if batch.shape[0] == 0:\n",
    "#                 continue\n",
    "#             batch = batch.to(device)\n",
    "\n",
    "#             # if batch_embedder is not None:\n",
    "#             #     batch = batch_embedder(batch)\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "#             if amp_scale:\n",
    "#                 with torch.cuda.amp.autocast():\n",
    "#                     predicted_noise, noise, noise_mask = model(batch)\n",
    "#                     if weighted_loss_func is not None:\n",
    "#                         loss = weighted_loss_func(predicted_noise, noise, noise_mask, stabilized_weights)\n",
    "#                     else:\n",
    "#                         loss = loss_func(predicted_noise, noise, noise_mask)\n",
    "#                 scaler.scale(loss).backward()\n",
    "#                 # Gradient clipping\n",
    "#                 if gradient_clip is not None:\n",
    "#                     scaler.unscale_(optimizer)  # Unscales the gradients of the optimizer's assigned parameters\n",
    "#                     torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clip)\n",
    "#                 scaler.step(optimizer)\n",
    "#                 scaler.update()\n",
    "#                 loss_list.append(loss.item())\n",
    "#             else:\n",
    "#                 predicted_noise, noise, noise_mask = model(batch)\n",
    "#                 if weighted_loss_func is not None:\n",
    "#                     loss = weighted_loss_func(predicted_noise, noise, noise_mask, stabilized_weights)\n",
    "#                 else:\n",
    "#                     loss = loss_func(predicted_noise, noise, noise_mask)\n",
    "#                 loss.backward()\n",
    "#                 # Gradient clipping\n",
    "#                 if gradient_clip is not None:\n",
    "#                     max_grad_norm = gradient_clip\n",
    "#                     torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "#                     if batch_embedder is not None:\n",
    "#                         torch.nn.utils.clip_grad_norm_(batch_embedder.parameters(), max_grad_norm)\n",
    "#                 optimizer.step()\n",
    "#                 loss_list.append(loss.item())\n",
    "\n",
    "#             epoch_loss = sum(loss_list[-len(data_loader):]) / len(data_loader)\n",
    "#             epoch_loss_list.append(epoch_loss)\n",
    "\n",
    "#             # Dynamic plot update focusing on recent losses\n",
    "#             if i % plot_every == 0:\n",
    "#                 ax1.clear()\n",
    "#                 # Only consider the recent losses for plotting (e.g., last 100)\n",
    "#                 display_window = plot_display_window\n",
    "#                 recent_losses = loss_list[-display_window:] if len(loss_list) > display_window else loss_list\n",
    "#                 # Dynamically set y-limits based on recent loss values\n",
    "#                 if recent_losses:\n",
    "#                     min_loss = min(recent_losses)\n",
    "#                     max_loss = max(recent_losses)\n",
    "#                     margin = 0.1 * (max_loss - min_loss) if max_loss != min_loss else 0.1 * max_loss\n",
    "#                     ax1.set_ylim(min_loss - margin, max_loss + margin)\n",
    "#                 ax1.plot(recent_losses)\n",
    "#                 if len(recent_losses) > moving_avg_window:\n",
    "#                     moving_avg = np.convolve(recent_losses, np.ones((moving_avg_window,)) / moving_avg_window, mode='valid')\n",
    "#                     x = np.arange(moving_avg_window - 1, len(recent_losses))\n",
    "#                     ax1.plot(x, moving_avg)\n",
    "#                     ax1.text(len(recent_losses) - 1, moving_avg[-1],\n",
    "#                              f\"{moving_avg[-1]:.3e}\")\n",
    "#                 if epoch_loss_list:\n",
    "#                     ax1.text(0.1, 0.9, f\"Epoch: {epoch} | Learning rate: {optimizer.param_groups[0]['lr']:.2e}\", transform=ax1.transAxes)\n",
    "#                 ax1.text(0.1, 0.8, f\"Loss: {epoch_loss_list[-1]:.3e} | Validation loss: {val_loss:.3e}\", transform=ax1.transAxes)\n",
    "#                 ax1.text(0.1, 0.7, f\"Time per step: {((time.time() - start) / (i + 1)):.2f} s | Time per epoch: {((time.time() - start) / (i + 1) * len(data_loader)):.2f} s\", transform=ax1.transAxes)\n",
    "#                 ax1.text(0.1, 0.6, f\"Time till finish (est.): {((time.time() - start) / (i + 1) * len(data_loader) * (epochs - epoch)) / 60:.2f} min\", transform=ax1.transAxes)\n",
    "#                 display(fig)\n",
    "#                 clear_output(wait=True)\n",
    "\n",
    "#         end = time.time()\n",
    "\n",
    "#         # Validation\n",
    "#         if epoch % validation_frequency == 0:\n",
    "#             loss_list_validation = []\n",
    "#             for i, batch in enumerate(data_loader_validation):\n",
    "#                 curr_treatments = batch['current_treatments']\n",
    "#                 vitals_or_prev_outputs = []\n",
    "#                 # vitals_or_prev_outputs.append(batch['vitals']) if self.has_vitals else None\n",
    "#                 # if self.autoregressive else None\n",
    "#                 vitals_or_prev_outputs.append(batch['prev_outputs'])\n",
    "#                 vitals_or_prev_outputs = torch.cat(vitals_or_prev_outputs, dim=-1)\n",
    "#                 static_features = batch['static_features']\n",
    "#                 outputs = batch['outputs']\n",
    "\n",
    "#                 batch = torch.cat((vitals_or_prev_outputs, curr_treatments), dim=-1)\n",
    "#                 batch = torch.cat((batch, static_features.unsqueeze(\n",
    "#                     1).expand(-1, batch.size(1), -1)), dim=-1)\n",
    "#                 batch = torch.cat((batch, outputs), dim=-1)\n",
    "\n",
    "#                 batch = batch.to(device)\n",
    "#                 # if batch_embedder is not None:\n",
    "#                 #     batch = batch_embedder(batch)\n",
    "#                 if i % validation_prp == 0:\n",
    "#                     predicted_noise, noise, noise_mask = model(batch)\n",
    "#                     loss = loss_func(predicted_noise, noise, noise_mask)\n",
    "#                     loss_list_validation.append(loss.item())\n",
    "\n",
    "#             val_loss = np.mean(loss_list_validation)\n",
    "\n",
    "#         total_time_end = time.time()\n",
    "#         total_time = total_time_end - total_time_start\n",
    "\n",
    "#         if verbose:\n",
    "#             print(f\"Epoch {epoch} completed in {end - start} seconds, Loss: {epoch_loss}\")\n",
    "#             print(f\"Validation Loss: {val_loss}\")\n",
    "\n",
    "#     print(f\"Took {total_time} seconds for {epoch} epochs.\")\n",
    "\n",
    "\n",
    "#     return model, loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# from IPython.display import display, clear_output\n",
    "# import matplotlib.pyplot as plt\n",
    "# import time\n",
    "# import statistics\n",
    "# from itertools import chain\n",
    "# import numpy as np\n",
    "# from collections import deque\n",
    "\n",
    "# def train(model, data_loader, data_loader_validation, epochs, lr, loss_func, batch_embedder,\n",
    "#           windowed_mode=False, window_mode=\"uniform\", window_start_mode=\"random\", min_window=50, max_window=100, neg_bin_p=0.95, train_on_all_every=4,\n",
    "#           annealing_mode = False, annealing_window=5, annealing_multiplier=1.25, annealing_ratio = 0.5, annealing_minimum = 1e-6,\n",
    "#           device=\"cuda\", verbose=False, plot_every=10,\n",
    "#           validation_frequency=1, validation_prp=10, moving_avg_window=10):\n",
    "\n",
    "#     batch_embedder = batch_embedder.to(device)\n",
    "#     model = model.to(device)\n",
    "\n",
    "#     optimizer = torch.optim.Adam(\n",
    "#         chain(batch_embedder.parameters(), model.parameters()),\n",
    "#         lr=lr\n",
    "#     )\n",
    "\n",
    "#     model.train()\n",
    "#     batch_embedder.train()\n",
    "#     loss_list = []\n",
    "#     initial_value = 1.0  # Initial value for equal probability\n",
    "#     window_losses = torch.ones(max_window - min_window + 1, device=device) * initial_value  # Track losses for each window length\n",
    "#     window_counts = torch.zeros(max_window - min_window + 1, device=device)  # Track counts for each window length\n",
    "#     loss_deques = [deque(maxlen=moving_avg_window) for _ in range(max_window - min_window + 1)]  # Deques for moving average\n",
    "#     if windowed_mode and window_mode == \"biased_loss\":\n",
    "#         fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(10, 12))\n",
    "#     else:\n",
    "#         fig, ax1 = plt.subplots(1, 1, figsize=(10, 6))\n",
    "#     epoch_loss_list = []\n",
    "#     val_loss = 0\n",
    "\n",
    "#     for epoch in range(epochs):\n",
    "#         # Annealing for the learning rate\n",
    "#         if annealing_mode and epoch > annealing_window:\n",
    "#             if len(epoch_loss_list) > 0 and epoch_loss_list[-1] >= annealing_multiplier * (statistics.mean(epoch_loss_list[-annealing_window:])):\n",
    "#                 for g in optimizer.param_groups:\n",
    "#                     if g['lr'] * annealing_ratio < annealing_minimum:\n",
    "#                         g['lr'] = annealing_minimum\n",
    "#                     else:\n",
    "#                         g['lr'] *= annealing_ratio\n",
    "\n",
    "#         start = time.time()\n",
    "#         for i, batch in enumerate(data_loader):\n",
    "\n",
    "#             batch = batch.to(device)\n",
    "#             batch = batch_embedder(batch)\n",
    "\n",
    "#             batch_length = batch.shape[1]\n",
    "\n",
    "#             # Windowed mode logic\n",
    "#             if windowed_mode:\n",
    "#                 if batch_length < min_window:\n",
    "#                     continue\n",
    "#                 if window_start_mode == \"random\":\n",
    "#                     cut_start = torch.randint(0, batch_length - window_length + 1, (1,)).item()\n",
    "#                 elif window_start_mode == \"fixed\":\n",
    "#                     cut_start = 0\n",
    "#                 if window_mode == \"uniform\":\n",
    "#                     while True:\n",
    "#                         window_length = torch.randint(min_window, batch_length + 1, (1,)).item()\n",
    "#                         cut_end = cut_start + window_length\n",
    "#                         if min_window <= (cut_end - cut_start) <= batch_length:\n",
    "#                             break\n",
    "#                     batch = batch[:, cut_start:cut_end, :]\n",
    "\n",
    "#                 elif window_mode == \"negative_binomial\":\n",
    "#                     total_count = 1\n",
    "#                     probs = neg_bin_p\n",
    "#                     distribution = torch.distributions.NegativeBinomial(total_count=total_count, probs=probs)\n",
    "#                     while True:\n",
    "#                         window_length = distribution.sample().item() + min_window\n",
    "#                         cut_end = cut_start + window_length\n",
    "#                         if min_window <= window_length <= batch_length:\n",
    "#                             break\n",
    "#                     batch = batch[:, cut_start:cut_end, :]\n",
    "\n",
    "#                 elif window_mode == \"biased_loss\":\n",
    "#                     if torch.min(window_counts) < 2:\n",
    "#                         # Use uniform distribution until each length has been used at least twice\n",
    "#                         window_probs = torch.ones_like(window_losses) / len(window_losses)\n",
    "#                     elif torch.sum(window_counts) % train_on_all_every == 0:\n",
    "#                         window_probs = torch.ones_like(window_losses) / len(window_losses)\n",
    "#                     else:\n",
    "#                         # Update probabilities based on moving average of losses\n",
    "#                         avg_losses = torch.tensor([np.mean(loss_deque) if len(loss_deque) > 0 else initial_value for loss_deque in loss_deques], device=device)\n",
    "#                         window_probs = avg_losses / avg_losses.sum()\n",
    "#                     while True:\n",
    "#                         window_length = torch.multinomial(window_probs, 1).item() + min_window\n",
    "#                         #check if the window length does work with the batch length\n",
    "#                         if window_length > batch_length:\n",
    "#                             continue\n",
    "#                         cut_end = cut_start + window_length\n",
    "#                         if min_window <= window_length <= batch_length:\n",
    "#                             break\n",
    "#                     batch = batch[:, cut_start:cut_end, :]\n",
    "#                     window_counts[window_length - min_window] += 1  # Update window counts\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "#             predicted_noise, noise, noise_mask = model(batch)\n",
    "#             loss = loss_func(predicted_noise, noise, noise_mask)\n",
    "#             loss.backward()\n",
    "#             # # Gradient clipping\n",
    "#             max_grad_norm = 1.0\n",
    "#             torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "#             torch.nn.utils.clip_grad_norm_(batch_embedder.parameters(), max_grad_norm)\n",
    "#             optimizer.step()\n",
    "#             loss_list.append(loss.item())\n",
    "\n",
    "#             epoch_loss = sum(loss_list[-len(data_loader):]) / len(data_loader)\n",
    "#             epoch_loss_list.append(epoch_loss)\n",
    "\n",
    "#             # Update window losses and moving average deque\n",
    "#             if windowed_mode and window_mode == \"biased_loss\":\n",
    "#                 window_idx = window_length - min_window\n",
    "#                 window_losses[window_idx] += loss.item()\n",
    "#                 loss_deques[window_idx].append(loss.item())\n",
    "\n",
    "#             # Dynamic plot update\n",
    "#             if i % plot_every == 0:\n",
    "#                 ax1.clear()\n",
    "#                 ax1.set_ylim(0, 1)\n",
    "#                 ax1.plot(loss_list)\n",
    "#                 if len(loss_list) > 100:\n",
    "#                     ax1.plot(np.convolve(loss_list, np.ones((100,))/100, mode='valid'))\n",
    "#                     ax1.text(len(loss_list) - 1, np.convolve(loss_list, np.ones((100,))/100, mode='valid')[-1],\n",
    "#                             str(round(np.convolve(loss_list, np.ones((100,))/100, mode='valid')[-1], 3)))\n",
    "#                 if len(epoch_loss_list) > 0:\n",
    "#                     ax1.text(0.1, 0.9, f\"Epoch: {epoch} | Learning rate: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "#                 # ax1.text(0.1, 0.8, f\"Learning rate: {optimizer.param_groups[0]['lr']:.4e}\")\n",
    "#                 ax1.text(0.1, 0.8, f\"Loss: {epoch_loss_list[-1]:.3e} | Validation loss: {val_loss:.3e}\")\n",
    "#                 ax1.text(0.1, 0.7, f\"Time per step: {((time.time() - start) / (i + 1)):.2f} s | Time per epoch: {((time.time() - start) / (i + 1) * len(data_loader)):.2f} s\")\n",
    "#                 ax1.text(0.1, 0.6, f\"Time till finish (est.): {((time.time() - start) / (i + 1) * len(data_loader) * (epochs - epoch)) / 60:.2f} min\")\n",
    "#                 if windowed_mode and window_mode == \"biased_loss\":\n",
    "#                     ax2.clear()\n",
    "#                     ax2.bar(range(min_window, max_window + 1), window_counts.cpu().numpy())\n",
    "#                     ax2.set_ylabel(\"Counts\")\n",
    "#                     ax2.set_title(\"Counts of Each Window Length Used\")\n",
    "\n",
    "#                     moving_avg_losses = torch.tensor([np.mean(loss_deque) if len(loss_deque) > 0 else initial_value for loss_deque in loss_deques], device=device).cpu().numpy()\n",
    "#                     ax3.clear()\n",
    "#                     ax3.bar(range(min_window, max_window + 1), moving_avg_losses)\n",
    "#                     ax3.set_xlabel(\"Window Length\")\n",
    "#                     ax3.set_ylabel(\"Moving Average Loss\")\n",
    "#                     ax3.set_title(\"Moving Average Loss for Each Window Length\")\n",
    "\n",
    "#                 display(fig)\n",
    "#                 clear_output(wait=True)\n",
    "\n",
    "#         end = time.time()\n",
    "\n",
    "#         # Validation\n",
    "#         if epoch % validation_frequency == 0:\n",
    "#             loss_list_validation = []\n",
    "#             for i, batch in enumerate(data_loader_validation):\n",
    "#                 batch = batch.to(device)\n",
    "#                 batch = batch_embedder(batch)\n",
    "#                 if i % validation_prp == 0:\n",
    "#                     predicted_noise, noise, noise_mask = model(batch)\n",
    "#                     loss = loss_func(predicted_noise, noise, noise_mask)\n",
    "#                     loss_list_validation.append(loss.item())\n",
    "\n",
    "#             val_loss = np.mean(loss_list_validation)\n",
    "\n",
    "#         if verbose:\n",
    "#             print(f\"Epoch {epoch} completed in {end - start} seconds, Loss: {epoch_loss}\")\n",
    "#             print(f\"Validation Loss: {val_loss}\")\n",
    "\n",
    "\n",
    "#     return model, loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_multiple_evaluations(\n",
    "    dataloader,\n",
    "    imputer,\n",
    "    training_mean,\n",
    "    training_std,\n",
    "    sample_number,\n",
    "    batch_embedder=None,\n",
    "    old_sample=[],\n",
    "    min_sequence_len=2,\n",
    "    max_sequence_len=None,\n",
    "    scale=1,\n",
    "    verbose=True,\n",
    "    show_max_diff=False,\n",
    "    show_rmse=False,\n",
    "    num_gpus=1\n",
    "):\n",
    "\n",
    "    imputer.eval()\n",
    "    final_samples = old_sample\n",
    "    max_seq_len = 0\n",
    "    total_batches = len(dataloader) * sample_number\n",
    "    completed_batches = 0\n",
    "    sample_time = []\n",
    "    average_sample_time = 0\n",
    "\n",
    "    for i in range(sample_number):\n",
    "        sample_start = time.time()\n",
    "        # print a line to separate the samples\n",
    "        print(\"-------------------------------------------------\")\n",
    "        print(f\"Running sample {i + 1}/{sample_number}\")\n",
    "        all_samples = []\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            # Get the data from the batch (collate_fn returns a tuple)\n",
    "            sequence_lengths = batch['sequence_lengths']\n",
    "            for key in batch.keys():\n",
    "                batch[key] = batch[key][sequence_lengths >= min_sequence_len]\n",
    "            sequence_lengths = batch['sequence_lengths']\n",
    "            if max_sequence_len is not None:\n",
    "                for key in batch.keys():\n",
    "                    batch[key] = batch[key][sequence_lengths <= max_sequence_len]\n",
    "                sequence_lengths = batch['sequence_lengths']\n",
    "            imputer.sequence_length = sequence_lengths\n",
    "\n",
    "            curr_treatments = batch['current_treatments']\n",
    "            vitals_or_prev_outputs = []\n",
    "            # vitals_or_prev_outputs.append(batch['vitals']) if self.has_vitals else None\n",
    "            # if self.autoregressive else None\n",
    "            vitals_or_prev_outputs.append(batch['prev_outputs'])\n",
    "            vitals_or_prev_outputs = torch.cat(vitals_or_prev_outputs, dim=-1)\n",
    "            static_features = batch['static_features']\n",
    "            outputs = batch['outputs'][:, :, 1:2]\n",
    "\n",
    "            batch = torch.cat(\n",
    "                (vitals_or_prev_outputs, curr_treatments), dim=-1)\n",
    "            batch = torch.cat((batch, static_features.unsqueeze(\n",
    "                1).expand(-1, batch.size(1), -1)), dim=-1)\n",
    "            batch = torch.cat((batch, outputs), dim=-1)\n",
    "            # select the entries in the batch that have more than min_sequence_len\n",
    "            # batch = batch[sequence_lengths >= min_sequence_len]\n",
    "            batch = batch.to(imputer.device)\n",
    "            # if batch_embedder is not None:\n",
    "            #     batch = batch_embedder(batch)\n",
    "            # seq_length = batch.shape[1]\n",
    "            # if seq_length < min_sequence_len:\n",
    "            #     # print(f\"Skipping batch {batch_idx + 1} as sequence length is less than {min_sequence_len}\")\n",
    "            #     completed_batches += 1\n",
    "            #     continue\n",
    "            # if max_sequence_len is not None and seq_length > max_sequence_len:\n",
    "            #     # print(f\"Skipping batch {batch_idx + 1} as sequence length is greater than {max_sequence_len}\")\n",
    "            #     completed_batches += 1\n",
    "            #     continue\n",
    "\n",
    "            # print(f\"sequence length: {sequence_lengths}\")\n",
    "\n",
    "            # Generate imputation masks for the current batch\n",
    "            imputation_masks = imputer.get_mask(\n",
    "                batch, strategy=\"selected_features_last_n_sequence_length\"\n",
    "            ).to(imputer.device)\n",
    "            imputer.features_to_impute = [6]\n",
    "            imputer.last_n_time = 1\n",
    "            with torch.no_grad():\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    imputed_samples = imputer.get_predictions(\n",
    "                        batch,\n",
    "                        imputation_masks,\n",
    "                        mean=training_mean,\n",
    "                        std=training_std,\n",
    "                        scale=scale,\n",
    "                        verbose=verbose,\n",
    "                        show_max_diff=show_max_diff,\n",
    "                        show_rmse=show_rmse\n",
    "                    )\n",
    "\n",
    "            all_samples.append(imputed_samples)\n",
    "\n",
    "            completed_batches += 1\n",
    "            progress = completed_batches / total_batches\n",
    "            print(f\"Overall Progress: {progress * 100:.2f}%\")\n",
    "            print(\n",
    "                f\"Time to finish (est.): {average_sample_time * (sample_number - i - 1) / 60:.2f} min\"\n",
    "            )\n",
    "\n",
    "        sample_end = time.time()\n",
    "        sample_time.append(sample_end - sample_start)\n",
    "        average_sample_time = sum(sample_time) / len(sample_time)\n",
    "        final_samples.append(all_samples)\n",
    "\n",
    "        rmse, rmse_median = calculate_rmse(\n",
    "            final_samples, training_mean, training_std)\n",
    "        print(f\"RMSE: {rmse:.3f} | RMSE (Median): {rmse_median:.3f}\")\n",
    "\n",
    "        if mlflow.active_run() is not None:\n",
    "            mlflow.log_metric(\"RMSE\", rmse, step=i)\n",
    "            mlflow.log_metric(\"RMSE_Median\", rmse_median, step=i)\n",
    "\n",
    "    return final_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import random\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torch.utils.data import Sampler\n",
    "# from collections import defaultdict\n",
    "# import torch\n",
    "# from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "# class CustomTimeSeriesDataset(Dataset):\n",
    "#     def __init__(\n",
    "#         self, data_tensor, sequence_lengths, min_seq_length=None, max_seq_length=None\n",
    "#     ):\n",
    "#         # Store the initial sequences and lengths\n",
    "#         self.data_tensor = data_tensor\n",
    "#         self.sequence_lengths = sequence_lengths\n",
    "#         self.min_seq_length = min_seq_length\n",
    "#         self.max_seq_length = max_seq_length\n",
    "\n",
    "#         # Filter sequences based on min and max sequence length\n",
    "#         if self.min_seq_length is not None:\n",
    "#             valid_indices = [\n",
    "#                 i\n",
    "#                 for i, length in enumerate(self.sequence_lengths)\n",
    "#                 if length >= self.min_seq_length\n",
    "#             ]\n",
    "#         else:\n",
    "#             valid_indices = list(range(len(self.sequence_lengths)))\n",
    "\n",
    "#         if self.max_seq_length is not None:\n",
    "#             valid_indices = [\n",
    "#                 i\n",
    "#                 for i in valid_indices\n",
    "#                 if self.sequence_lengths[i] <= self.max_seq_length\n",
    "#             ]\n",
    "\n",
    "#         self.data_tensor = self.data_tensor[valid_indices]\n",
    "#         self.sequence_lengths = [self.sequence_lengths[i]\n",
    "#                                  for i in valid_indices]\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.sequence_lengths)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         seq_length = int(self.sequence_lengths[idx])\n",
    "#         return self.data_tensor[idx, :seq_length, :], seq_length\n",
    "\n",
    "\n",
    "# # class CustomTimeSeriesDataset(Dataset):\n",
    "# #     def __init__(self, data_tensor, sequence_lengths, min_seq_length=None, max_seq_length=None):\n",
    "# #         self.data_tensor = data_tensor\n",
    "# #         self.sequence_lengths = sequence_lengths\n",
    "\n",
    "# #     def __len__(self):\n",
    "# #         return len(self.sequence_lengths)\n",
    "\n",
    "# #     def __getitem__(self, idx):\n",
    "# #         seq_length = int(self.sequence_lengths[idx])\n",
    "# #         return self.data_tensor[idx, :seq_length, :], seq_length\n",
    "\n",
    "\n",
    "# class LengthBatchSampler(Sampler):\n",
    "#     def __init__(self, dataset, batch_size):\n",
    "#         self.sequence_lengths = dataset.sequence_lengths\n",
    "#         self.batch_size = batch_size\n",
    "#         self.batches = self._create_batches()\n",
    "\n",
    "#     def _create_batches(self):\n",
    "#         length_to_indices = defaultdict(list)\n",
    "#         for idx, length in enumerate(self.sequence_lengths):\n",
    "#             length_to_indices[length].append(idx)\n",
    "\n",
    "#         batches = []\n",
    "#         for length, indices in length_to_indices.items():\n",
    "#             # Split indices into batches of the specified batch size\n",
    "#             for i in range(0, len(indices), self.batch_size):\n",
    "#                 batches.append(indices[i: i + self.batch_size])\n",
    "#         return batches\n",
    "\n",
    "#     def __iter__(self):\n",
    "#         for batch in self.batches:\n",
    "#             yield batch\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.batches)\n",
    "\n",
    "\n",
    "# def collate_fn(batch):\n",
    "#     data, lengths = zip(*batch)\n",
    "#     data = torch.stack(data)\n",
    "#     return data\n",
    "\n",
    "\n",
    "# def create_dataloader(\n",
    "#     data_tensor, sequence_lengths, batch_size, min_seq_length=None, max_seq_length=None\n",
    "# ):\n",
    "#     dataset = CustomTimeSeriesDataset(\n",
    "#         data_tensor, sequence_lengths, min_seq_length, max_seq_length\n",
    "#     )\n",
    "#     sampler = LengthBatchSampler(dataset, batch_size)\n",
    "#     dataloader = DataLoader(\n",
    "#         dataset, batch_sampler=sampler, collate_fn=collate_fn)\n",
    "#     return dataloader\n",
    "\n",
    "\n",
    "# class BalancedLengthBatchSampler(Sampler):\n",
    "#     def __init__(self, dataset, batch_size, balance_factor=1.0):\n",
    "#         self.sequence_lengths = dataset.sequence_lengths\n",
    "#         self.batch_size = batch_size\n",
    "#         self.balance_factor = balance_factor\n",
    "#         self.batches = self._create_balanced_batches()\n",
    "\n",
    "#     def _create_balanced_batches(self):\n",
    "#         # Group indices by sequence length\n",
    "#         length_to_indices = defaultdict(list)\n",
    "#         for idx, length in enumerate(self.sequence_lengths):\n",
    "#             length_to_indices[length].append(idx)\n",
    "\n",
    "#         # Calculate the maximum count of indices for balancing\n",
    "#         max_count = max(len(indices) for indices in length_to_indices.values())\n",
    "\n",
    "#         # Balance the distribution of sequence lengths by oversampling shorter sequences\n",
    "#         balanced_batches = []\n",
    "#         for length, indices in length_to_indices.items():\n",
    "#             count = len(indices)\n",
    "#             if count < max_count:\n",
    "#                 repeat_factor = int(self.balance_factor * (max_count / count))\n",
    "#                 # Correctly oversample the list elements\n",
    "#                 oversampled_indices = indices * repeat_factor\n",
    "#                 # oversampled_indices = oversampled_indices[:max_count]\n",
    "#             else:\n",
    "#                 oversampled_indices = indices\n",
    "\n",
    "#             # Shuffle the indices of this particular length\n",
    "#             random.shuffle(oversampled_indices)\n",
    "\n",
    "#             # Create batches for this length\n",
    "#             for i in range(0, len(oversampled_indices), self.batch_size):\n",
    "#                 batch = oversampled_indices[i: i + self.batch_size]\n",
    "#                 if len(batch) == self.batch_size:\n",
    "#                     balanced_batches.append(batch)\n",
    "\n",
    "#         # Shuffle the list of balanced batches to ensure random order\n",
    "#         random.shuffle(balanced_batches)\n",
    "\n",
    "#         return balanced_batches\n",
    "\n",
    "#     def __iter__(self):\n",
    "#         for batch in self.batches:\n",
    "#             yield batch\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.batches)\n",
    "\n",
    "\n",
    "# def create_balanced_dataloader(\n",
    "#     data_tensor,\n",
    "#     sequence_lengths,\n",
    "#     batch_size,\n",
    "#     balance_factor=1.0,\n",
    "#     min_seq_length=None,\n",
    "#     max_seq_length=None,\n",
    "# ):\n",
    "#     dataset = CustomTimeSeriesDataset(\n",
    "#         data_tensor, sequence_lengths, min_seq_length, max_seq_length\n",
    "#     )\n",
    "#     sampler = BalancedLengthBatchSampler(dataset, batch_size, balance_factor)\n",
    "#     dataloader = DataLoader(\n",
    "#         dataset, batch_sampler=sampler, collate_fn=collate_fn)\n",
    "#     return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = create_balanced_dataloader(\n",
    "#     training_data_tensor,\n",
    "#     training_data_sequence_lengths,\n",
    "#     batch_size=100,\n",
    "#     balance_factor=1.0,\n",
    "#     min_seq_length=2,\n",
    "#     max_seq_length=None,\n",
    "# )\n",
    "\n",
    "# val_loader = create_balanced_dataloader(\n",
    "#     validation_data_tensor,\n",
    "#     validation_data_sequence_lengths,\n",
    "#     batch_size=100,\n",
    "#     balance_factor=1.0,\n",
    "#     min_seq_length=2,\n",
    "#     max_seq_length=None,\n",
    "# )\n",
    "\n",
    "# # check the size of the train, val, and test sets\n",
    "# print(len(train_loader))\n",
    "# print(len(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_data_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical_indices_sizes = {\n",
    "#     # 'time_step' : [0, 1],\n",
    "#     \"chemo_application_prev\": [0, 1],\n",
    "#     \"radio_application_prev\": [1, 1],\n",
    "#     \"patient_type_tile\": [2, 1],\n",
    "# }\n",
    "\n",
    "# numerical_indices = {\"cancer_volume\": 3}\n",
    "\n",
    "# numerical_indices = {\n",
    "#     'chemo_application_prev': 0,\n",
    "#     'radio_application_prev': 1,\n",
    "#     'patient_type_tile': 2,\n",
    "#     'cancer_volume': 3\n",
    "# }\n",
    "\n",
    "\n",
    "# training_data_tensor_embedded = data_embedder(training_data_tensor)\n",
    "# validation_data_tensor_embedded = data_embedder(validation_data_tensor)\n",
    "# test_data_factuals_tensor_embedded = data_embedder(test_data_factuals_tensor)\n",
    "# test_data_counterfactuals_tensor_embedded = data_embedder(test_data_counterfactuals_tensor)\n",
    "# test_data_seq_tensor_embedded = data_embedder(test_data_seq_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "%matplotlib inline\n",
    "from IPython.display import display, clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import statistics\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "def train(model, data_loader, data_loader_validation, epochs, lr, loss_func, weighted_loss_func = None, batch_embedder = None, gradient_clip = 1.0,\n",
    "          windowed_mode=False, window_mode=\"uniform\", window_start_mode=\"random\", min_window=50, max_window=100, neg_bin_p=0.95, train_on_all_every=4,\n",
    "          annealing_mode = False, annealing_window=5, annealing_multiplier=1.25, annealing_ratio = 0.5, annealing_minimum = 1e-6,\n",
    "          device=\"cuda\", num_gpus=1, verbose=False, plot_every=10,\n",
    "          validation_frequency=1, validation_prp=10, moving_avg_window=10):\n",
    "\n",
    "    # Check for GPU availability\n",
    "    available_gpus = torch.cuda.device_count()\n",
    "    if available_gpus < num_gpus:\n",
    "        print(f\"Requested {num_gpus} GPUs, but only {available_gpus} are available.\")\n",
    "        num_gpus = available_gpus\n",
    "    else:\n",
    "        print(f\"Using {num_gpus} GPUs for training.\")\n",
    "        #also print gpu model\n",
    "        print(f\"GPU model: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "    if num_gpus > 1:\n",
    "        model = torch.nn.DataParallel(model, device_ids=list(range(num_gpus)))\n",
    "        if batch_embedder is not None:\n",
    "            batch_embedder = torch.nn.DataParallel(batch_embedder, device_ids=list(range(num_gpus)))\n",
    "\n",
    "    if batch_embedder is not None:\n",
    "        batch_embedder = batch_embedder.to(device)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    if batch_embedder is not None:\n",
    "        optimizer = torch.optim.Adam(\n",
    "            chain(batch_embedder.parameters(), model.parameters()),\n",
    "            lr=lr\n",
    "        )\n",
    "    else:\n",
    "        optimizer = torch.optim.Adam(\n",
    "            model.parameters(),\n",
    "            lr=lr\n",
    "        )\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # if batch_embedder is not None:\n",
    "    #     batch_embedder.train()\n",
    "    \n",
    "    loss_list = []\n",
    "    initial_value = 1.0  # Initial value for equal probability\n",
    "    window_losses = torch.ones(max_window - min_window + 1, device=device) * initial_value  # Track losses for each window length\n",
    "    window_counts = torch.zeros(max_window - min_window + 1, device=device)  # Track counts for each window length\n",
    "    loss_deques = [deque(maxlen=moving_avg_window) for _ in range(max_window - min_window + 1)]  # Deques for moving average\n",
    "    if windowed_mode and window_mode == \"biased_loss\":\n",
    "        fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(10, 12))\n",
    "    else:\n",
    "        fig, ax1 = plt.subplots(1, 1, figsize=(10, 6))\n",
    "    epoch_loss_list = []\n",
    "    val_loss = 0\n",
    "\n",
    "    total_time_start = time.time()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Annealing for the learning rate\n",
    "        if annealing_mode and epoch > annealing_window:\n",
    "            if len(epoch_loss_list) > 0 and epoch_loss_list[-1] >= annealing_multiplier * (statistics.mean(epoch_loss_list[-annealing_window:])):\n",
    "                for g in optimizer.param_groups:\n",
    "                    if g['lr'] * annealing_ratio < annealing_minimum:\n",
    "                        g['lr'] = annealing_minimum\n",
    "                    else:\n",
    "                        g['lr'] *= annealing_ratio\n",
    "\n",
    "        start = time.time()\n",
    "        for i, batch in enumerate(data_loader):\n",
    "\n",
    "            if 'stabilized_weights' in batch:\n",
    "                stabilized_weights = batch['stabilized_weights']\n",
    "            curr_treatments = batch['current_treatments']\n",
    "            vitals_or_prev_outputs = []\n",
    "            # vitals_or_prev_outputs.append(batch['vitals']) if self.has_vitals else None\n",
    "            # if self.autoregressive else None\n",
    "            vitals_or_prev_outputs.append(batch['prev_outputs'])\n",
    "            vitals_or_prev_outputs = torch.cat(vitals_or_prev_outputs, dim=-1)\n",
    "            static_features = batch['static_features']\n",
    "            outputs = batch['outputs']\n",
    "\n",
    "            batch = torch.cat((vitals_or_prev_outputs, curr_treatments), dim=-1)\n",
    "            batch = torch.cat((batch, static_features.unsqueeze(\n",
    "                1).expand(-1, batch.size(1), -1)), dim=-1)\n",
    "            batch = torch.cat((batch, outputs), dim=-1)\n",
    "\n",
    "            batch = batch.to(device)\n",
    "\n",
    "            # if batch_embedder is not None:\n",
    "            #     batch = batch_embedder(batch)\n",
    "\n",
    "            batch_length = batch.shape[1]\n",
    "            # batch_length = max_window\n",
    "\n",
    "            # Windowed mode logic\n",
    "            if windowed_mode:\n",
    "                if batch_length < min_window:\n",
    "                    continue\n",
    "                if window_start_mode == \"random\":\n",
    "                    cut_start = torch.randint(0, batch_length - window_length + 1, (1,)).item()\n",
    "                elif window_start_mode == \"fixed\":\n",
    "                    cut_start = 0\n",
    "                if window_mode == \"uniform\":\n",
    "                    while True:\n",
    "                        window_length = torch.randint(min_window, batch_length + 1, (1,)).item()\n",
    "                        cut_end = cut_start + window_length\n",
    "                        if min_window <= (cut_end - cut_start) <= batch_length:\n",
    "                            break\n",
    "                    batch = batch[:, cut_start:cut_end, :]\n",
    "                    stabilized_weights = stabilized_weights[:, cut_start:cut_end]\n",
    "\n",
    "                elif window_mode == \"negative_binomial\":\n",
    "                    total_count = 1\n",
    "                    probs = neg_bin_p\n",
    "                    distribution = torch.distributions.NegativeBinomial(total_count=total_count, probs=probs)\n",
    "                    while True:\n",
    "                        window_length = distribution.sample().item() + min_window\n",
    "                        cut_end = cut_start + window_length\n",
    "                        if min_window <= window_length <= batch_length:\n",
    "                            break\n",
    "                    batch = batch[:, cut_start:cut_end, :]\n",
    "\n",
    "                elif window_mode == \"biased_loss\":\n",
    "                    if torch.min(window_counts) < 2:\n",
    "                        # Use uniform distribution until each length has been used at least twice\n",
    "                        window_probs = torch.ones_like(window_losses) / len(window_losses)\n",
    "                    elif torch.sum(window_counts) % train_on_all_every == 0:\n",
    "                        window_probs = torch.ones_like(window_losses) / len(window_losses)\n",
    "                    else:\n",
    "                        # Update probabilities based on moving average of losses\n",
    "                        avg_losses = torch.tensor([np.mean(loss_deque) if len(loss_deque) > 0 else initial_value for loss_deque in loss_deques], device=device)\n",
    "                        window_probs = avg_losses / avg_losses.sum()\n",
    "                    while True:\n",
    "                        window_length = torch.multinomial(window_probs, 1).item() + min_window\n",
    "                        #check if the window length does work with the batch length\n",
    "                        if window_length > batch_length:\n",
    "                            continue\n",
    "                        cut_end = cut_start + window_length\n",
    "                        if min_window <= window_length <= batch_length:\n",
    "                            break\n",
    "                    batch = batch[:, cut_start:cut_end, :]\n",
    "                    window_counts[window_length - min_window] += 1  # Update window counts\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            predicted_noise, noise, noise_mask = model(batch)\n",
    "            \n",
    "            if weighted_loss_func is not None:\n",
    "                loss = weighted_loss_func(predicted_noise, noise, noise_mask, stabilized_weights)\n",
    "            else:\n",
    "                loss = loss_func(predicted_noise, noise, noise_mask)\n",
    "            \n",
    "            loss.backward()\n",
    "            # # Gradient clipping\n",
    "            if gradient_clip is not None:\n",
    "                max_grad_norm = gradient_clip\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "                if batch_embedder is not None:\n",
    "                    torch.nn.utils.clip_grad_norm_(batch_embedder.parameters(), max_grad_norm)\n",
    "            optimizer.step()\n",
    "            loss_list.append(loss.item())\n",
    "\n",
    "            epoch_loss = sum(loss_list[-len(data_loader):]) / len(data_loader)\n",
    "            epoch_loss_list.append(epoch_loss)\n",
    "            \n",
    "            # Update window losses and moving average deque\n",
    "            if windowed_mode and window_mode == \"biased_loss\":\n",
    "                window_idx = window_length - min_window\n",
    "                window_losses[window_idx] += loss.item()\n",
    "                loss_deques[window_idx].append(loss.item())\n",
    "\n",
    "            # Dynamic plot update\n",
    "            if i % plot_every == 0:\n",
    "                ax1.clear()\n",
    "                ax1.set_ylim(0, 1)\n",
    "                ax1.plot(loss_list)\n",
    "                if len(loss_list) > 100:\n",
    "                    ax1.plot(np.convolve(loss_list, np.ones((100,))/100, mode='valid'))\n",
    "                    ax1.text(len(loss_list) - 1, np.convolve(loss_list, np.ones((100,))/100, mode='valid')[-1],\n",
    "                            str(round(np.convolve(loss_list, np.ones((100,))/100, mode='valid')[-1], 3)))\n",
    "                if len(epoch_loss_list) > 0:\n",
    "                    ax1.text(0.1, 0.9, f\"Epoch: {epoch} | Learning rate: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "                # ax1.text(0.1, 0.8, f\"Learning rate: {optimizer.param_groups[0]['lr']:.4e}\")\n",
    "                ax1.text(0.1, 0.8, f\"Loss: {epoch_loss_list[-1]:.3e} | Validation loss: {val_loss:.3e}\")\n",
    "                ax1.text(0.1, 0.7, f\"Time per step: {((time.time() - start) / (i + 1)):.2f} s | Time per epoch: {((time.time() - start) / (i + 1) * len(data_loader)):.2f} s\")\n",
    "                ax1.text(0.1, 0.6, f\"Time till finish (est.): {((time.time() - start) / (i + 1) * len(data_loader) * (epochs - epoch)) / 60:.2f} min\")\n",
    "                if windowed_mode and window_mode == \"biased_loss\":\n",
    "                    ax2.clear()\n",
    "                    ax2.bar(range(min_window, max_window + 1), window_counts.cpu().numpy())\n",
    "                    ax2.set_ylabel(\"Counts\")\n",
    "                    ax2.set_title(\"Counts of Each Window Length Used\")\n",
    "\n",
    "                    moving_avg_losses = torch.tensor([np.mean(loss_deque) if len(loss_deque) > 0 else initial_value for loss_deque in loss_deques], device=device).cpu().numpy()\n",
    "                    ax3.clear()\n",
    "                    ax3.bar(range(min_window, max_window + 1), moving_avg_losses)\n",
    "                    ax3.set_xlabel(\"Window Length\")\n",
    "                    ax3.set_ylabel(\"Moving Average Loss\")\n",
    "                    ax3.set_title(\"Moving Average Loss for Each Window Length\")\n",
    "\n",
    "                display(fig)\n",
    "                clear_output(wait=True)\n",
    "\n",
    "        end = time.time()\n",
    "\n",
    "        # Validation\n",
    "        if epoch % validation_frequency == 0:\n",
    "            loss_list_validation = []\n",
    "            for i, batch in enumerate(data_loader_validation):\n",
    "                curr_treatments = batch['current_treatments']\n",
    "                vitals_or_prev_outputs = []\n",
    "                # vitals_or_prev_outputs.append(batch['vitals']) if self.has_vitals else None\n",
    "                # if self.autoregressive else None\n",
    "                vitals_or_prev_outputs.append(batch['prev_outputs'])\n",
    "                vitals_or_prev_outputs = torch.cat(vitals_or_prev_outputs, dim=-1)\n",
    "                static_features = batch['static_features']\n",
    "                outputs = batch['outputs']\n",
    "\n",
    "                batch = torch.cat((vitals_or_prev_outputs, curr_treatments), dim=-1)\n",
    "                batch = torch.cat((batch, static_features.unsqueeze(\n",
    "                    1).expand(-1, batch.size(1), -1)), dim=-1)\n",
    "                batch = torch.cat((batch, outputs), dim=-1)\n",
    "\n",
    "                batch = batch.to(device)\n",
    "                # if batch_embedder is not None:\n",
    "                #     batch = batch_embedder(batch)\n",
    "                if i % validation_prp == 0:\n",
    "                    predicted_noise, noise, noise_mask = model(batch)\n",
    "                    loss = loss_func(predicted_noise, noise, noise_mask)\n",
    "                    loss_list_validation.append(loss.item())\n",
    "\n",
    "            val_loss = np.mean(loss_list_validation)\n",
    "\n",
    "        total_time_end = time.time()\n",
    "        total_time = total_time_end - total_time_start\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Epoch {epoch} completed in {end - start} seconds, Loss: {epoch_loss}\")\n",
    "            print(f\"Validation Loss: {val_loss}\")\n",
    "    \n",
    "    print(f\"Took {total_time} seconds for {epoch} epochs.\")\n",
    "            \n",
    "\n",
    "\n",
    "    return model, loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diffusion_imputer = diffusion_imputation(\n",
    "#     emb_dim=128,\n",
    "#     # strategy=\"forecasting_last_n_time\",\n",
    "#     # strategy=\"random\",\n",
    "#     # missing_prp=0.5,\n",
    "#     # strategy='selected_features',\n",
    "#     strategy=\"selected_features_last_n_time\",\n",
    "#     last_n_time=1,\n",
    "#     features_to_impute=[0, 1, 2, 3],\n",
    "#     # excluded_features = [i for i in range(6)], #[2],#[0,1,2,3,5], #for the embedded stock names which we don't need to predict\n",
    "#     # strategy='selected_features_and_selected_features_after_time',\n",
    "#     # features_to_impute_completely=[2],\n",
    "#     # features_to_impute_after_time=[3],\n",
    "#     num_residual_layers=2,\n",
    "#     diffusion_steps=50,\n",
    "#     diffusion_beta_schedule=\"quadratic\",\n",
    "#     num_heads=8,\n",
    "#     kernel_size=(1, 1),\n",
    "#     ff_dim=512,\n",
    "#     num_cells=1,\n",
    "#     dropout=0,\n",
    "#     # csdi, csdi_moded_transformer, rsa, rsa_moded_transformer, moded_transformer_alone, rsa_csdi\n",
    "#     method=\"rsa_moded_transformer\",\n",
    "#     device=\"cuda\",\n",
    "# )\n",
    "\n",
    "# # data_embedder = DataEmbedder(\n",
    "# #     categorical_indices_sizes, numerical_indices, training_data_tensor\n",
    "# # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train(\n",
    "#     diffusion_imputer,\n",
    "#     train_dataloader,\n",
    "#     val_dataloader,\n",
    "#     num_gpus=1,\n",
    "#     batch_embedder=None,\n",
    "#     gradient_clip=True,\n",
    "#     windowed_mode=True,\n",
    "#     window_mode=\"uniform\",\n",
    "#     window_start_mode=\"fixed\",\n",
    "#     # train_on_all_every=100,\n",
    "#     min_window=2,\n",
    "#     max_window=60,\n",
    "#     device=\"cuda\",\n",
    "#     epochs=100,\n",
    "#     lr=1e-4,\n",
    "#     annealing_mode=True,\n",
    "#     annealing_window=2,\n",
    "#     annealing_multiplier=1.0,\n",
    "#     annealing_ratio=0.5,\n",
    "#     annealing_minimum=1e-7,\n",
    "#     loss_func=diffusion_imputer.loss_func,\n",
    "#     weighted_loss_func=diffusion_imputer.weighted_loss_func,\n",
    "#     validation_frequency=5,\n",
    "#     validation_prp=1,\n",
    "#     verbose=False,\n",
    "#     plot_every=20\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_hybrid(\n",
    "#     diffusion_imputer,\n",
    "#     hybrid_model,\n",
    "#     train_loader,\n",
    "#     val_loader,\n",
    "#     batch_embedder = embedder,\n",
    "#     epochs = 20,\n",
    "#     lr = 0.001,\n",
    "#     annealing_window = 5,\n",
    "#     annealing_multiplier = 1,\n",
    "#     loss_func = diffusion_imputer.loss_func,\n",
    "#     hybrid_loss_func = hybrid_model.loss_func,\n",
    "#     hybrid_start_epoch = 0,\n",
    "#     hybrid_every_n_epoch = 5,\n",
    "#     validation_frequency=2,\n",
    "#     validation_prp=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import mlflow\n",
    "# import time\n",
    "# import math\n",
    "# import statistics\n",
    "# from torch.utils.data import DataLoader\n",
    "# from itertools import chain\n",
    "# from omegaconf import OmegaConf\n",
    "\n",
    "\n",
    "# def distill_teacher_to_student(teacher, student, dataloader, loss_fn, num_epochs, lr, device=\"cuda\", gradient_clip=1.0,\n",
    "#                                amp_scale=True, plot_every=10, plot_display_window=100, moving_avg_window=10):\n",
    "#     \"\"\"\n",
    "#     Distills a teacher diffusion imputation model (with many diffusion steps)\n",
    "#     into a student model (with fewer steps) using a simple MSE loss.\n",
    "\n",
    "#     The teacher is kept in evaluation mode while the student is trained.\n",
    "#     \"\"\"\n",
    "\n",
    "#     teacher.eval()\n",
    "#     student.train()\n",
    "#     teacher.to(device)\n",
    "#     student.to(device)\n",
    "\n",
    "#     optimizer = torch.optim.Adam(\n",
    "#         student.parameters(),\n",
    "#         lr=lr\n",
    "#     )\n",
    "\n",
    "#     # Initialize dynamic plotting\n",
    "#     loss_list = []\n",
    "#     fig, ax1 = plt.subplots(1, 1, figsize=(10, 6))\n",
    "#     epoch_loss_list = []\n",
    "\n",
    "#     if mlflow.active_run() is not None:\n",
    "#         mlflow.log_param(\"distillation_teacher_steps\", teacher.diffusion_steps)\n",
    "#         mlflow.log_param(\"distillation_student_steps\", student.diffusion_steps)\n",
    "\n",
    "#     if amp_scale:\n",
    "#         scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         epoch_loss = 0.0\n",
    "#         start = time.time()\n",
    "#         for i, batch in enumerate(dataloader):\n",
    "#             sequence_lengths = batch['sequence_lengths']\n",
    "#             student.sequence_length = sequence_lengths\n",
    "#             teacher.sequence_length = sequence_lengths\n",
    "#             if 'stabilized_weights' in batch:\n",
    "#                 stabilized_weights = batch['stabilized_weights']\n",
    "#             curr_treatments = batch['current_treatments']\n",
    "#             vitals_or_prev_outputs = []\n",
    "#             vitals_or_prev_outputs.append(batch['prev_outputs'])\n",
    "#             vitals_or_prev_outputs = torch.cat(vitals_or_prev_outputs, dim=-1)\n",
    "#             static_features = batch['static_features']\n",
    "#             outputs = batch['outputs']\n",
    "\n",
    "#             batch = torch.cat(\n",
    "#                 (vitals_or_prev_outputs, curr_treatments), dim=-1)\n",
    "\n",
    "#             batch = torch.cat((batch, static_features.unsqueeze(\n",
    "#                 1).expand(-1, batch.size(1), -1)), dim=-1)\n",
    "#             batch = torch.cat((batch, outputs), dim=-1)\n",
    "#             if batch.shape[0] == 0:\n",
    "#                 continue\n",
    "#             batch = batch.to(device)\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "#             if amp_scale:\n",
    "#                 with torch.cuda.amp.autocast():\n",
    "#                     # Teacher output (no grad)\n",
    "#                     with torch.no_grad():\n",
    "#                         teacher_output, teacher_noise, teacher_noise_mask = teacher(\n",
    "#                             batch)\n",
    "#                     student_output, student_noise, student_noise_mask = student(\n",
    "#                         batch)\n",
    "#                     loss = loss_fn(student_output, teacher_output,\n",
    "#                                    student_noise_mask)\n",
    "#                     if math.isnan(loss.item()):\n",
    "#                         continue\n",
    "#                 scaler.scale(loss).backward()\n",
    "#                 if gradient_clip is not None:\n",
    "#                     scaler.unscale_(optimizer)\n",
    "#                     torch.nn.utils.clip_grad_norm_(\n",
    "#                         student.parameters(), gradient_clip)\n",
    "#                 else:\n",
    "#                     scaler.unscale_(optimizer)\n",
    "#                 optimizer.step()\n",
    "#                 scaler.update()\n",
    "#                 loss_list.append(loss.item())\n",
    "#             else:\n",
    "#                 with torch.no_grad():\n",
    "#                     teacher_output, teacher_noise, teacher_noise_mask = teacher(\n",
    "#                         batch)\n",
    "#                 student_output, student_noise, student_noise_mask = student(\n",
    "#                     batch)\n",
    "#                 loss = loss_fn(student_output, teacher_output,\n",
    "#                                student_noise_mask)\n",
    "#                 if math.isnan(loss.item()):\n",
    "#                     continue\n",
    "#                 loss.backward()\n",
    "#                 if gradient_clip is not None:\n",
    "#                     torch.nn.utils.clip_grad_norm_(\n",
    "#                         model.parameters(), gradient_clip)\n",
    "#                 optimizer.step()\n",
    "#                 loss_list.append(loss.item())\n",
    "\n",
    "#             epoch_loss = sum(loss_list[-len(dataloader):]) / len(dataloader)\n",
    "#             epoch_loss_list.append(epoch_loss)\n",
    "\n",
    "#             # Dynamic plot update focusing on recent losses\n",
    "#             if i % plot_every == 0:\n",
    "#                 ax1.clear()\n",
    "#                 # Only consider the recent losses for plotting (e.g., last 100)\n",
    "#                 display_window = plot_display_window\n",
    "#                 recent_losses = loss_list[-display_window:] if len(\n",
    "#                     loss_list) > display_window else loss_list\n",
    "#                 # Dynamically set y-limits based on recent loss values\n",
    "#                 if recent_losses:\n",
    "#                     min_loss = min(recent_losses)\n",
    "#                     max_loss = max(recent_losses)\n",
    "#                     margin = 0.1 * \\\n",
    "#                         (max_loss - min_loss) if max_loss != min_loss else 0.1 * max_loss\n",
    "#                     ax1.set_ylim(min_loss - margin, max_loss + margin)\n",
    "#                 ax1.plot(recent_losses)\n",
    "#                 if len(recent_losses) > moving_avg_window:\n",
    "#                     moving_avg = np.convolve(recent_losses, np.ones(\n",
    "#                         (moving_avg_window,)) / moving_avg_window, mode='valid')\n",
    "#                     x = np.arange(moving_avg_window - 1, len(recent_losses))\n",
    "#                     ax1.plot(x, moving_avg)\n",
    "#                     ax1.text(len(recent_losses) - 1, moving_avg[-1],\n",
    "#                              f\"{moving_avg[-1]:.3e}\")\n",
    "#                 if epoch_loss_list:\n",
    "#                     ax1.text(\n",
    "#                         0.1, 0.9, f\"Epoch: {epoch} | LR: {optimizer.param_groups[0]['lr']:.2e}\", transform=ax1.transAxes)\n",
    "#                 ax1.text(\n",
    "#                     0.1, 0.8, f\"Loss: {epoch_loss_list[-1]:.3e}\", transform=ax1.transAxes)\n",
    "#                 ax1.text(\n",
    "#                     0.1, 0.7, f\"Time per step: {((time.time() - start) / (i + 1)):.2f} s | Time per epoch: {((time.time() - start) / (i + 1) * len(dataloader)):.2f} s\", transform=ax1.transAxes)\n",
    "#                 ax1.text(\n",
    "#                     0.1, 0.6, f\"Time till finish: {((time.time() - start) / (i + 1) * len(dataloader) * (num_epochs - epoch)) / 60:.2f} min\", transform=ax1.transAxes)\n",
    "#                 display(fig)\n",
    "#                 clear_output(wait=True)\n",
    "\n",
    "#         avg_loss = epoch_loss / len(dataloader)\n",
    "#         elapsed = time.time() - start\n",
    "#         # print(\n",
    "#         #     f\"Distillation Epoch {epoch+1}/{num_epochs}: Avg Loss = {avg_loss:.4f} (Epoch time: {elapsed:.2f} s)\")\n",
    "\n",
    "#         # Log distillation loss to MLflow if active\n",
    "#         if mlflow.active_run() is not None:\n",
    "#             mlflow.log_metric(\"distillation_epoch_loss\", avg_loss, step=epoch)\n",
    "\n",
    "#     return student\n",
    "\n",
    "\n",
    "# # Suppose your existing diffusion_imputer is the teacher.\n",
    "# teacher = diffusion_imputer\n",
    "# # Create a student model with similar architecture but fewer diffusion steps.\n",
    "# diffusion_imputer_student = diffusion_imputation(\n",
    "#     emb_dim=64,\n",
    "#     strategy=\"selected_features_sequence_length\",\n",
    "#     features_to_impute=[6],\n",
    "#     num_residual_layers=1,            # Possibly fewer residual layers\n",
    "#     # Fewer diffusion steps (e.g., 20 instead of 1000)\n",
    "#     diffusion_steps=50,\n",
    "#     diffusion_beta_schedule=\"quadratic\",\n",
    "#     num_heads=8,\n",
    "#     kernel_size=(1, 1),\n",
    "#     ff_dim=512,\n",
    "#     num_cells=1,\n",
    "#     dropout=0,\n",
    "#     method=\"csdi\",\n",
    "#     device=\"cuda\",\n",
    "# )\n",
    "\n",
    "# # Define a dataloader for distillation (could be the same as train_dataloader)\n",
    "# distill_dataloader = DataLoader(\n",
    "#     weighted_causal_diff.dataset_collection.val_f,\n",
    "#     batch_size=1000,\n",
    "#     shuffle=True,\n",
    "#     num_workers=1,\n",
    "#     pin_memory=True\n",
    "# )\n",
    "\n",
    "# distill_loss_fn = teacher.loss_func  # nn.MSELoss()\n",
    "\n",
    "# diffusion_imputer_student = distill_teacher_to_student(\n",
    "#     teacher, diffusion_imputer_student, distill_dataloader,\n",
    "#     distill_loss_fn, num_epochs=20000, lr=5e-5, gradient_clip=5.0\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import copy\n",
    "import logging\n",
    "import requests\n",
    "import zipfile\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from einops import rearrange\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "\n",
    "from src.data import RealDatasetCollection, SyntheticDatasetCollection\n",
    "from src.data.cancer_sim.dataset import SyntheticCancerDataset\n",
    "from src.models import TimeVaryingCausalModel\n",
    "from src.models.utils import (\n",
    "    grad_reverse,\n",
    "    BRTreatmentOutcomeHead,\n",
    "    AlphaRise,\n",
    "    clip_normalize_stabilized_weights,\n",
    ")\n",
    "from src.models.utils_lstm import VariationalLSTM\n",
    "from copy import deepcopy\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Data embedding before feeding into the model\n",
    "\n",
    "# This will first one-hot encode all the categorical features and then embed them to n columns. The resulting columns will then be concatenated with the numerical features. The result will then be used to create the torch tensor for the model. The torch tensor will be shaped as (Cases, Time, Features).\n",
    "\n",
    "# The input data will be a dataframe like this:\n",
    "\n",
    "\n",
    "class DataEmbedder(nn.Module):\n",
    "    def __init__(self, categorical_indices_sizes, numerical_indices, dataset):\n",
    "        super(DataEmbedder, self).__init__()\n",
    "        # dictionary with feature name, and a list of index and size\n",
    "        self.categoricals = categorical_indices_sizes\n",
    "        self.numerics = numerical_indices  # dictionary with feature name and index\n",
    "        self.embeddings = nn.ModuleDict()\n",
    "        self.mapping_dicts = {}\n",
    "\n",
    "        # Initialize embeddings and mapping dictionaries\n",
    "        for key in self.categoricals:\n",
    "            unique_values = np.unique(dataset[:, :, self.categoricals[key][0]])\n",
    "            self.mapping_dicts[key] = {\n",
    "                name: idx for idx, name in enumerate(unique_values)\n",
    "            }\n",
    "            self.embeddings[key] = nn.Embedding(\n",
    "                num_embeddings=len(unique_values),\n",
    "                embedding_dim=self.categoricals[key][1],\n",
    "            )\n",
    "            print(\n",
    "                f\"Feature: {key}, Categories: {len(unique_values)}, Embedding Size: {self.categoricals[key][1]}\"\n",
    "            )\n",
    "\n",
    "    def forward(self, dataset):\n",
    "        # Apply embeddings to the categorical indices\n",
    "        if len(self.categoricals) == 0:\n",
    "            return dataset\n",
    "        else:\n",
    "            embedded_features = []\n",
    "            for key in self.categoricals:\n",
    "                # Map the categorical values to their corresponding indices\n",
    "                indices = dataset[:, :,\n",
    "                                  self.categoricals[key][0]].cpu().numpy()\n",
    "                mapped_indices = np.vectorize(\n",
    "                    self.mapping_dicts[key].get)(indices)\n",
    "                mapped_indices = torch.tensor(\n",
    "                    mapped_indices, dtype=torch.long, device=dataset.device\n",
    "                )\n",
    "                # print(f\"Feature: {key}, Mapped Indices: {mapped_indices}\")\n",
    "                embedded_features.append(self.embeddings[key](mapped_indices))\n",
    "\n",
    "            embedded_features = torch.cat(embedded_features, dim=-1)\n",
    "\n",
    "            numeric_features = dataset[:, :, list(\n",
    "                self.numerics.values())].float()\n",
    "\n",
    "            # Concatenate the embedded features with the numerical data\n",
    "            result = torch.cat([embedded_features, numeric_features], dim=-1)\n",
    "\n",
    "            feature_count_embedded = len(self.numerics) + sum(\n",
    "                [self.categoricals[key][1] for key in self.categoricals]\n",
    "            )\n",
    "\n",
    "            result = result.reshape(\n",
    "                dataset.shape[0], -1, feature_count_embedded)\n",
    "\n",
    "            return result\n",
    "\n",
    "class moded_TimesSeriesAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    A module that computes multi-head attention given query, key, and value tensors for time series data of shape (b, t, f, e)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, num_heads: int):\n",
    "        \"\"\"\n",
    "        Constructor.\n",
    "\n",
    "        Inputs:\n",
    "        - input_dim: Dimension of the input query, key, and value. We assume they all have\n",
    "          the same dimensions. This is basically the dimension of the embedding.\n",
    "        - num_heads: Number of attention heads\n",
    "        \"\"\"\n",
    "        super(moded_TimesSeriesAttention, self).__init__()\n",
    "\n",
    "        assert embed_dim % num_heads == 0\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dim_per_head = embed_dim // num_heads\n",
    "\n",
    "        self.linear_query = nn.Linear(embed_dim, embed_dim)\n",
    "        self.linear_key = nn.Linear(embed_dim, embed_dim)\n",
    "        self.linear_value = nn.Linear(\n",
    "            embed_dim, embed_dim\n",
    "        )  # (self.num_heads * self.dim_per_head * self.dim_per_head))\n",
    "        self.output_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        self.softmax = nn.Softmax2d()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        query: torch.Tensor,\n",
    "        key: torch.Tensor,\n",
    "        value: torch.Tensor,\n",
    "        mask: torch.Tensor = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Compute the attended feature representations.\n",
    "\n",
    "        Inputs:\n",
    "        - query: Tensor of the shape BxTxFXE, where B is the batch size, T is the time dimension, F is the feature dimension,\n",
    "        and E is the embedding dimension\n",
    "        - key: Tensor of the shape BxTxFXE\n",
    "        - value: Tensor of the shape BxTxFXE\n",
    "        - mask: Tensor indicating where the attention should *not* be performed\n",
    "        \"\"\"\n",
    "        b = query.shape[0]\n",
    "        t = query.shape[1]\n",
    "        f = query.shape[2]\n",
    "        e = query.shape[3]\n",
    "        d = self.dim_per_head\n",
    "        h = self.num_heads\n",
    "\n",
    "        query_linear = self.linear_query(query)\n",
    "        key_linear = self.linear_key(key)\n",
    "        value_linear = self.linear_value(value)\n",
    "\n",
    "        query_reshaped = query_linear.reshape(\n",
    "            b, t, f, self.num_heads, self.dim_per_head\n",
    "        )\n",
    "        key_reshaped = key_linear.reshape(\n",
    "            b, t, f, self.num_heads, self.dim_per_head)\n",
    "        value_reshaped = value_linear.reshape(\n",
    "            b, t, f, self.num_heads, self.dim_per_head\n",
    "        )  # , self.dim_per_head)\n",
    "\n",
    "        query_reshaped = query_reshaped.permute(0, 3, 1, 2, 4)  # BxHxTxFxD\n",
    "        key_reshaped = key_reshaped.permute(0, 3, 1, 2, 4)  # BxHxTxFxD\n",
    "        value_reshaped = value_reshaped.permute(\n",
    "            0, 3, 1, 2, 4)  # , 5) # BxHxTxFxDxD\n",
    "\n",
    "        kq = torch.einsum(\"bhtfd,bhxyd->bhtfxy\", key_reshaped, query_reshaped)\n",
    "\n",
    "        dot_prod_scores = kq / math.sqrt(self.dim_per_head)\n",
    "\n",
    "        # softmax across last 2 features (use softmax2d)\n",
    "        dot_prod_scores = dot_prod_scores.reshape(b * h, t * f, t, f)\n",
    "        dot_prod_scores = self.softmax(dot_prod_scores)\n",
    "        dot_prod_scores = dot_prod_scores.reshape(b, h, t, f, t, f)\n",
    "\n",
    "        out = torch.einsum(\"bhtfxy,bhtfd->bhtfd\",\n",
    "                           dot_prod_scores, value_reshaped)\n",
    "        out = out.permute(0, 2, 3, 1, 4).reshape(b, t, f, e)\n",
    "        out = self.output_linear(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "class moded_TransformerEncoderCell(nn.Module):\n",
    "    \"\"\"\n",
    "    A single cell (unit) for the Transformer encoder.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, num_heads: int, ff_dim: int, dropout: float):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - embed_dim: embedding dimension for each element in the time series data\n",
    "        - num_heads: Number of attention heads in a multi-head attention module\n",
    "        - ff_dim: The hidden dimension for a feedforward network\n",
    "        - dropout: Dropout ratio for the output of the multi-head attention and feedforward\n",
    "          modules.\n",
    "        \"\"\"\n",
    "        super(moded_TransformerEncoderCell, self).__init__()\n",
    "\n",
    "        self.time_series_attention = moded_TimesSeriesAttention(\n",
    "            embed_dim, num_heads)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(embed_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.linear1 = nn.Linear(embed_dim, ff_dim)\n",
    "        self.linear2 = nn.Linear(ff_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor = None):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - x: Tensor of the shape BxTxFXE, where B is the batch size, T is the time dimension, F is the feature dimension,\n",
    "        and E is the embedding dimension\n",
    "        - mask: Tensor for multi-head attention\n",
    "        \"\"\"\n",
    "\n",
    "        attention2 = self.time_series_attention(x, x, x, mask)\n",
    "        attention = x + self.dropout1(attention2)\n",
    "        attention = self.layer_norm(attention)\n",
    "\n",
    "        attention2 = self.linear2(\n",
    "            self.dropout(self.activation(self.linear1(attention)))\n",
    "        )\n",
    "        attention = attention + self.dropout2(attention2)\n",
    "        attention = self.layer_norm(attention)\n",
    "\n",
    "        return attention\n",
    "class moded_TransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A full encoder consisting of a set of TransformerEncoderCell.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int,\n",
    "        num_heads: int,\n",
    "        ff_dim: int,\n",
    "        num_cells: int,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - embed_dim: embedding dimension for each element in the time series data\n",
    "        - num_heads: Number of attention heads in a multi-head attention module\n",
    "        - ff_dim: The hidden dimension for a feedforward network\n",
    "        - num_cells: Number of time series attention cells in the encoder\n",
    "        - dropout: Dropout ratio for the output of the multi-head attention and feedforward\n",
    "          modules.\n",
    "        \"\"\"\n",
    "        super(moded_TransformerEncoder, self).__init__()\n",
    "\n",
    "        self.norm = None\n",
    "\n",
    "        self.encoder_modules = nn.ModuleList(\n",
    "            moded_TransformerEncoderCell(embed_dim, num_heads, ff_dim, dropout)\n",
    "            for _ in range(num_cells)\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor = None):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - x: Tensor of the shape BxTxFXE, where B is the batch size, T is the time dimension, F is the feature dimension,\n",
    "        and E is the embedding dimension\n",
    "        - mask: Tensor for multi-head attention\n",
    "\n",
    "        Return:\n",
    "        - y: Tensor of the shape BxTxFXE\n",
    "        \"\"\"\n",
    "\n",
    "        # run encoder modules and add residual connections\n",
    "        for encoder_module in self.encoder_modules:\n",
    "            x = encoder_module(x, mask)\n",
    "\n",
    "        y = x\n",
    "\n",
    "        return y\n",
    "\n",
    "## RSA\n",
    "class TimesSeriesAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_in,\n",
    "        d_out,\n",
    "        nh=8,\n",
    "        dk=0,\n",
    "        dv=0,\n",
    "        dd=0,\n",
    "        kernel_size=(3, 7),\n",
    "        stride=(1, 1, 1),\n",
    "        kernel_type=\"VplusR\",  # ['V', 'R', 'VplusR']\n",
    "        feat_type=\"VplusR\",  # ['V', 'R', 'VplusR']\n",
    "    ):\n",
    "        super(TimesSeriesAttention, self).__init__()\n",
    "\n",
    "        self.d_in = d_in\n",
    "        self.d_out = d_out\n",
    "        self.nh = nh\n",
    "        self.dv = dv = d_out // nh if dv == 0 else dv\n",
    "        self.dk = dk = dv if dk == 0 else dk\n",
    "        self.dd = dd = dk if dd == 0 else dd\n",
    "\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.kernel_type = kernel_type\n",
    "        self.feat_type = feat_type\n",
    "\n",
    "        assert self.kernel_type in [\n",
    "            \"V\",\n",
    "            \"R\",\n",
    "            \"VplusR\",\n",
    "        ], \"Not implemented involution type: {}\".format(self.kernel_type)\n",
    "        assert self.feat_type in [\n",
    "            \"V\",\n",
    "            \"R\",\n",
    "            \"VplusR\",\n",
    "        ], \"Not implemented feature type: {}\".format(self.feat_type)\n",
    "\n",
    "        # print(\"d_in: {}, d_out: {}, nh: {}, dk: {}, dv: {}, dd:{}, kernel_size: {}, kernel_type: {}, feat_type: {}\"\n",
    "        #       .format(d_in, d_out, nh, dk, dv, self.dd, kernel_size, kernel_type, feat_type))\n",
    "\n",
    "        self.ksize = ksize = kernel_size[0] * kernel_size[1]\n",
    "        self.pad = pad = tuple(k // 2 for k in kernel_size)\n",
    "\n",
    "        # hidden dimension\n",
    "        d_hid = nh * dk + dv if self.kernel_type == \"V\" else nh * dk + dk + dv\n",
    "\n",
    "        # Linear projection\n",
    "        # self.projection = nn.Conv2d(d_in, d_hid, 1, bias=False)\n",
    "        self.projection_linear = nn.Sequential(\n",
    "            nn.Linear(d_in, d_hid, bias=False),\n",
    "            nn.SiLU(inplace=True),\n",
    "            nn.Linear(d_hid, d_hid, bias=False),\n",
    "        )\n",
    "\n",
    "        # Intervolution Kernel\n",
    "        if self.kernel_type == \"V\":\n",
    "            self.H2 = nn.Conv2d(1, dd, kernel_size,\n",
    "                                padding=self.pad, bias=False)\n",
    "        elif self.kernel_type == \"R\":\n",
    "            self.H1 = nn.Conv2d(\n",
    "                dk, dk * dd, kernel_size, padding=self.pad, groups=dk, bias=False\n",
    "            )\n",
    "            self.H2 = nn.Conv2d(1, dd, kernel_size,\n",
    "                                padding=self.pad, bias=False)\n",
    "        elif self.kernel_type == \"VplusR\":\n",
    "            self.P1 = nn.Parameter(\n",
    "                torch.randn(dk, dd).unsqueeze(0) * np.sqrt(1 / (ksize * dd)),\n",
    "                requires_grad=True,\n",
    "            )\n",
    "            self.H1 = nn.Conv2d(\n",
    "                dk, dk * dd, kernel_size, padding=self.pad, groups=dk, bias=False\n",
    "            )\n",
    "            self.H2 = nn.Conv2d(1, dd, kernel_size,\n",
    "                                padding=self.pad, bias=False)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        # Feature embedding layer\n",
    "        if self.feat_type == \"V\":\n",
    "            pass\n",
    "        elif self.feat_type == \"R\":\n",
    "            self.G = nn.Conv2d(1, dv, kernel_size,\n",
    "                               padding=self.pad, bias=False)\n",
    "        elif self.feat_type == \"VplusR\":\n",
    "            self.G = nn.Conv2d(1, dv, kernel_size,\n",
    "                               padding=self.pad, bias=False)\n",
    "            self.I = nn.Parameter(\n",
    "                torch.eye(dk).unsqueeze(0), requires_grad=True)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        # Downsampling layer\n",
    "        if max(self.stride) > 1:\n",
    "            self.avgpool = nn.AvgPool2d(\n",
    "                kernel_size=(1, 3), stride=(1, 2), padding=(0, 1)\n",
    "            )\n",
    "\n",
    "    def L2norm(self, x, d=1):\n",
    "        eps = 1e-6\n",
    "        norm = x**2\n",
    "        norm = norm.sum(dim=d, keepdim=True) + eps\n",
    "        norm = norm ** (0.5)\n",
    "        return x / norm\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # print(x.shape)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        N, C, T, H = x.shape\n",
    "\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "\n",
    "        \"\"\"Linear projection\"\"\"\n",
    "        # x_proj = self.projection(x)\n",
    "        x_proj = self.projection_linear(x)\n",
    "        x_proj = x_proj.permute(0, 3, 1, 2)\n",
    "        # print(x_proj.shape)\n",
    "\n",
    "        if self.kernel_type != \"V\":\n",
    "            q, k, v = torch.split(\n",
    "                x_proj, [self.nh * self.dk, self.dk, self.dv], dim=1)\n",
    "        else:\n",
    "            q, v = torch.split(x_proj, [self.nh * self.dk, self.dv], dim=1)\n",
    "\n",
    "        \"\"\"Normalization\"\"\"\n",
    "        q = rearrange(q, \"b (nh k) t h -> b nh k t h\", k=self.dk)\n",
    "        q = self.L2norm(q, d=2)\n",
    "        q = rearrange(q, \"b nh k t h -> (b t h) nh k\")\n",
    "\n",
    "        v = self.L2norm(v, d=1)\n",
    "\n",
    "        if self.kernel_type != \"V\":\n",
    "            k = self.L2norm(k, d=1)\n",
    "\n",
    "        \"\"\"\n",
    "        q = (b t h) nh k\n",
    "        k = b k t h\n",
    "        v = b v t h\n",
    "        \"\"\"\n",
    "\n",
    "        # Intervolution generation\n",
    "        # Basic kernel\n",
    "        if self.kernel_type == \"V\":\n",
    "            kernel = q\n",
    "        # Relational kernel\n",
    "        else:\n",
    "            K_H1 = self.H1(k)\n",
    "            K_H1 = rearrange(K_H1, \"b (k d) t h-> (b t h) k d\", k=self.dk)\n",
    "\n",
    "            if self.kernel_type == \"VplusR\":\n",
    "                K_H1 = K_H1 + self.P1\n",
    "\n",
    "            kernel = torch.einsum(\n",
    "                \"abc,abd->acd\", q.transpose(1, 2), K_H1\n",
    "            )  # (bth, nh, d)\n",
    "\n",
    "        # feature generation\n",
    "        # Appearance feature\n",
    "        v = rearrange(v, \"b (v 1) t h-> (b v) 1 t h\")\n",
    "\n",
    "        V = self.H2(v)  # (bv, d, t, h)\n",
    "        feature = rearrange(V, \"(b v) d t h -> (b t h) v d\", v=self.dv)\n",
    "\n",
    "        # Relational feature\n",
    "        if self.feat_type in [\"R\", \"VplusR\"]:\n",
    "            V_G = self.G(v)  # (bv, v2, t, h)\n",
    "            V_G = rearrange(V_G, \"(b v) v2 t h -> (b t h) v v2\", v=self.dv)\n",
    "\n",
    "            if self.feat_type == \"VplusR\":\n",
    "                V_G = V_G + self.I\n",
    "\n",
    "            feature = torch.einsum(\"abc,abd->acd\", V_G,\n",
    "                                   feature)  # (bth, v2, d)\n",
    "\n",
    "        # kernel * feat\n",
    "        out = torch.einsum(\"abc,adc->adb\", kernel, feature)  # (bth, nh, v2)\n",
    "\n",
    "        out = rearrange(out, \"(b t h) nh v -> b (nh v) t h\", t=T, h=H)\n",
    "\n",
    "        if max(self.stride) > 1:\n",
    "            out = self.avgpool(out)\n",
    "\n",
    "        out = out.permute(0, 2, 3, 1)\n",
    "\n",
    "        return out\n",
    "class TransformerEncoderCell(nn.Module):\n",
    "    \"\"\"\n",
    "    A single cell (unit) for the Transformer encoder.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, embed_dim: int, num_heads: int, kernel_size, ff_dim: int, dropout: float\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - embed_dim: embedding dimension for each element in the time series data\n",
    "        - num_heads: Number of attention heads in a multi-head attention module\n",
    "        - ff_dim: The hidden dimension for a feedforward network\n",
    "        - dropout: Dropout ratio for the output of the multi-head attention and feedforward\n",
    "          modules.\n",
    "        \"\"\"\n",
    "        super(TransformerEncoderCell, self).__init__()\n",
    "\n",
    "        self.time_series_attention = TimesSeriesAttention(\n",
    "            embed_dim, embed_dim, nh=num_heads, kernel_size=kernel_size\n",
    "        )\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(embed_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.linear1 = nn.Linear(embed_dim, ff_dim)\n",
    "        self.linear2 = nn.Linear(ff_dim, embed_dim)\n",
    "\n",
    "    def forward(self, data: torch.Tensor, embeddings, mask: torch.Tensor = None):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - x: Tensor of the shape BxTxFXE, where B is the batch size, T is the time dimension, F is the feature dimension,\n",
    "        and E is the embedding dimension\n",
    "        - mask: Tensor for multi-head attention\n",
    "        \"\"\"\n",
    "\n",
    "        # attention2 = self.time_series_attention(x, x, x, mask)\n",
    "        attention2 = self.time_series_attention(data)\n",
    "        attention = data + self.dropout1(attention2)\n",
    "        attention = self.layer_norm(attention)\n",
    "\n",
    "        attention2 = self.linear2(\n",
    "            self.dropout(self.activation(self.linear1(attention)))\n",
    "        )\n",
    "        attention = attention + self.dropout2(attention2)\n",
    "        attention = self.layer_norm(attention)\n",
    "\n",
    "        return attention\n",
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A full encoder consisting of a set of TransformerEncoderCell.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int,\n",
    "        num_heads: int,\n",
    "        kernel_size,\n",
    "        ff_dim: int,\n",
    "        num_cells: int,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - embed_dim: embedding dimension for each element in the time series data\n",
    "        - num_heads: Number of attention heads in a multi-head attention module\n",
    "        - ff_dim: The hidden dimension for a feedforward network\n",
    "        - num_cells: Number of time series attention cells in the encoder\n",
    "        - dropout: Dropout ratio for the output of the multi-head attention and feedforward\n",
    "          modules.\n",
    "        \"\"\"\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "\n",
    "        self.norm = None\n",
    "\n",
    "        self.encoder_modules = nn.ModuleList(\n",
    "            TransformerEncoderCell(embed_dim, num_heads,\n",
    "                                   kernel_size, ff_dim, dropout)\n",
    "            for _ in range(num_cells)\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor = None):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - x: Tensor of the shape BxTxFXE, where B is the batch size, T is the time dimension, F is the feature dimension,\n",
    "        and E is the embedding dimension\n",
    "        - mask: Tensor for multi-head attention\n",
    "\n",
    "        Return:\n",
    "        - y: Tensor of the shape BxTxFXE\n",
    "        \"\"\"\n",
    "\n",
    "        # run encoder modules and add residual connections\n",
    "        for encoder_module in self.encoder_modules:\n",
    "            x = encoder_module(x, mask)\n",
    "\n",
    "        y = x\n",
    "\n",
    "        return y\n",
    "## CSDI transformer\n",
    "\n",
    "def get_torch_trans(num_heads=8, num_cells=1, embed_dim=128, ff_dim=512, dropout=0.1):\n",
    "    encoder_layer = nn.TransformerEncoderLayer(\n",
    "        d_model=embed_dim,\n",
    "        nhead=num_heads,\n",
    "        dim_feedforward=ff_dim,\n",
    "        activation=\"gelu\",\n",
    "        dropout=dropout,\n",
    "    )\n",
    "    return nn.TransformerEncoder(encoder_layer, num_layers=num_cells)\n",
    "## Embeddings\n",
    "\n",
    "\n",
    "class ContinuousDiffusionEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_dim, projection_dim=None, max_steps=1000):\n",
    "        super(ContinuousDiffusionEmbedding, self).__init__()\n",
    "        if projection_dim is None:\n",
    "            projection_dim = embedding_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.projection1 = nn.Linear(embedding_dim, projection_dim)\n",
    "        self.projection2 = nn.Linear(projection_dim, embedding_dim)\n",
    "        # maximum steps expected (for normalization)\n",
    "        self.max_steps = max_steps\n",
    "\n",
    "    def forward(self, diffusion_step, data, device=\"cpu\"):\n",
    "        # Ensure diffusion_step is on the correct device.\n",
    "        diffusion_step = diffusion_step.to(device)\n",
    "\n",
    "        # If diffusion_step is a scalar or a 1-element tensor, expand it to match the batch size.\n",
    "        if diffusion_step.dim() == 0 or diffusion_step.numel() == 1:\n",
    "            diffusion_step = diffusion_step.expand(data.shape[0])\n",
    "\n",
    "        # Normalize diffusion step to [0, 1]\n",
    "        t_normalized = diffusion_step.float() / self.max_steps  # shape: (batch,) or (1,)\n",
    "\n",
    "        # Use half for sine and half for cosine.\n",
    "        dim = self.embedding_dim // 2\n",
    "        frequencies = 10.0 ** (torch.arange(dim, device=device,\n",
    "                               dtype=torch.float) / (dim - 1) * 4.0)\n",
    "\n",
    "        # Compute angles: (batch, dim)\n",
    "        angles = t_normalized.unsqueeze(1) * frequencies.unsqueeze(0)\n",
    "\n",
    "        # Compute sine and cosine parts and concatenate to get (batch, embedding_dim)\n",
    "        embedding = torch.cat([torch.sin(angles), torch.cos(angles)], dim=1)\n",
    "\n",
    "        # Pass through projection layers with SiLU activation.\n",
    "        embedding = self.projection1(embedding)\n",
    "        embedding = F.silu(embedding)\n",
    "        embedding = self.projection2(embedding)\n",
    "        embedding = F.silu(embedding)\n",
    "\n",
    "        # Expand the embedding to match the shape of the input data.\n",
    "        # Assume data shape is (b, t, f, e)\n",
    "        embedding = embedding.unsqueeze(1).unsqueeze(1)  # shape: (b, 1, 1, e)\n",
    "        embedding = embedding.expand(-1, data.shape[1], data.shape[2], -1)\n",
    "        return embedding\n",
    "\n",
    "# class DiffusionEmbedding(nn.Module):\n",
    "#     def __init__(self, num_steps, embedding_dim, projection_dim=None):\n",
    "#         super(DiffusionEmbedding, self).__init__()\n",
    "#         if projection_dim is None:\n",
    "#             projection_dim = embedding_dim\n",
    "#         self.register_buffer(\n",
    "#             \"embedding\",\n",
    "#             self._build_embedding(num_steps, embedding_dim / 2),\n",
    "#             persistent=False,\n",
    "#         )\n",
    "#         self.projection1 = nn.Linear(embedding_dim, projection_dim)\n",
    "#         self.projection2 = nn.Linear(projection_dim, embedding_dim)\n",
    "\n",
    "#     def forward(self, diffusion_step, data, device=\"cpu\"):\n",
    "#         x = self.embedding[diffusion_step]\n",
    "#         x = self.projection1(x)\n",
    "#         x = F.silu(x)\n",
    "#         x = self.projection2(x)\n",
    "#         x = F.silu(x)\n",
    "#         x = torch.zeros(data.shape).to(device) + x.unsqueeze(1).unsqueeze(1)\n",
    "#         return x\n",
    "\n",
    "#     def _build_embedding(self, num_steps, dim=64):\n",
    "#         steps = torch.arange(num_steps).unsqueeze(1)  # (T,1)\n",
    "#         frequencies = 10.0 ** (torch.arange(dim) / (dim - 1) * 4.0).unsqueeze(\n",
    "#             0\n",
    "#         )  # (1,dim)\n",
    "#         table = steps * frequencies  # (T,dim)\n",
    "#         table = torch.cat(\n",
    "#             [torch.sin(table), torch.cos(table)], dim=1)  # (T,dim*2)\n",
    "#         return table\n",
    "\n",
    "\n",
    "class TimeEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_dim, max_len=10000.0):\n",
    "        super(TimeEmbedding, self).__init__()\n",
    "        self.max_len = max_len\n",
    "        self.learnable = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, embedding_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(embedding_dim, embedding_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, data, device=\"cpu\"):\n",
    "\n",
    "        b, l, f, e = data.shape\n",
    "        pe = None\n",
    "        pe_row = torch.arange(l)\n",
    "\n",
    "        pe = pe_row.unsqueeze(0)\n",
    "        pe = pe.unsqueeze(2)\n",
    "\n",
    "        pe = pe.repeat((b, 1, e))\n",
    "        pe = pe.float()\n",
    "\n",
    "        pe[:, :, 0::2] = torch.sin(\n",
    "            pe[:, :, 0::2] / (self.max_len ** (torch.arange(0, e, 2) / e))\n",
    "        )\n",
    "        pe[:, :, 1::2] = torch.cos(\n",
    "            pe[:, :, 1::2] / (self.max_len ** (torch.arange(0, e, 2) / e))\n",
    "        )\n",
    "\n",
    "        pe = pe.to(device).unsqueeze(2).repeat((1, 1, f, 1))\n",
    "\n",
    "        # pe = torch.arange(l).unsqueeze(0).unsqueeze(-1).unsqueeze(-1).to(device)\n",
    "        # pe = torch.zeros(data.shape).to(device) + pe\n",
    "\n",
    "        # div_term = 1 / torch.pow(\n",
    "        #     self.max_len, torch.arange(0, f, 2) / f\n",
    "        # ).unsqueeze(-1).to(device)\n",
    "\n",
    "        # pe[:, :, 0::2] = torch.sin(pe[:, :, 0::2] * div_term)\n",
    "        # pe[:, :, 1::2] = torch.cos(pe[:, :, 1::2] * div_term)\n",
    "\n",
    "        return self.learnable(pe)\n",
    "\n",
    "\n",
    "class FeatureEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_dim, max_len=10000.0):\n",
    "        super(FeatureEmbedding, self).__init__()\n",
    "        self.max_len = max_len\n",
    "        self.learnable = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, embedding_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(embedding_dim, embedding_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, data, device=\"cpu\"):\n",
    "        b, l, f, e = data.shape\n",
    "\n",
    "        pe = None\n",
    "        pe_row = torch.arange(f)\n",
    "\n",
    "        pe = pe_row.unsqueeze(0)\n",
    "        pe = pe.unsqueeze(2)\n",
    "\n",
    "        pe = pe.repeat((b, 1, e))\n",
    "        pe = pe.float()\n",
    "\n",
    "        pe[:, :, 0::2] = torch.sin(\n",
    "            pe[:, :, 0::2] / (self.max_len ** (torch.arange(0, e, 2) / e))\n",
    "        )\n",
    "        pe[:, :, 1::2] = torch.cos(\n",
    "            pe[:, :, 1::2] / (self.max_len ** (torch.arange(0, e, 2) / e))\n",
    "        )\n",
    "\n",
    "        pe = pe.to(device).unsqueeze(1).repeat((1, l, 1, 1))\n",
    "\n",
    "        # pe = torch.arange(f).unsqueeze(0).unsqueeze(0).unsqueeze(-1).to(device)\n",
    "        # pe = torch.zeros(data.shape).to(device) + pe\n",
    "\n",
    "        # div_term = 1 / torch.pow(\n",
    "        #     self.max_len, torch.arange(0, e, 2) / e\n",
    "        # ).to(device)\n",
    "\n",
    "        # pe[:, :, :, 0::2] = torch.sin(pe[:, :, :, 0::2] * div_term)\n",
    "        # pe[:, :, :, 1::2] = torch.cos(pe[:, :, :, 1::2] * div_term)\n",
    "\n",
    "        return self.learnable(pe)\n",
    "# Residual block\n",
    "\n",
    "def Conv1d_with_init(in_channels, out_channels, kernel_size):\n",
    "    layer = nn.Conv1d(in_channels, out_channels, kernel_size)\n",
    "    nn.init.kaiming_normal_(layer.weight)\n",
    "    return layer\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_heads=8,\n",
    "        num_cells=1,\n",
    "        kernel_size=(3, 7),\n",
    "        embed_dim=128,\n",
    "        ff_dim=512,\n",
    "        dropout=0.1,\n",
    "        method=\"rsa\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.method = method\n",
    "\n",
    "        self.embedding_add = nn.Sequential(\n",
    "            nn.Linear(embed_dim * 4, embed_dim * 4),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(embed_dim * 4, embed_dim * 2),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(embed_dim * 2, embed_dim),\n",
    "        )\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.mid_projection = Conv1d_with_init(embed_dim, 2 * embed_dim, 1)\n",
    "        # nn.Linear(embed_dim, embed_dim*2)\n",
    "        self.output_projection = Conv1d_with_init(embed_dim, 2 * embed_dim, 1)\n",
    "        # self.output_projection = nn.Linear(embed_dim, embed_dim*2)\n",
    "\n",
    "        if method == \"rsa\":\n",
    "            self.feature_and_time_transformer = TransformerEncoder(\n",
    "                embed_dim=embed_dim,\n",
    "                num_heads=num_heads,\n",
    "                kernel_size=kernel_size,\n",
    "                ff_dim=ff_dim,\n",
    "                num_cells=num_cells,\n",
    "                dropout=dropout,\n",
    "            )\n",
    "            self.linear_time_and_feature = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        elif method == \"csdi\":\n",
    "            self.time_layer = get_torch_trans(\n",
    "                num_heads=num_heads,\n",
    "                num_cells=num_cells,\n",
    "                embed_dim=embed_dim,\n",
    "                ff_dim=ff_dim,\n",
    "                dropout=dropout,\n",
    "            )\n",
    "            self.feature_layer = get_torch_trans(\n",
    "                num_heads=num_heads,\n",
    "                num_cells=num_cells,\n",
    "                embed_dim=embed_dim,\n",
    "                ff_dim=ff_dim,\n",
    "                dropout=dropout,\n",
    "            )\n",
    "            self.linear_time = nn.Linear(embed_dim, embed_dim)\n",
    "            self.linear_feature = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        elif method == \"csdi_moded_transformer\":\n",
    "            self.time_layer = get_torch_trans(\n",
    "                num_heads=num_heads,\n",
    "                num_cells=num_cells,\n",
    "                embed_dim=embed_dim,\n",
    "                ff_dim=ff_dim,\n",
    "                dropout=dropout,\n",
    "            )\n",
    "            self.feature_layer = get_torch_trans(\n",
    "                num_heads=num_heads,\n",
    "                num_cells=num_cells,\n",
    "                embed_dim=embed_dim,\n",
    "                ff_dim=ff_dim,\n",
    "                dropout=dropout,\n",
    "            )\n",
    "            self.linear_time = nn.Linear(embed_dim, embed_dim)\n",
    "            self.linear_feature = nn.Linear(embed_dim, embed_dim)\n",
    "            self.feature_and_time_transformer = moded_TransformerEncoder(\n",
    "                embed_dim=embed_dim,\n",
    "                num_heads=num_heads,\n",
    "                ff_dim=ff_dim,\n",
    "                num_cells=num_cells,\n",
    "                dropout=dropout,\n",
    "            )\n",
    "            self.linear_time_and_feature = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        elif method == \"rsa_csdi\":\n",
    "            self.time_layer = get_torch_trans(\n",
    "                num_heads=num_heads,\n",
    "                num_cells=num_cells,\n",
    "                embed_dim=embed_dim,\n",
    "                ff_dim=ff_dim,\n",
    "                dropout=dropout,\n",
    "            )\n",
    "            self.feature_layer = get_torch_trans(\n",
    "                num_heads=num_heads,\n",
    "                num_cells=num_cells,\n",
    "                embed_dim=embed_dim,\n",
    "                ff_dim=ff_dim,\n",
    "                dropout=dropout,\n",
    "            )\n",
    "            self.linear_time = nn.Linear(embed_dim, embed_dim)\n",
    "            self.linear_feature = nn.Linear(embed_dim, embed_dim)\n",
    "            self.feature_and_time_transformer = TransformerEncoder(\n",
    "                embed_dim=embed_dim,\n",
    "                num_heads=num_heads,\n",
    "                kernel_size=kernel_size,\n",
    "                ff_dim=ff_dim,\n",
    "                num_cells=num_cells,\n",
    "                dropout=dropout,\n",
    "            )\n",
    "            self.linear_time_and_feature = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        elif method == \"rsa_moded_transformer\":\n",
    "            self.feature_and_time_transformer = TransformerEncoder(\n",
    "                embed_dim=embed_dim,\n",
    "                num_heads=num_heads,\n",
    "                kernel_size=kernel_size,\n",
    "                ff_dim=ff_dim,\n",
    "                num_cells=num_cells,\n",
    "                dropout=dropout,\n",
    "            )\n",
    "            self.linear_time_and_feature = nn.Linear(embed_dim, embed_dim)\n",
    "            self.moded_feature_and_time_transformer = moded_TransformerEncoder(\n",
    "                embed_dim=embed_dim,\n",
    "                num_heads=num_heads,\n",
    "                ff_dim=ff_dim,\n",
    "                num_cells=num_cells,\n",
    "                dropout=dropout,\n",
    "            )\n",
    "            self.moded_linear_time_and_feature = nn.Linear(\n",
    "                embed_dim, embed_dim)\n",
    "\n",
    "        elif method == \"moded_transformer_alone\":\n",
    "            self.moded_feature_and_time_transformer = moded_TransformerEncoder(\n",
    "                embed_dim=embed_dim,\n",
    "                num_heads=num_heads,\n",
    "                ff_dim=ff_dim,\n",
    "                num_cells=num_cells,\n",
    "                dropout=dropout,\n",
    "            )\n",
    "            self.moded_linear_time_and_feature = nn.Linear(\n",
    "                embed_dim, embed_dim)\n",
    "\n",
    "        elif method == \"simple_neural_network\":\n",
    "            self.linear1 = nn.Linear(embed_dim, embed_dim)\n",
    "            self.linear2 = nn.Linear(embed_dim, ff_dim)\n",
    "            self.linear3 = nn.Linear(ff_dim, ff_dim)\n",
    "            self.linear4 = nn.Linear(ff_dim, embed_dim)\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        logging.info(\"Initializing ResidualBlock with method: %s\", method)\n",
    "\n",
    "    def forward_time(self, y, base_shape):\n",
    "        b, t, f, e = base_shape\n",
    "        y = y.permute(0, 2, 1, 3).reshape(b * f, t, e)\n",
    "        y = self.time_layer(y.permute(1, 0, 2)).permute(1, 0, 2)\n",
    "        y = y.reshape(b, f, t, e).permute(0, 2, 1, 3)\n",
    "        return y\n",
    "\n",
    "    def forward_feature(self, y, base_shape):\n",
    "        b, t, f, e = base_shape\n",
    "        y = y.reshape(b * t, f, e)\n",
    "        y = self.feature_layer(y.permute(1, 0, 2)).permute(1, 0, 2)\n",
    "        y = y.reshape(b, t, f, e)\n",
    "        return y\n",
    "\n",
    "    def forward(self, noised_data, diffusion_emb, time_emb, feature_emb):\n",
    "\n",
    "        logging.info(\"ResidualBlock forward started\")\n",
    "\n",
    "        b, t, f, e = noised_data.shape\n",
    "        base_shape = noised_data.shape\n",
    "\n",
    "        y = torch.stack((noised_data, diffusion_emb,\n",
    "                        time_emb, feature_emb), dim=-1)\n",
    "        y = y.reshape(b, t, f, -1)\n",
    "        y = self.embedding_add(y)\n",
    "        y_resid = y\n",
    "\n",
    "        if self.method == \"rsa\":\n",
    "            y = self.feature_and_time_transformer(y)\n",
    "            y = y.squeeze(-1)\n",
    "            y = self.linear_time_and_feature(y)\n",
    "\n",
    "        elif self.method == \"csdi\":\n",
    "            y = self.forward_time(y, base_shape)\n",
    "            y = self.linear_time(y)\n",
    "            y = (y + y_resid) / math.sqrt(2.0)\n",
    "            y = self.layer_norm(y)\n",
    "            y = self.forward_feature(y, base_shape)\n",
    "            y = self.linear_feature(y)\n",
    "\n",
    "        elif self.method == \"csdi_moded_transformer\":\n",
    "            y = self.forward_time(y, base_shape)\n",
    "            y = self.linear_time(y)\n",
    "            y = (y + y_resid) / math.sqrt(2.0)\n",
    "            y = self.layer_norm(y)\n",
    "            y = self.forward_feature(y, base_shape)\n",
    "            y = self.linear_feature(y)\n",
    "            y = (y + y_resid) / math.sqrt(2.0)\n",
    "            y_resid = y\n",
    "            y = self.layer_norm(y)\n",
    "            y = self.feature_and_time_transformer(y)\n",
    "            y = self.linear_time_and_feature(y)\n",
    "\n",
    "        elif self.method == \"rsa_csdi\":\n",
    "            y = self.forward_time(y, base_shape)\n",
    "            y = self.linear_time(y)\n",
    "            y = (y + y_resid) / math.sqrt(2.0)\n",
    "            y = self.layer_norm(y)\n",
    "            y = self.forward_feature(y, base_shape)\n",
    "            y = self.linear_feature(y)\n",
    "            y = (y + y_resid) / math.sqrt(2.0)\n",
    "            y_resid = y\n",
    "            y = self.layer_norm(y)\n",
    "            y = self.feature_and_time_transformer(y)\n",
    "            y = y.squeeze(-1)\n",
    "            y = self.linear_time_and_feature(y)\n",
    "\n",
    "        elif self.method == \"rsa_moded_transformer\":\n",
    "            y = self.feature_and_time_transformer(y)\n",
    "            y = y.squeeze(-1)\n",
    "            y = self.linear_time_and_feature(y)\n",
    "            y = (y + y_resid) / math.sqrt(2.0)\n",
    "            y = self.layer_norm(y)\n",
    "            y = self.moded_feature_and_time_transformer(y)\n",
    "            y = self.moded_linear_time_and_feature(y)\n",
    "\n",
    "        elif self.method == \"moded_transformer_alone\":\n",
    "            y = self.moded_feature_and_time_transformer(y)\n",
    "            y = self.moded_linear_time_and_feature(y)\n",
    "\n",
    "        elif self.method == \"simple_neural_network\":\n",
    "            y = y.reshape(b, t * f, e)\n",
    "            y = self.linear1(y)\n",
    "            y = F.silu(y)\n",
    "            y = self.linear2(y)\n",
    "            y = F.silu(y)\n",
    "            # y = self.linear3(y)\n",
    "            # y = F.silu(y)\n",
    "            y = self.linear4(y)\n",
    "            y = y.reshape(b, t, f, e)\n",
    "\n",
    "        y = (y + y_resid) / math.sqrt(2.0)\n",
    "        y = self.layer_norm(y)\n",
    "        y = y.permute(0, 3, 1, 2).reshape(b, e, t * f)\n",
    "        y = self.mid_projection(y)\n",
    "        # y = y.permute(0, 3, 2, 1).reshape(b, 2*e, t*f)\n",
    "\n",
    "        gate, filter = torch.chunk(y, 2, dim=1)\n",
    "        y = torch.sigmoid(gate) * torch.tanh(filter)  # (b,e,f*t)\n",
    "        # y = y.permute(0, 2, 1)\n",
    "        y = self.output_projection(y)\n",
    "        # y = y.permute(0, 2, 1)\n",
    "\n",
    "        residual, skip = torch.chunk(y, 2, dim=1)\n",
    "        residual = residual.permute(0, 2, 1)\n",
    "        skip = skip.permute(0, 2, 1)\n",
    "        residual = residual.reshape(base_shape)\n",
    "        skip = skip.reshape(base_shape)\n",
    "\n",
    "        logging.info(\"ResidualBlock forward completed\")\n",
    "\n",
    "        return (noised_data + residual) / math.sqrt(2.0), skip\n",
    "class ModelLoop(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim=128,\n",
    "        # diffusion_steps=1000,\n",
    "        max_steps=1000,\n",
    "        num_heads=8,\n",
    "        kernel_size=(3, 7),\n",
    "        num_cells=1,\n",
    "        num_residual_layers=4,\n",
    "        ff_dim=512,\n",
    "        dropout=0.1,\n",
    "        method=\"rsa\",\n",
    "        device=\"cpu\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self.emb_dim = embed_dim\n",
    "\n",
    "        # self.data_embedding_linear = nn.Sequential(\n",
    "        #     nn.Linear(1, self.emb_dim),\n",
    "        #     nn.SiLU(),\n",
    "        #     nn.Linear(self.emb_dim, self.emb_dim)\n",
    "        # )\n",
    "        # self.x_embedding = nn.Sequential(\n",
    "        #     nn.Linear(1, self.emb_dim),\n",
    "        #     nn.SiLU(),\n",
    "        #     nn.Linear(self.emb_dim, self.emb_dim)\n",
    "        # )\n",
    "\n",
    "        self.data_embedding_linear = Conv1d_with_init(1, self.emb_dim, 1)\n",
    "        self.x_embedding = Conv1d_with_init(2, self.emb_dim, 1)\n",
    "\n",
    "        self.output = Conv1d_with_init(self.emb_dim, 1, 1)\n",
    "        self.output_final = Conv1d_with_init(self.emb_dim, 1, 1)\n",
    "\n",
    "        # self.x_add = nn.Sequential(\n",
    "        #     nn.Linear(embed_dim*num_residual_layers, embed_dim*num_residual_layers),\n",
    "        #     nn.SiLU(),\n",
    "        #     nn.Linear(embed_dim*num_residual_layers, embed_dim)\n",
    "        # )\n",
    "\n",
    "        # self.diffusion_embedding = DiffusionEmbedding(\n",
    "        #     diffusion_steps, embed_dim)\n",
    "        self.diffusion_embedding = ContinuousDiffusionEmbedding(\n",
    "            embedding_dim=embed_dim,\n",
    "            max_steps=max_steps\n",
    "        )\n",
    "        self.time_embedding = TimeEmbedding(embed_dim)\n",
    "        self.feature_embedding = FeatureEmbedding(embed_dim)\n",
    "\n",
    "        self.residual_layers = nn.ModuleList(\n",
    "            ResidualBlock(\n",
    "                num_heads=num_heads,\n",
    "                num_cells=num_cells,\n",
    "                kernel_size=kernel_size,\n",
    "                embed_dim=embed_dim,\n",
    "                ff_dim=ff_dim,\n",
    "                dropout=dropout,\n",
    "                method=method,\n",
    "            )\n",
    "            for _ in range(num_residual_layers)\n",
    "        )\n",
    "\n",
    "        # self.output = nn.Sequential(\n",
    "        #     nn.Linear(self.emb_dim, self.emb_dim),\n",
    "        #     nn.SiLU(),\n",
    "        #     nn.Linear(self.emb_dim, 1)\n",
    "        # )\n",
    "\n",
    "        # self.output_final = nn.Sequential(\n",
    "        #     nn.Linear(self.emb_dim, self.emb_dim),\n",
    "        #     nn.SiLU(),\n",
    "        #     nn.Linear(self.emb_dim, 1)\n",
    "        # )\n",
    "\n",
    "        logging.info(\"Initializing ModelLoop with embed_dim: %s, method: %s\", embed_dim, method)\n",
    "\n",
    "    def forward(self, noised_data, noise_mask, diffusion_t):\n",
    "\n",
    "        logging.info(\"ModelLoop forward: noised_data shape: %s\", noised_data.shape)\n",
    "\n",
    "        b, t, f, a = noised_data.shape\n",
    "\n",
    "        noised_data_reshaped = noised_data.permute(\n",
    "            0, 3, 1, 2).reshape(b, 1, t * f)\n",
    "        noised_data_embedded = (\n",
    "            self.data_embedding_linear(noised_data_reshaped)\n",
    "            .permute(0, 2, 1)\n",
    "            .reshape(b, t, f, self.emb_dim)\n",
    "        )\n",
    "        diffusion_embedding = self.diffusion_embedding(\n",
    "            diffusion_t, noised_data_embedded, device=self.device\n",
    "        )\n",
    "        time_embedding = self.time_embedding(\n",
    "            noised_data_embedded, device=self.device)\n",
    "        feature_embedding = self.feature_embedding(\n",
    "            noised_data_embedded, device=self.device\n",
    "        )\n",
    "\n",
    "        x = noised_data_embedded\n",
    "        skip = []\n",
    "        for layer in self.residual_layers:\n",
    "            x, skip_connection = layer(\n",
    "                x, diffusion_embedding, time_embedding, feature_embedding\n",
    "            )\n",
    "            skip.append(skip_connection)\n",
    "            x = x.permute(0, 3, 1, 2).reshape(b, self.emb_dim, t * f)\n",
    "            x = self.output(x).permute(0, 2, 1).reshape(b, t, f)\n",
    "            x = torch.stack((x, noised_data.squeeze(-1)), dim=-1)\n",
    "            # x = x * noise_mask + noised_data * (1 - noise_mask)\n",
    "            x = x.permute(0, 3, 1, 2).reshape(b, 2, t * f)\n",
    "            x = self.x_embedding(x).permute(\n",
    "                0, 2, 1).reshape(b, t, f, self.emb_dim)\n",
    "\n",
    "        x = torch.sum(torch.stack(skip, dim=-1), dim=-1) / math.sqrt(\n",
    "            len(self.residual_layers)\n",
    "        )\n",
    "        # x = torch.stack(skip, dim = -1).reshape(b, t, f, -1)\n",
    "        # x = self.x_add(x)\n",
    "        x = x.permute(0, 3, 1, 2).reshape(b, self.emb_dim, t * f)\n",
    "        x = self.output_final(x).permute(\n",
    "            0, 2, 1).reshape(b, t, f, 1).squeeze(-1)\n",
    "\n",
    "        logging.info(\"ModelLoop forward: output shape: %s\", x.shape)\n",
    "\n",
    "        return x\n",
    "# Beta Schedules\n",
    "\n",
    "def get_named_beta_schedule(schedule_name, num_diffusion_timesteps):\n",
    "    \"\"\"\n",
    "    Get a pre-defined beta schedule for the given name.\n",
    "\n",
    "    The beta schedule library consists of beta schedules which remain similar\n",
    "    in the limit of num_diffusion_timesteps.\n",
    "    Beta schedules may be added, but should not be removed or changed once\n",
    "    they are committed to maintain backwards compatibility.\n",
    "    \"\"\"\n",
    "    if schedule_name == \"linear\":\n",
    "        # Linear schedule from Ho et al, extended to work for any number of\n",
    "        # diffusion steps.\n",
    "        if num_diffusion_timesteps < 100:\n",
    "            scale = 100 / num_diffusion_timesteps\n",
    "        else:\n",
    "            scale = 1000 / num_diffusion_timesteps\n",
    "        beta_start = scale * 0.0001\n",
    "        beta_end = scale * 0.02\n",
    "        return torch.linspace(beta_start, beta_end, num_diffusion_timesteps)\n",
    "\n",
    "    elif schedule_name == \"cosine\":\n",
    "        return betas_for_alpha_bar(\n",
    "            num_diffusion_timesteps,\n",
    "            lambda t: math.cos((t + 0.008) / 1.008 * math.pi / 2) ** 2,\n",
    "        )\n",
    "\n",
    "    elif schedule_name == \"quadratic\":\n",
    "        scale = 50 / num_diffusion_timesteps\n",
    "        beta_start = scale * 0.0001\n",
    "        beta_end = scale * 0.5\n",
    "        return (\n",
    "            torch.linspace(beta_start**0.5, beta_end**0.5,\n",
    "                           num_diffusion_timesteps) ** 2\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError(f\"unknown beta schedule: {schedule_name}\")\n",
    "\n",
    "\n",
    "def betas_for_alpha_bar(num_diffusion_timesteps, alpha_bar, max_beta=0.999):\n",
    "    \"\"\"\n",
    "    Create a beta schedule that discretizes the given alpha_t_bar function,\n",
    "    which defines the cumulative product of (1-beta) over time from t = [0,1].\n",
    "\n",
    "    :param num_diffusion_timesteps: the number of betas to produce.\n",
    "    :param alpha_bar: a lambda that takes an argument t from 0 to 1 and\n",
    "                      produces the cumulative product of (1-beta) up to that\n",
    "                      part of the diffusion process.\n",
    "    :param max_beta: the maximum beta to use; use values lower than 1 to\n",
    "                     prevent singularities.\n",
    "    \"\"\"\n",
    "    betas = []\n",
    "    for i in range(num_diffusion_timesteps):\n",
    "        t1 = i / num_diffusion_timesteps\n",
    "        t2 = (i + 1) / num_diffusion_timesteps\n",
    "        betas.append(min(1 - alpha_bar(t2) / alpha_bar(t1), max_beta))\n",
    "    return torch.tensor(betas)\n",
    "# Imputer\n",
    "\n",
    "class diffusion_imputation(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        emb_dim,\n",
    "        excluded_features=None,\n",
    "        # vocab_size,\n",
    "        # pad_idx= None,\n",
    "        strategy=\"random\",\n",
    "        num_residual_layers=4,\n",
    "        features_to_impute=None,\n",
    "        features_to_impute_completely=None,\n",
    "        features_to_impute_after_time=None,\n",
    "        last_n_time=1,\n",
    "        missing_prp=0.1,\n",
    "        diffusion_steps=1000,\n",
    "        max_steps=1000,\n",
    "        diffusion_beta_schedule=\"quadratic\",\n",
    "        num_heads=8,\n",
    "        kernel_size=(3, 7),\n",
    "        ff_dim=512,\n",
    "        num_cells=2,\n",
    "        dropout=0.1,\n",
    "        method=\"rsa\",\n",
    "        device=\"cpu\",\n",
    "        sequence_length=None\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self.emb_dim = emb_dim\n",
    "        self.strategy = strategy\n",
    "        self.features_to_impute = features_to_impute\n",
    "        self.exclude_features = excluded_features\n",
    "        self.num_residual_layers = num_residual_layers\n",
    "        self.features_to_impute_completely = features_to_impute_completely\n",
    "        self.features_to_impute_after_time = features_to_impute_after_time\n",
    "        self.last_n_time = last_n_time\n",
    "        self.missing_prp = missing_prp\n",
    "        self.diffusion_steps = diffusion_steps\n",
    "        self.diffusion_beta_schedule = diffusion_beta_schedule\n",
    "        self.num_heads = num_heads\n",
    "        self.kernel_size = kernel_size\n",
    "        self.ff_dim = ff_dim\n",
    "        self.num_cells = num_cells\n",
    "        self.dropout = dropout\n",
    "        self.method = method\n",
    "        self.sequence_length = sequence_length\n",
    "        self.max_steps = max_steps\n",
    "\n",
    "        # set device to cuda if available\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = \"cuda\"\n",
    "\n",
    "        self.model_loop = ModelLoop(\n",
    "            embed_dim=self.emb_dim,\n",
    "            # diffusion_steps=diffusion_steps,\n",
    "            max_steps=max_steps,\n",
    "            num_heads=num_heads,\n",
    "            kernel_size=kernel_size,\n",
    "            ff_dim=ff_dim,\n",
    "            num_cells=num_cells,\n",
    "            dropout=dropout,\n",
    "            num_residual_layers=num_residual_layers,\n",
    "            method=method,\n",
    "            device=self.device,\n",
    "        )\n",
    "\n",
    "        self.beta = get_named_beta_schedule(\n",
    "            diffusion_beta_schedule, diffusion_steps)\n",
    "\n",
    "        # self.beta = torch.linspace(0.0001, 0.5, diffusion_steps)\n",
    "\n",
    "        # self.beta = torch.linspace(\n",
    "        #         0.0001 ** 0.5, 0.5 ** 0.5, diffusion_steps\n",
    "        #     ) ** 2\n",
    "\n",
    "        self.alpha_hat = 1 - self.beta\n",
    "        self.alpha = torch.cumprod(self.alpha_hat, dim=0)\n",
    "        self.alpha_torch = torch.tensor(self.alpha).float()\n",
    "\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def get_mask(self, data, strategy=\"random\"):\n",
    "\n",
    "        b = data.shape[0]\n",
    "        t = data.shape[1]\n",
    "        f = data.shape[2]\n",
    "\n",
    "        if strategy == \"forecasting\":\n",
    "            forecasted_time = torch.randint(2, t, (b, 1, 1))\n",
    "            mask = torch.zeros_like(data)\n",
    "            for i in range(b):\n",
    "                mask[i, forecasted_time[i]:, :] = 1\n",
    "\n",
    "        if strategy == \"forecasting_last_n_time\":\n",
    "            mask = torch.zeros_like(data)\n",
    "            mask[:, -self.last_n_time, :] = 1\n",
    "\n",
    "        if strategy == \"death_prediction\":\n",
    "            mask = torch.zeros_like(data)\n",
    "            # death is the last 7 columns of the data\n",
    "            mask[:, :, -1] = 1\n",
    "\n",
    "        if strategy == \"random_features\":\n",
    "            selected_features = torch.randint(0, f, (b, 1, 1))\n",
    "            mask = torch.zeros_like(data)\n",
    "            mask[:, :, selected_features] = 1\n",
    "\n",
    "        if strategy == \"selected_features\":\n",
    "            mask = torch.zeros_like(data)\n",
    "            mask[:, :, self.features_to_impute] = 1\n",
    "\n",
    "        if strategy == \"selected_features_after_time\":\n",
    "            selected_time = torch.randint(1, t, (b, 1, 1))\n",
    "            mask = torch.zeros_like(data)\n",
    "            mask[:, selected_time:, self.features_to_impute] = 1\n",
    "\n",
    "        if strategy == \"selected_features_last_n_time\":\n",
    "            mask = torch.zeros_like(data)\n",
    "            mask[:, -self.last_n_time:, self.features_to_impute] = 1\n",
    "\n",
    "        if strategy == \"selected_features_last_n_sequence_length\":\n",
    "            assert self.sequence_length is not None\n",
    "            mask = torch.zeros_like(data)\n",
    "            for i in range(self.sequence_length.shape[0]):\n",
    "                sequence_length = int(self.sequence_length[i])\n",
    "                if i < mask.shape[0]:\n",
    "                    mask[i, (sequence_length - self.last_n_time)\n",
    "                             :sequence_length, self.features_to_impute] = 1\n",
    "\n",
    "        if strategy == \"selected_features_sequence_length\":\n",
    "            mask = torch.zeros_like(data)\n",
    "            for i in range(self.sequence_length.shape[0]):\n",
    "                sequence_length = int(self.sequence_length[i])\n",
    "                if i < mask.shape[0]:\n",
    "                    mask[i, :sequence_length, self.features_to_impute] = 1\n",
    "\n",
    "        if strategy == \"whole_sequence\":\n",
    "            mask = torch.ones_like(data)\n",
    "\n",
    "        if strategy == \"random\":\n",
    "            mask = torch.rand(size=(b, t, f))\n",
    "            mask = mask < self.missing_prp\n",
    "            mask = mask.float()\n",
    "\n",
    "        if strategy == \"selected_features_and_selected_features_after_time\":\n",
    "            mask = torch.zeros_like(data)\n",
    "            mask[:, :, self.features_to_impute_completely] = 1\n",
    "            mask[:, -self.last_n_time:, self.features_to_impute_after_time] = 1\n",
    "\n",
    "        if self.exclude_features is not None:\n",
    "            mask[:, :, self.exclude_features] = 0\n",
    "\n",
    "        return mask\n",
    "\n",
    "    def loss_func(self, predicted_noise, noise, noise_mask):\n",
    "        # noise = torch.nan_to_num(noise, nan=0.0)\n",
    "        # predicted_noise = torch.nan_to_num(predicted_noise, nan=0.0)\n",
    "        residual = noise - predicted_noise\n",
    "        num_obs = torch.sum(noise_mask)\n",
    "        loss = (residual**2).sum() / num_obs\n",
    "        return loss\n",
    "\n",
    "    def weighted_loss_func(self, predicted_noise, noise, noise_mask, stabilized_weights):\n",
    "        # Calculate the residuals\n",
    "        residual = noise - predicted_noise\n",
    "\n",
    "        # Get the sample weights\n",
    "        # print(f\"stabilized_weights shape: {stabilized_weights.shape}\")\n",
    "        sw = stabilized_weights.to(self.device)\n",
    "        # clip sw at 5th and 95th percentile\n",
    "        sw = torch.clamp(sw, 0.05, 0.95)\n",
    "        sw = sw.unsqueeze(-1).repeat(1, 1, residual.shape[-1]) * noise_mask\n",
    "        # print(residual.shape)\n",
    "        # print(sw)\n",
    "        # Apply the sample weights to the squared residuals\n",
    "        weighted_squared_residuals = (residual**2) * sw\n",
    "\n",
    "        # Sum the weighted squared residuals\n",
    "        weighted_loss_sum = weighted_squared_residuals.sum()\n",
    "\n",
    "        # Normalize the loss by the sum of the weights\n",
    "        loss = weighted_loss_sum / sw.sum()\n",
    "\n",
    "        # print(loss)\n",
    "        return loss\n",
    "\n",
    "    def explode_trajectories(self, data, projection_horizon):\n",
    "\n",
    "        self.data = data\n",
    "        # assert self.processed\n",
    "\n",
    "        # logger.info(f'Exploding {self.subset_name} dataset before testing (multiple sequences)')\n",
    "\n",
    "        outputs = self.data['outputs']\n",
    "        prev_outputs = self.data['prev_outputs']\n",
    "        sequence_lengths = self.data['sequence_lengths']\n",
    "        # vitals = self.data['vitals']\n",
    "        # next_vitals = self.data['next_vitals']\n",
    "        active_entries = self.data['active_entries']\n",
    "        current_treatments = self.data['current_treatments']\n",
    "        previous_treatments = self.data['prev_treatments']\n",
    "        static_features = self.data['static_features']\n",
    "        # repeat static features t times (first dimension in outputs)\n",
    "        static_features = static_features.unsqueeze(\n",
    "            1).repeat(1, outputs.shape[1], 1)\n",
    "        if 'stabilized_weights' in self.data:\n",
    "            stabilized_weights = self.data['stabilized_weights']\n",
    "\n",
    "        num_patients, max_seq_length, num_features = outputs.shape\n",
    "        num_seq2seq_rows = num_patients * max_seq_length\n",
    "\n",
    "        seq2seq_previous_treatments = np.zeros(\n",
    "            (num_seq2seq_rows, max_seq_length, previous_treatments.shape[-1]))\n",
    "        seq2seq_current_treatments = np.zeros(\n",
    "            (num_seq2seq_rows, max_seq_length, current_treatments.shape[-1]))\n",
    "        seq2seq_static_features = np.zeros(\n",
    "            (num_seq2seq_rows, max_seq_length, static_features.shape[-1]))\n",
    "        seq2seq_outputs = np.zeros(\n",
    "            (num_seq2seq_rows, max_seq_length, outputs.shape[-1]))\n",
    "        seq2seq_prev_outputs = np.zeros(\n",
    "            (num_seq2seq_rows, max_seq_length, prev_outputs.shape[-1]))\n",
    "        # seq2seq_vitals = np.zeros((num_seq2seq_rows, max_seq_length, vitals.shape[-1]))\n",
    "        # seq2seq_next_vitals = np.zeros((num_seq2seq_rows, max_seq_length - 1, next_vitals.shape[-1]))\n",
    "        seq2seq_active_entries = np.zeros(\n",
    "            (num_seq2seq_rows, max_seq_length, active_entries.shape[-1]))\n",
    "        seq2seq_sequence_lengths = np.zeros(num_seq2seq_rows)\n",
    "        if 'stabilized_weights' in self.data:\n",
    "            seq2seq_stabilized_weights = np.zeros(\n",
    "                (num_seq2seq_rows, max_seq_length))\n",
    "\n",
    "        total_seq2seq_rows = 0  # we use this to shorten any trajectories later\n",
    "\n",
    "        for i in range(num_patients):\n",
    "            sequence_length = int(sequence_lengths[i])\n",
    "\n",
    "            for t in range(projection_horizon, sequence_length):  # shift outputs back by 1\n",
    "                seq2seq_active_entries[total_seq2seq_rows, :(\n",
    "                    t + 1), :] = active_entries[i, :(t + 1), :]\n",
    "                if 'stabilized_weights' in self.data:\n",
    "                    seq2seq_stabilized_weights[total_seq2seq_rows, :(\n",
    "                        t + 1)] = stabilized_weights[i, :(t + 1)]\n",
    "                seq2seq_previous_treatments[total_seq2seq_rows, :(\n",
    "                    t + 1), :] = previous_treatments[i, :(t + 1), :]\n",
    "                seq2seq_current_treatments[total_seq2seq_rows, :(\n",
    "                    t + 1), :] = current_treatments[i, :(t + 1), :]\n",
    "                seq2seq_outputs[total_seq2seq_rows, :(\n",
    "                    t + 1), :] = outputs[i, :(t + 1), :]\n",
    "                seq2seq_prev_outputs[total_seq2seq_rows, :(\n",
    "                    t + 1), :] = prev_outputs[i, :(t + 1), :]\n",
    "                seq2seq_static_features[total_seq2seq_rows, :(\n",
    "                    t + 1), :] = static_features[i, :(t + 1), :]\n",
    "                # seq2seq_vitals[total_seq2seq_rows, :(t + 1), :] = vitals[i, :(t + 1), :]\n",
    "                # seq2seq_next_vitals[total_seq2seq_rows, :min(t + 1, sequence_length - 1), :] = \\\n",
    "                #     next_vitals[i, :min(t + 1, sequence_length - 1), :]\n",
    "                seq2seq_sequence_lengths[total_seq2seq_rows] = t + 1\n",
    "                # seq2seq_static_features[total_seq2seq_rows] = static_features[i]\n",
    "\n",
    "                total_seq2seq_rows += 1\n",
    "\n",
    "        # Filter everything shorter\n",
    "        seq2seq_previous_treatments = seq2seq_previous_treatments[:total_seq2seq_rows, :, :]\n",
    "        seq2seq_current_treatments = seq2seq_current_treatments[:total_seq2seq_rows, :, :]\n",
    "        seq2seq_static_features = seq2seq_static_features[:total_seq2seq_rows, :]\n",
    "        seq2seq_outputs = seq2seq_outputs[:total_seq2seq_rows, :, :]\n",
    "        seq2seq_prev_outputs = seq2seq_prev_outputs[:total_seq2seq_rows, :, :]\n",
    "        # seq2seq_vitals = seq2seq_vitals[:total_seq2seq_rows, :, :]\n",
    "        # seq2seq_next_vitals = seq2seq_next_vitals[:total_seq2seqprocessed_rows, :, :]\n",
    "        seq2seq_active_entries = seq2seq_active_entries[:total_seq2seq_rows, :, :]\n",
    "        seq2seq_sequence_lengths = seq2seq_sequence_lengths[:total_seq2seq_rows]\n",
    "\n",
    "        if 'stabilized_weights' in self.data:\n",
    "            seq2seq_stabilized_weights = seq2seq_stabilized_weights[:total_seq2seq_rows]\n",
    "\n",
    "        new_data = {\n",
    "            'prev_treatments': seq2seq_previous_treatments,\n",
    "            'current_treatments': seq2seq_current_treatments,\n",
    "            'static_features': seq2seq_static_features,\n",
    "            'prev_outputs': seq2seq_prev_outputs,\n",
    "            'outputs': seq2seq_outputs,\n",
    "            # 'vitals': seq2seq_vitals,\n",
    "            # 'next_vitals': seq2seq_next_vitals,\n",
    "            # 'unscaled_outputs': seq2seq_outputs * self.scaling_params['output_stds'] + self.scaling_params['output_means'],\n",
    "            'sequence_lengths': seq2seq_sequence_lengths,\n",
    "            'active_entries': seq2seq_active_entries,\n",
    "        }\n",
    "        if 'stabilized_weights' in self.data:\n",
    "            new_data['stabilized_weights'] = seq2seq_stabilized_weights\n",
    "\n",
    "        # self.data = new_data\n",
    "        # self.exploded = True\n",
    "\n",
    "        # data_shapes = {k: v.shape for k, v in self.data.items()}\n",
    "        # logger.info(f'Shape of processed {self.subset_name} data: {data_shapes}')\n",
    "\n",
    "        return new_data\n",
    "\n",
    "    def get_exploded_dataset(self, dataset, min_length=1, only_active_entries=True, max_length=None):\n",
    "        exploded_dataset = deepcopy(dataset)\n",
    "        if max_length is None:\n",
    "            max_length = max(exploded_dataset['sequence_lengths'][:])\n",
    "        if not only_active_entries:\n",
    "            exploded_dataset['active_entries'][:, :, :] = 1.0\n",
    "            exploded_dataset['sequence_lengths'][:] = max_length\n",
    "        # exploded_dataset.explode_trajectories(min_length)\n",
    "        exploded_dataset = self.explode_trajectories(\n",
    "            exploded_dataset, min_length)\n",
    "        return exploded_dataset\n",
    "\n",
    "    def forward(self, data):\n",
    "\n",
    "        # data = self.get_exploded_dataset(data, 1, only_active_entries=True)\n",
    "        # curr_treatments = data['current_treatments']\n",
    "        # vitals_or_prev_outputs = []\n",
    "        # # vitals_or_prev_outputs.append(data['vitals']) if self.has_vitals else None\n",
    "        # # if self.autoregressive else None\n",
    "        # vitals_or_prev_outputs.append(data['prev_outputs'])\n",
    "        # vitals_or_prev_outputs = torch.cat(vitals_or_prev_outputs, dim=-1)\n",
    "        # static_features = data['static_features']\n",
    "        # outputs = data['outputs']\n",
    "\n",
    "        # x = torch.cat((vitals_or_prev_outputs, curr_treatments), dim=-1)\n",
    "        # x = torch.cat((x, static_features.unsqueeze(\n",
    "        #     1).expand(-1, x.size(1), -1)), dim=-1)\n",
    "        # x = torch.cat((x, outputs), dim=-1)\n",
    "        # data = x\n",
    "        # data = data.to(self.device)\n",
    "        # print(f\"Data shape: {data.shape}\")\n",
    "        # print(data)\n",
    "        b, t, f = data.shape\n",
    "\n",
    "        noise_mask = self.get_mask(data, self.strategy).to(self.device)\n",
    "        # print(noise_mask[0])\n",
    "        # print(data[0])\n",
    "        noise = torch.randn((b, t, f)).to(self.device)\n",
    "        noise = noise_mask * noise\n",
    "\n",
    "        diffusion_t = torch.randint(0, self.diffusion_steps, (b, 1)).squeeze(1)\n",
    "        alpha = self.alpha_torch[diffusion_t].unsqueeze(\n",
    "            1).unsqueeze(2).to(self.device)\n",
    "\n",
    "        noised_data = data * noise_mask\n",
    "        noised_data = noised_data * (alpha**0.5) + noise * ((1 - alpha) ** 0.5)\n",
    "        conditional_data = data * (1 - noise_mask)\n",
    "        noised_data = noised_data + conditional_data\n",
    "        noised_data = noised_data.float()\n",
    "\n",
    "        predicted_noise = self.model_loop(\n",
    "            noised_data.unsqueeze(3), noise_mask.unsqueeze(3), diffusion_t\n",
    "        )\n",
    "        predicted_noise = predicted_noise * noise_mask\n",
    "\n",
    "        return (predicted_noise, noise, noise_mask)\n",
    "\n",
    "    def eval_with_grad(self, data, scale=1):\n",
    "\n",
    "        # with torch.no_grad():\n",
    "        imputation_mask = self.get_mask(data, self.strategy).to(self.device)\n",
    "        conditional_data = data * (1 - imputation_mask)\n",
    "        random_noise = torch.randn_like(data) * imputation_mask * scale\n",
    "        data_2 = (conditional_data + random_noise).unsqueeze(3)\n",
    "\n",
    "        b, ti, f, e = data_2.shape\n",
    "        imputed_samples = torch.zeros((b, ti, f)).to(self.device)\n",
    "        x = conditional_data + random_noise\n",
    "\n",
    "        for t in range(self.diffusion_steps - 1, -1, -1):\n",
    "\n",
    "            x = x.unsqueeze(3).float()\n",
    "            predicted_noise = self.model_loop(\n",
    "                x, imputation_mask.unsqueeze(\n",
    "                    3), torch.tensor([t]).to(self.device)\n",
    "            )\n",
    "            predicted_noise = predicted_noise * imputation_mask\n",
    "\n",
    "            coeff1 = 1 / self.alpha_hat[t] ** 0.5\n",
    "            coeff2 = (1 - self.alpha_hat[t]) / (1 - self.alpha[t]) ** 0.5\n",
    "\n",
    "            x = x.squeeze(3)\n",
    "            x = coeff1 * (x - coeff2 * predicted_noise)\n",
    "\n",
    "            if t > 0:\n",
    "                noise = torch.randn_like(x)\n",
    "                sigma = (\n",
    "                    (1.0 - self.alpha[t - 1]) /\n",
    "                    (1.0 - self.alpha[t]) * self.beta[t]\n",
    "                ) ** 0.5\n",
    "                x += sigma * noise\n",
    "\n",
    "            x = data_2.squeeze(3) * (1 - imputation_mask) + x * imputation_mask\n",
    "\n",
    "            imputed_samples = x\n",
    "\n",
    "        return (imputed_samples, data, imputation_mask)\n",
    "\n",
    "    # def get_predictions(\n",
    "    #     self,\n",
    "    #     data,\n",
    "    #     imputation_mask,\n",
    "    #     mean,\n",
    "    #     std,\n",
    "    #     scale=1,\n",
    "    #     verbose=True,\n",
    "    #     show_max_diff=False,\n",
    "    #     show_rmse=False,\n",
    "    # ):\n",
    "\n",
    "    #     conditional_data = data * (1 - imputation_mask)\n",
    "    #     random_noise = torch.randn_like(data) * imputation_mask * scale\n",
    "    #     data_2 = (conditional_data + random_noise).unsqueeze(3)\n",
    "\n",
    "    #     b, ti, f, e = data_2.shape\n",
    "    #     imputed_samples = torch.zeros((b, ti, f)).to(self.device)\n",
    "    #     x = conditional_data + random_noise\n",
    "\n",
    "    #     with torch.no_grad():\n",
    "\n",
    "    #         # for t in range(self.diffusion_steps - 1, -1, -1):\n",
    "    #         for t in tqdm(range(self.diffusion_steps - 1, -1, -1), desc=\"Diffusion Steps\", leave=False):\n",
    "    #             x = x.unsqueeze(3).float()\n",
    "    #             predicted_noise = self.model_loop(\n",
    "    #                 x, imputation_mask.unsqueeze(\n",
    "    #                     3), torch.tensor([t]).to(self.device)\n",
    "    #             )\n",
    "    #             predicted_noise = predicted_noise * imputation_mask\n",
    "\n",
    "    #             coeff1 = 1 / self.alpha_hat[t] ** 0.5\n",
    "    #             coeff2 = (1 - self.alpha_hat[t]) / (1 - self.alpha[t]) ** 0.5\n",
    "\n",
    "    #             x = x.squeeze(3)\n",
    "    #             x = coeff1 * (x - coeff2 * predicted_noise)\n",
    "\n",
    "    #             if t > 0:\n",
    "    #                 noise = torch.randn_like(x)\n",
    "    #                 sigma = (\n",
    "    #                     (1.0 - self.alpha[t - 1]) /\n",
    "    #                     (1.0 - self.alpha[t]) * self.beta[t]\n",
    "    #                 ) ** 0.5\n",
    "    #                 x += sigma * noise\n",
    "\n",
    "    #             x = data_2.squeeze(3) * (1 - imputation_mask) + \\\n",
    "    #                 x * imputation_mask\n",
    "\n",
    "    #         imputed_samples = x.detach()\n",
    "    #         imputed_samples[torch.isnan(imputed_samples)] = 0\n",
    "\n",
    "    #     if show_max_diff == True:\n",
    "    #         # show the data at torch.max(torch.abs(data[imputation_mask !=0] - imputed_samples[imputation_mask !=0]))\n",
    "    #         print(\n",
    "    #             \"max difference = \",\n",
    "    #             torch.max(\n",
    "    #                 torch.abs(\n",
    "    #                     data[imputation_mask != 0]\n",
    "    #                     - imputed_samples[imputation_mask != 0]\n",
    "    #                 )\n",
    "    #             ).item(),\n",
    "    #         )\n",
    "    #         print(\n",
    "    #             \"data at max difference = \",\n",
    "    #             data[imputation_mask != 0][\n",
    "    #                 torch.argmax(\n",
    "    #                     torch.abs(\n",
    "    #                         data[imputation_mask != 0]\n",
    "    #                         - imputed_samples[imputation_mask != 0]\n",
    "    #                     )\n",
    "    #                 )\n",
    "    #             ].item(),\n",
    "    #         )\n",
    "    #         print(\n",
    "    #             \"imputed at max difference = \",\n",
    "    #             imputed_samples[imputation_mask != 0][\n",
    "    #                 torch.argmax(\n",
    "    #                     torch.abs(\n",
    "    #                         data[imputation_mask != 0]\n",
    "    #                         - imputed_samples[imputation_mask != 0]\n",
    "    #                     )\n",
    "    #                 )\n",
    "    #             ].item(),\n",
    "    #         )\n",
    "\n",
    "    #     mae = torch.mean(\n",
    "    #         torch.abs(\n",
    "    #             data[imputation_mask != 0] -\n",
    "    #             imputed_samples[imputation_mask != 0]\n",
    "    #         )\n",
    "    #     ).item()\n",
    "    #     if verbose == True:\n",
    "    #         print(\"mae = \", mae)\n",
    "\n",
    "    #     if show_rmse == True:\n",
    "    #         # descale the data\n",
    "    #         imputed_samples_copy = imputed_samples.detach().clone()\n",
    "    #         imputed_samples_copy = imputed_samples_copy * std + mean\n",
    "    #         data_copy = data.detach().clone()\n",
    "    #         data_copy = data_copy * std + mean\n",
    "    #         rmse = torch.sqrt(\n",
    "    #             torch.mean(\n",
    "    #                 (\n",
    "    #                     data_copy[imputation_mask != 0]\n",
    "    #                     - imputed_samples_copy[imputation_mask != 0]\n",
    "    #                 )\n",
    "    #                 ** 2\n",
    "    #             )\n",
    "    #         ).item()\n",
    "    #         rmse = rmse / 1150 * 100\n",
    "    #         print(\"rmse = \", rmse)\n",
    "    #     # data_to_print = data[imputation_mask !=0]\n",
    "    #     # imputed_samples_to_print = imputed_samples[imputation_mask !=0]\n",
    "    #     # print(\"data:\", data_to_print)\n",
    "    #     # print(\"imputed:\", imputed_samples_to_print)\n",
    "    #     # print(\"absolute difference in the first 100 : \", torch.abs(data_to_print - imputed_samples_to_print)[:100])\n",
    "    #     # print(\"mae = \", torch.mean(torch.abs(data_to_print - imputed_samples_to_print)).item())\n",
    "\n",
    "    #     return (imputed_samples, data, imputation_mask, mae)\n",
    "\n",
    "    def get_predictions(\n",
    "        self,\n",
    "        data,\n",
    "        imputation_mask,\n",
    "        mean,\n",
    "        std,\n",
    "        extra_steps=0,\n",
    "        scale=1,\n",
    "        verbose=True,\n",
    "        show_max_diff=False,\n",
    "        show_rmse=False,\n",
    "    ):\n",
    "        # Compute total diffusion steps to use during inference.\n",
    "        total_diffusion_steps = self.diffusion_steps + extra_steps\n",
    "\n",
    "        # If extra steps are requested, generate an extended beta schedule.\n",
    "        if extra_steps > 0:\n",
    "            if self.diffusion_beta_schedule == \"linear\":\n",
    "                if self.diffusion_steps < 100:\n",
    "                    scale_lin = 100 / self.diffusion_steps\n",
    "                else:\n",
    "                    scale_lin = 1000 / self.diffusion_steps\n",
    "                beta_start = scale_lin * 0.0001\n",
    "                beta_end = scale_lin * 0.02\n",
    "                extended_betas = torch.linspace(\n",
    "                    beta_start, beta_end, total_diffusion_steps)\n",
    "            elif self.diffusion_beta_schedule == \"cosine\":\n",
    "                def alpha_bar(t):\n",
    "                    return math.cos((t + 0.008) / 1.008 * math.pi / 2) ** 2\n",
    "                # Create a sequence of time steps in [0,1]\n",
    "                steps = torch.linspace(0, 1, total_diffusion_steps + 1)\n",
    "                max_beta = 0.999\n",
    "                beta_list = []\n",
    "                for i in range(total_diffusion_steps):\n",
    "                    t1 = steps[i].item()\n",
    "                    t2 = steps[i + 1].item()\n",
    "                    beta_val = min(1 - alpha_bar(t2) / alpha_bar(t1), max_beta)\n",
    "                    beta_list.append(beta_val)\n",
    "                extended_betas = torch.tensor(beta_list)\n",
    "            elif self.diffusion_beta_schedule == \"quadratic\":\n",
    "                scale_quad = 50 / self.diffusion_steps\n",
    "                beta_start = scale_quad * 0.0001\n",
    "                beta_end = scale_quad * 0.5\n",
    "                extended_betas = (torch.linspace(\n",
    "                    beta_start**0.5, beta_end**0.5, total_diffusion_steps)) ** 2\n",
    "            else:\n",
    "                raise NotImplementedError(\n",
    "                    f\"Unknown beta schedule: {self.diffusion_beta_schedule}\")\n",
    "\n",
    "            extended_alpha_hat = 1 - extended_betas\n",
    "            extended_alpha = torch.cumprod(extended_alpha_hat, dim=0)\n",
    "            extended_beta = extended_betas\n",
    "        else:\n",
    "            # Use the precomputed schedule from training.\n",
    "            total_diffusion_steps = self.diffusion_steps\n",
    "            extended_alpha = self.alpha_torch.to(self.device)\n",
    "            extended_alpha_hat = 1 - self.beta.to(self.device)\n",
    "            extended_beta = self.beta.to(self.device)\n",
    "\n",
    "        conditional_data = data * (1 - imputation_mask)\n",
    "        random_noise = torch.randn_like(data) * imputation_mask * scale\n",
    "        data_2 = (conditional_data + random_noise).unsqueeze(3)\n",
    "\n",
    "        b, ti, f, e = data_2.shape\n",
    "        imputed_samples = torch.zeros((b, ti, f)).to(self.device)\n",
    "        x = conditional_data + random_noise\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Reverse diffusion loop over the extended number of steps.\n",
    "            for t in tqdm(range(total_diffusion_steps - 1, -1, -1), desc=\"Diffusion Steps\", leave=False):\n",
    "                x = x.unsqueeze(3).float()\n",
    "                predicted_noise = self.model_loop(\n",
    "                    x, imputation_mask.unsqueeze(\n",
    "                        3), torch.tensor([t]).to(self.device)\n",
    "                )\n",
    "                predicted_noise = predicted_noise * imputation_mask\n",
    "\n",
    "                # Compute coefficients using the extended schedule.\n",
    "                coeff1 = 1 / (extended_alpha_hat[t] ** 0.5)\n",
    "                coeff2 = (1 - extended_alpha_hat[t]) / \\\n",
    "                    ((1 - extended_alpha[t]) ** 0.5)\n",
    "\n",
    "                x = x.squeeze(3)\n",
    "                x = coeff1 * (x - coeff2 * predicted_noise)\n",
    "\n",
    "                if t > 0:\n",
    "                    noise = torch.randn_like(x)\n",
    "                    sigma = (\n",
    "                        ((1.0 - extended_alpha[t - 1]) / (1.0 - extended_alpha[t]) * extended_beta[t]) ** 0.5)\n",
    "                    x += sigma * noise\n",
    "\n",
    "                # Reinstate the conditional (unnoised) values.\n",
    "                x = data_2.squeeze(3) * (1 - imputation_mask) + \\\n",
    "                    x * imputation_mask\n",
    "\n",
    "            imputed_samples = x.detach()\n",
    "            imputed_samples[torch.isnan(imputed_samples)] = 0\n",
    "\n",
    "        if show_max_diff:\n",
    "            print(\n",
    "                \"max difference = \",\n",
    "                torch.max(\n",
    "                    torch.abs(\n",
    "                        data[imputation_mask != 0] -\n",
    "                        imputed_samples[imputation_mask != 0]\n",
    "                    )\n",
    "                ).item(),\n",
    "            )\n",
    "            print(\n",
    "                \"data at max difference = \",\n",
    "                data[imputation_mask != 0][\n",
    "                    torch.argmax(\n",
    "                        torch.abs(\n",
    "                            data[imputation_mask != 0] -\n",
    "                            imputed_samples[imputation_mask != 0]\n",
    "                        )\n",
    "                    )\n",
    "                ].item(),\n",
    "            )\n",
    "            print(\n",
    "                \"imputed at max difference = \",\n",
    "                imputed_samples[imputation_mask != 0][\n",
    "                    torch.argmax(\n",
    "                        torch.abs(\n",
    "                            data[imputation_mask != 0] -\n",
    "                            imputed_samples[imputation_mask != 0]\n",
    "                        )\n",
    "                    )\n",
    "                ].item(),\n",
    "            )\n",
    "\n",
    "        mae = torch.mean(torch.abs(\n",
    "            data[imputation_mask != 0] - imputed_samples[imputation_mask != 0])).item()\n",
    "        if verbose:\n",
    "            print(\"mae = \", mae)\n",
    "\n",
    "        if show_rmse:\n",
    "            imputed_samples_copy = imputed_samples.detach().clone() * std + mean\n",
    "            data_copy = data.detach().clone() * std + mean\n",
    "            rmse = torch.sqrt(torch.mean(\n",
    "                (data_copy[imputation_mask != 0] - imputed_samples_copy[imputation_mask != 0]) ** 2)).item()\n",
    "            rmse = rmse / 1150 * 100\n",
    "            print(\"rmse = \", rmse)\n",
    "\n",
    "        return (imputed_samples, data, imputation_mask, mae)\n",
    "\n",
    "# # New main function for config handling and logging initialization\n",
    "# def main(config: DictConfig):\n",
    "#     logging.basicConfig(level=logging.INFO)\n",
    "#     logger = logging.getLogger(__name__)\n",
    "#     logger.info(\"Starting CausalDiff with config:\\n%s\", OmegaConf.to_yaml(config))\n",
    "\n",
    "#     # Instantiate your model or application components using config args.\n",
    "#     # Example:\n",
    "#     # model = ModelLoop(\n",
    "#     #     embed_dim=config.model.embed_dim,\n",
    "#     #     diffusion_steps=config.model.diffusion_steps,\n",
    "#     #     num_heads=config.model.num_heads,\n",
    "#     #     kernel_size=config.model.kernel_size,\n",
    "#     #     ff_dim=config.model.ff_dim,\n",
    "#     #     num_cells=config.model.num_cells,\n",
    "#     #     dropout=config.model.dropout,\n",
    "#     #     num_residual_layers=config.model.num_residual_layers,\n",
    "#     #     method=config.model.method,\n",
    "#     #     device=config.device,\n",
    "#     # )\n",
    "#     # ...additional logic for training/evaluation...\n",
    "\n",
    "# # Main guard to load config and start main()\n",
    "# if __name__ == \"__main__\":\n",
    "#     import sys\n",
    "#     # Load config from the provided YAML file or use a default path\n",
    "#     config_path = sys.argv[1] if len(sys.argv) > 1 else \"configs/causaldiff.yaml\"\n",
    "#     config = OmegaConf.load(config_path)\n",
    "#     main(config)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
