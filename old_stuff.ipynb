{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class PropensityNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(PropensityNN, self).__init__()\n",
    "        # self.fc1 = nn.Linear(input_size, 128)\n",
    "        # self.fc2 = nn.Linear(128, 64)\n",
    "        # self.fc3 = nn.Linear(64, output_size)\n",
    "        self.simple_linear = nn.Linear(input_size, output_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = torch.relu(self.fc1(x))\n",
    "        # x = torch.relu(self.fc2(x))\n",
    "        # x = self.fc3(x)\n",
    "        x = self.simple_linear(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MSMPropensityTreatment(CausalDiff):\n",
    "\n",
    "    model_type = 'propensity_treatment'\n",
    "\n",
    "    def __init__(self,\n",
    "                 args: DictConfig,\n",
    "                 dataset_collection: Union[RealDatasetCollection,\n",
    "                                           SyntheticDatasetCollection] = None,\n",
    "                 autoregressive: bool = None, has_vitals: bool = None, **kwargs):\n",
    "        super().__init__(args, dataset_collection, autoregressive, has_vitals)\n",
    "\n",
    "        self.input_size = self.dim_treatments\n",
    "        logger.info(f'Input size of {self.model_type}: {self.input_size}')\n",
    "        self.output_size = self.dim_treatments\n",
    "\n",
    "        self.propensity_treatment = PropensityNN(\n",
    "            self.input_size, self.output_size)\n",
    "        self.optimizer = optim.Adam(\n",
    "            self.propensity_treatment.parameters(), lr=0.001)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.save_hyperparameters(args)\n",
    "\n",
    "    def get_inputs(self, dataset: Dataset) -> np.array:\n",
    "        active_entries = dataset.data['active_entries']\n",
    "        prev_treatments = dataset.data['prev_treatments']\n",
    "        inputs = (prev_treatments * active_entries).sum(1)\n",
    "        return inputs\n",
    "\n",
    "    def fit(self):\n",
    "        self.prepare_data()\n",
    "        train_f = self.get_exploded_dataset(\n",
    "            self.dataset_collection.train_f, min_length=self.lag_features)\n",
    "        active_entries = train_f.data['active_entries']\n",
    "        last_entries = active_entries - \\\n",
    "            np.concatenate([active_entries[:, 1:, :], np.zeros(\n",
    "                (active_entries.shape[0], 1, 1))], axis=1)\n",
    "\n",
    "        # Inputs\n",
    "        inputs = self.get_inputs(train_f)\n",
    "\n",
    "        # Outputs\n",
    "        current_treatments = train_f.data['current_treatments']\n",
    "        outputs = (current_treatments * last_entries).sum(1)\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        inputs = torch.tensor(inputs, dtype=torch.float32)\n",
    "        outputs = torch.tensor(outputs, dtype=torch.float32)\n",
    "\n",
    "        # Create DataLoader\n",
    "        dataset = TensorDataset(inputs, outputs)\n",
    "        dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "        # Training loop\n",
    "        # Training loop\n",
    "        for epoch in range(args.exp.max_epochs):\n",
    "            epoch_loss = 0.0\n",
    "            for batch_inputs, batch_outputs in dataloader:\n",
    "                self.optimizer.zero_grad()\n",
    "                predictions = self.propensity_treatment(batch_inputs)\n",
    "                loss = self.criterion(\n",
    "                    predictions, batch_outputs)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "            epoch_loss /= len(dataloader)\n",
    "            print(\n",
    "                f\"Epoch {epoch+1}/{args.exp.max_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "            # also show accuracy\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            with torch.no_grad():\n",
    "                for batch_inputs, batch_outputs in dataloader:\n",
    "                    outputs = self.propensity_treatment(batch_inputs)\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    total += batch_outputs.size(0)\n",
    "                    # Convert one-hot targets to indices:\n",
    "                    target_labels = batch_outputs.argmax(dim=1)\n",
    "                    correct += (predicted == target_labels).sum().item()\n",
    "            print(f\"Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "\n",
    "class MSMPropensityHistory(CausalDiff):\n",
    "\n",
    "    model_type = 'propensity_history'\n",
    "\n",
    "    def __init__(self,\n",
    "                 args: DictConfig,\n",
    "                 dataset_collection: Union[RealDatasetCollection,\n",
    "                                           SyntheticDatasetCollection] = None,\n",
    "                 autoregressive: bool = None, has_vitals: bool = None, **kwargs):\n",
    "        super().__init__(args, dataset_collection, autoregressive, has_vitals)\n",
    "\n",
    "        self.input_size = self.dim_treatments + \\\n",
    "            self.dim_static_features + self.lag_features\n",
    "        self.input_size += self.dim_vitals if self.has_vitals else 0\n",
    "        self.input_size += self.dim_outcome if self.autoregressive else 0\n",
    "\n",
    "        logger.info(f'Input size of {self.model_type}: {self.input_size}')\n",
    "        self.output_size = self.dim_treatments\n",
    "\n",
    "        self.propensity_history = PropensityNN(\n",
    "            self.input_size, self.output_size)\n",
    "        self.optimizer = optim.Adam(\n",
    "            self.propensity_history.parameters(), lr=0.001)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.save_hyperparameters(args)\n",
    "\n",
    "    def get_inputs(self, dataset: Dataset, projection_horizon=0) -> np.array:\n",
    "        active_entries = dataset.data['active_entries']\n",
    "        lagged_entries = active_entries - \\\n",
    "            np.concatenate([active_entries[:, self.lag_features + 1:, :],\n",
    "                            np.zeros((active_entries.shape[0], self.lag_features + 1, 1))], axis=1)\n",
    "        if projection_horizon > 0:\n",
    "            lagged_entries = np.concatenate([lagged_entries[:, projection_horizon:, :],\n",
    "                                             np.zeros((active_entries.shape[0], projection_horizon, 1))], axis=1)\n",
    "\n",
    "        active_entries_before_projection = np.concatenate([active_entries[:, projection_horizon:, :],\n",
    "                                                          np.zeros((active_entries.shape[0], projection_horizon, 1))], axis=1)\n",
    "\n",
    "        prev_treatments = dataset.data['prev_treatments']\n",
    "        inputs = [(prev_treatments * active_entries_before_projection).sum(1)]\n",
    "        if self.has_vitals:\n",
    "            vitals = dataset.data['vitals']\n",
    "            inputs.append(vitals[np.repeat(lagged_entries, self.dim_vitals, 2) == 1.0].reshape(vitals.shape[0],\n",
    "                                                                                               (self.lag_features + 1) *\n",
    "                                                                                               self.dim_vitals))\n",
    "        if self.autoregressive:\n",
    "            prev_outputs = dataset.data['prev_outputs']\n",
    "            inputs.append(prev_outputs[np.repeat(lagged_entries, self.dim_outcome, 2) == 1.0].reshape(prev_outputs.shape[0],\n",
    "                                                                                                      (self.lag_features + 1) *\n",
    "                                                                                                      self.dim_outcome))\n",
    "        static_features = dataset.data['static_features']\n",
    "        inputs.append(static_features)\n",
    "        return np.concatenate(inputs, axis=1)\n",
    "\n",
    "    def fit(self):\n",
    "        self.prepare_data()\n",
    "        train_f = self.get_exploded_dataset(\n",
    "            self.dataset_collection.train_f, min_length=self.lag_features)\n",
    "        active_entries = train_f.data['active_entries']\n",
    "        last_entries = active_entries - \\\n",
    "            np.concatenate([active_entries[:, 1:, :], np.zeros(\n",
    "                (active_entries.shape[0], 1, 1))], axis=1)\n",
    "\n",
    "        # Inputs\n",
    "        inputs = self.get_inputs(train_f)\n",
    "\n",
    "        # Outputs\n",
    "        current_treatments = train_f.data['current_treatments']\n",
    "        outputs = (current_treatments * last_entries).sum(1)\n",
    "        # Convert to PyTorch tensors\n",
    "        inputs = torch.tensor(inputs, dtype=torch.float32)\n",
    "        outputs = torch.tensor(outputs, dtype=torch.float32)\n",
    "\n",
    "        # Create DataLoader\n",
    "        dataset = TensorDataset(inputs, outputs)\n",
    "        dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(args.exp.max_epochs):\n",
    "            epoch_loss = 0.0\n",
    "            for batch_inputs, batch_outputs in dataloader:\n",
    "                self.optimizer.zero_grad()\n",
    "                predictions = self.propensity_history(batch_inputs)\n",
    "                loss = self.criterion(predictions, batch_outputs)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "            epoch_loss /= len(dataloader)\n",
    "            print(\n",
    "                f\"Epoch {epoch + 1}/{args.exp.max_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "            # also show accuracy\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for batch_inputs, batch_outputs in dataloader:\n",
    "                predictions = self.propensity_history(batch_inputs)\n",
    "                _, predicted = torch.max(predictions.data, 1)\n",
    "                total += batch_outputs.size(0)\n",
    "\n",
    "                target_labels = batch_outputs.argmax(dim=1)\n",
    "                correct += (predicted == target_labels).sum().item()\n",
    "\n",
    "            print(f\"Accuracy: {100 * correct / total:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
