{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class PropensityNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(PropensityNN, self).__init__()\n",
    "        # self.fc1 = nn.Linear(input_size, 128)\n",
    "        # self.fc2 = nn.Linear(128, 64)\n",
    "        # self.fc3 = nn.Linear(64, output_size)\n",
    "        self.simple_linear = nn.Linear(input_size, output_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = torch.relu(self.fc1(x))\n",
    "        # x = torch.relu(self.fc2(x))\n",
    "        # x = self.fc3(x)\n",
    "        x = self.simple_linear(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MSMPropensityTreatment(CausalDiff):\n",
    "\n",
    "    model_type = 'propensity_treatment'\n",
    "\n",
    "    def __init__(self,\n",
    "                 args: DictConfig,\n",
    "                 dataset_collection: Union[RealDatasetCollection,\n",
    "                                           SyntheticDatasetCollection] = None,\n",
    "                 autoregressive: bool = None, has_vitals: bool = None, **kwargs):\n",
    "        super().__init__(args, dataset_collection, autoregressive, has_vitals)\n",
    "\n",
    "        self.input_size = self.dim_treatments\n",
    "        logger.info(f'Input size of {self.model_type}: {self.input_size}')\n",
    "        self.output_size = self.dim_treatments\n",
    "\n",
    "        self.propensity_treatment = PropensityNN(\n",
    "            self.input_size, self.output_size)\n",
    "        self.optimizer = optim.Adam(\n",
    "            self.propensity_treatment.parameters(), lr=0.001)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.save_hyperparameters(args)\n",
    "\n",
    "    def get_inputs(self, dataset: Dataset) -> np.array:\n",
    "        active_entries = dataset.data['active_entries']\n",
    "        prev_treatments = dataset.data['prev_treatments']\n",
    "        inputs = (prev_treatments * active_entries).sum(1)\n",
    "        return inputs\n",
    "\n",
    "    def fit(self):\n",
    "        self.prepare_data()\n",
    "        train_f = self.get_exploded_dataset(\n",
    "            self.dataset_collection.train_f, min_length=self.lag_features)\n",
    "        active_entries = train_f.data['active_entries']\n",
    "        last_entries = active_entries - \\\n",
    "            np.concatenate([active_entries[:, 1:, :], np.zeros(\n",
    "                (active_entries.shape[0], 1, 1))], axis=1)\n",
    "\n",
    "        # Inputs\n",
    "        inputs = self.get_inputs(train_f)\n",
    "\n",
    "        # Outputs\n",
    "        current_treatments = train_f.data['current_treatments']\n",
    "        outputs = (current_treatments * last_entries).sum(1)\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        inputs = torch.tensor(inputs, dtype=torch.float32)\n",
    "        outputs = torch.tensor(outputs, dtype=torch.float32)\n",
    "\n",
    "        # Create DataLoader\n",
    "        dataset = TensorDataset(inputs, outputs)\n",
    "        dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "        # Training loop\n",
    "        # Training loop\n",
    "        for epoch in range(args.exp.max_epochs):\n",
    "            epoch_loss = 0.0\n",
    "            for batch_inputs, batch_outputs in dataloader:\n",
    "                self.optimizer.zero_grad()\n",
    "                predictions = self.propensity_treatment(batch_inputs)\n",
    "                loss = self.criterion(\n",
    "                    predictions, batch_outputs)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "            epoch_loss /= len(dataloader)\n",
    "            print(\n",
    "                f\"Epoch {epoch+1}/{args.exp.max_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "            # also show accuracy\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            with torch.no_grad():\n",
    "                for batch_inputs, batch_outputs in dataloader:\n",
    "                    outputs = self.propensity_treatment(batch_inputs)\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    total += batch_outputs.size(0)\n",
    "                    # Convert one-hot targets to indices:\n",
    "                    target_labels = batch_outputs.argmax(dim=1)\n",
    "                    correct += (predicted == target_labels).sum().item()\n",
    "            print(f\"Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "\n",
    "class MSMPropensityHistory(CausalDiff):\n",
    "\n",
    "    model_type = 'propensity_history'\n",
    "\n",
    "    def __init__(self,\n",
    "                 args: DictConfig,\n",
    "                 dataset_collection: Union[RealDatasetCollection,\n",
    "                                           SyntheticDatasetCollection] = None,\n",
    "                 autoregressive: bool = None, has_vitals: bool = None, **kwargs):\n",
    "        super().__init__(args, dataset_collection, autoregressive, has_vitals)\n",
    "\n",
    "        self.input_size = self.dim_treatments + \\\n",
    "            self.dim_static_features + self.lag_features\n",
    "        self.input_size += self.dim_vitals if self.has_vitals else 0\n",
    "        self.input_size += self.dim_outcome if self.autoregressive else 0\n",
    "\n",
    "        logger.info(f'Input size of {self.model_type}: {self.input_size}')\n",
    "        self.output_size = self.dim_treatments\n",
    "\n",
    "        self.propensity_history = PropensityNN(\n",
    "            self.input_size, self.output_size)\n",
    "        self.optimizer = optim.Adam(\n",
    "            self.propensity_history.parameters(), lr=0.001)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.save_hyperparameters(args)\n",
    "\n",
    "    def get_inputs(self, dataset: Dataset, projection_horizon=0) -> np.array:\n",
    "        active_entries = dataset.data['active_entries']\n",
    "        lagged_entries = active_entries - \\\n",
    "            np.concatenate([active_entries[:, self.lag_features + 1:, :],\n",
    "                            np.zeros((active_entries.shape[0], self.lag_features + 1, 1))], axis=1)\n",
    "        if projection_horizon > 0:\n",
    "            lagged_entries = np.concatenate([lagged_entries[:, projection_horizon:, :],\n",
    "                                             np.zeros((active_entries.shape[0], projection_horizon, 1))], axis=1)\n",
    "\n",
    "        active_entries_before_projection = np.concatenate([active_entries[:, projection_horizon:, :],\n",
    "                                                          np.zeros((active_entries.shape[0], projection_horizon, 1))], axis=1)\n",
    "\n",
    "        prev_treatments = dataset.data['prev_treatments']\n",
    "        inputs = [(prev_treatments * active_entries_before_projection).sum(1)]\n",
    "        if self.has_vitals:\n",
    "            vitals = dataset.data['vitals']\n",
    "            inputs.append(vitals[np.repeat(lagged_entries, self.dim_vitals, 2) == 1.0].reshape(vitals.shape[0],\n",
    "                                                                                               (self.lag_features + 1) *\n",
    "                                                                                               self.dim_vitals))\n",
    "        if self.autoregressive:\n",
    "            prev_outputs = dataset.data['prev_outputs']\n",
    "            inputs.append(prev_outputs[np.repeat(lagged_entries, self.dim_outcome, 2) == 1.0].reshape(prev_outputs.shape[0],\n",
    "                                                                                                      (self.lag_features + 1) *\n",
    "                                                                                                      self.dim_outcome))\n",
    "        static_features = dataset.data['static_features']\n",
    "        inputs.append(static_features)\n",
    "        return np.concatenate(inputs, axis=1)\n",
    "\n",
    "    def fit(self):\n",
    "        self.prepare_data()\n",
    "        train_f = self.get_exploded_dataset(\n",
    "            self.dataset_collection.train_f, min_length=self.lag_features)\n",
    "        active_entries = train_f.data['active_entries']\n",
    "        last_entries = active_entries - \\\n",
    "            np.concatenate([active_entries[:, 1:, :], np.zeros(\n",
    "                (active_entries.shape[0], 1, 1))], axis=1)\n",
    "\n",
    "        # Inputs\n",
    "        inputs = self.get_inputs(train_f)\n",
    "\n",
    "        # Outputs\n",
    "        current_treatments = train_f.data['current_treatments']\n",
    "        outputs = (current_treatments * last_entries).sum(1)\n",
    "        # Convert to PyTorch tensors\n",
    "        inputs = torch.tensor(inputs, dtype=torch.float32)\n",
    "        outputs = torch.tensor(outputs, dtype=torch.float32)\n",
    "\n",
    "        # Create DataLoader\n",
    "        dataset = TensorDataset(inputs, outputs)\n",
    "        dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(args.exp.max_epochs):\n",
    "            epoch_loss = 0.0\n",
    "            for batch_inputs, batch_outputs in dataloader:\n",
    "                self.optimizer.zero_grad()\n",
    "                predictions = self.propensity_history(batch_inputs)\n",
    "                loss = self.criterion(predictions, batch_outputs)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "            epoch_loss /= len(dataloader)\n",
    "            print(\n",
    "                f\"Epoch {epoch + 1}/{args.exp.max_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "            # also show accuracy\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for batch_inputs, batch_outputs in dataloader:\n",
    "                predictions = self.propensity_history(batch_inputs)\n",
    "                _, predicted = torch.max(predictions.data, 1)\n",
    "                total += batch_outputs.size(0)\n",
    "\n",
    "                target_labels = batch_outputs.argmax(dim=1)\n",
    "                correct += (predicted == target_labels).sum().item()\n",
    "\n",
    "            print(f\"Accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# from IPython.display import display, clear_output\n",
    "# import matplotlib.pyplot as plt\n",
    "# import time\n",
    "# import statistics\n",
    "# from itertools import chain\n",
    "# import numpy as np\n",
    "# from collections import deque\n",
    "\n",
    "# def train(model, data_loader, data_loader_validation, epochs, lr, loss_func, batch_embedder,\n",
    "#           windowed_mode=False, window_mode=\"uniform\", window_start_mode=\"random\", min_window=50, max_window=100, neg_bin_p=0.95, train_on_all_every=4,\n",
    "#           annealing_mode = False, annealing_window=5, annealing_multiplier=1.25, annealing_ratio = 0.5, annealing_minimum = 1e-6,\n",
    "#           device=\"cuda\", verbose=False, plot_every=10,\n",
    "#           validation_frequency=1, validation_prp=10, moving_avg_window=10):\n",
    "\n",
    "#     batch_embedder = batch_embedder.to(device)\n",
    "#     model = model.to(device)\n",
    "\n",
    "#     optimizer = torch.optim.Adam(\n",
    "#         chain(batch_embedder.parameters(), model.parameters()),\n",
    "#         lr=lr\n",
    "#     )\n",
    "\n",
    "#     model.train()\n",
    "#     batch_embedder.train()\n",
    "#     loss_list = []\n",
    "#     initial_value = 1.0  # Initial value for equal probability\n",
    "#     window_losses = torch.ones(max_window - min_window + 1, device=device) * initial_value  # Track losses for each window length\n",
    "#     window_counts = torch.zeros(max_window - min_window + 1, device=device)  # Track counts for each window length\n",
    "#     loss_deques = [deque(maxlen=moving_avg_window) for _ in range(max_window - min_window + 1)]  # Deques for moving average\n",
    "#     if windowed_mode and window_mode == \"biased_loss\":\n",
    "#         fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(10, 12))\n",
    "#     else:\n",
    "#         fig, ax1 = plt.subplots(1, 1, figsize=(10, 6))\n",
    "#     epoch_loss_list = []\n",
    "#     val_loss = 0\n",
    "\n",
    "#     for epoch in range(epochs):\n",
    "#         # Annealing for the learning rate\n",
    "#         if annealing_mode and epoch > annealing_window:\n",
    "#             if len(epoch_loss_list) > 0 and epoch_loss_list[-1] >= annealing_multiplier * (statistics.mean(epoch_loss_list[-annealing_window:])):\n",
    "#                 for g in optimizer.param_groups:\n",
    "#                     if g['lr'] * annealing_ratio < annealing_minimum:\n",
    "#                         g['lr'] = annealing_minimum\n",
    "#                     else:\n",
    "#                         g['lr'] *= annealing_ratio\n",
    "\n",
    "#         start = time.time()\n",
    "#         for i, batch in enumerate(data_loader):\n",
    "\n",
    "#             batch = batch.to(device)\n",
    "#             batch = batch_embedder(batch)\n",
    "\n",
    "#             batch_length = batch.shape[1]\n",
    "\n",
    "#             # Windowed mode logic\n",
    "#             if windowed_mode:\n",
    "#                 if batch_length < min_window:\n",
    "#                     continue\n",
    "#                 if window_start_mode == \"random\":\n",
    "#                     cut_start = torch.randint(0, batch_length - window_length + 1, (1,)).item()\n",
    "#                 elif window_start_mode == \"fixed\":\n",
    "#                     cut_start = 0\n",
    "#                 if window_mode == \"uniform\":\n",
    "#                     while True:\n",
    "#                         window_length = torch.randint(min_window, batch_length + 1, (1,)).item()\n",
    "#                         cut_end = cut_start + window_length\n",
    "#                         if min_window <= (cut_end - cut_start) <= batch_length:\n",
    "#                             break\n",
    "#                     batch = batch[:, cut_start:cut_end, :]\n",
    "\n",
    "#                 elif window_mode == \"negative_binomial\":\n",
    "#                     total_count = 1\n",
    "#                     probs = neg_bin_p\n",
    "#                     distribution = torch.distributions.NegativeBinomial(total_count=total_count, probs=probs)\n",
    "#                     while True:\n",
    "#                         window_length = distribution.sample().item() + min_window\n",
    "#                         cut_end = cut_start + window_length\n",
    "#                         if min_window <= window_length <= batch_length:\n",
    "#                             break\n",
    "#                     batch = batch[:, cut_start:cut_end, :]\n",
    "\n",
    "#                 elif window_mode == \"biased_loss\":\n",
    "#                     if torch.min(window_counts) < 2:\n",
    "#                         # Use uniform distribution until each length has been used at least twice\n",
    "#                         window_probs = torch.ones_like(window_losses) / len(window_losses)\n",
    "#                     elif torch.sum(window_counts) % train_on_all_every == 0:\n",
    "#                         window_probs = torch.ones_like(window_losses) / len(window_losses)\n",
    "#                     else:\n",
    "#                         # Update probabilities based on moving average of losses\n",
    "#                         avg_losses = torch.tensor([np.mean(loss_deque) if len(loss_deque) > 0 else initial_value for loss_deque in loss_deques], device=device)\n",
    "#                         window_probs = avg_losses / avg_losses.sum()\n",
    "#                     while True:\n",
    "#                         window_length = torch.multinomial(window_probs, 1).item() + min_window\n",
    "#                         #check if the window length does work with the batch length\n",
    "#                         if window_length > batch_length:\n",
    "#                             continue\n",
    "#                         cut_end = cut_start + window_length\n",
    "#                         if min_window <= window_length <= batch_length:\n",
    "#                             break\n",
    "#                     batch = batch[:, cut_start:cut_end, :]\n",
    "#                     window_counts[window_length - min_window] += 1  # Update window counts\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "#             predicted_noise, noise, noise_mask = model(batch)\n",
    "#             loss = loss_func(predicted_noise, noise, noise_mask)\n",
    "#             loss.backward()\n",
    "#             # # Gradient clipping\n",
    "#             max_grad_norm = 1.0\n",
    "#             torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "#             torch.nn.utils.clip_grad_norm_(batch_embedder.parameters(), max_grad_norm)\n",
    "#             optimizer.step()\n",
    "#             loss_list.append(loss.item())\n",
    "\n",
    "#             epoch_loss = sum(loss_list[-len(data_loader):]) / len(data_loader)\n",
    "#             epoch_loss_list.append(epoch_loss)\n",
    "\n",
    "#             # Update window losses and moving average deque\n",
    "#             if windowed_mode and window_mode == \"biased_loss\":\n",
    "#                 window_idx = window_length - min_window\n",
    "#                 window_losses[window_idx] += loss.item()\n",
    "#                 loss_deques[window_idx].append(loss.item())\n",
    "\n",
    "#             # Dynamic plot update\n",
    "#             if i % plot_every == 0:\n",
    "#                 ax1.clear()\n",
    "#                 ax1.set_ylim(0, 1)\n",
    "#                 ax1.plot(loss_list)\n",
    "#                 if len(loss_list) > 100:\n",
    "#                     ax1.plot(np.convolve(loss_list, np.ones((100,))/100, mode='valid'))\n",
    "#                     ax1.text(len(loss_list) - 1, np.convolve(loss_list, np.ones((100,))/100, mode='valid')[-1],\n",
    "#                             str(round(np.convolve(loss_list, np.ones((100,))/100, mode='valid')[-1], 3)))\n",
    "#                 if len(epoch_loss_list) > 0:\n",
    "#                     ax1.text(0.1, 0.9, f\"Epoch: {epoch} | Learning rate: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "#                 # ax1.text(0.1, 0.8, f\"Learning rate: {optimizer.param_groups[0]['lr']:.4e}\")\n",
    "#                 ax1.text(0.1, 0.8, f\"Loss: {epoch_loss_list[-1]:.3e} | Validation loss: {val_loss:.3e}\")\n",
    "#                 ax1.text(0.1, 0.7, f\"Time per step: {((time.time() - start) / (i + 1)):.2f} s | Time per epoch: {((time.time() - start) / (i + 1) * len(data_loader)):.2f} s\")\n",
    "#                 ax1.text(0.1, 0.6, f\"Time till finish (est.): {((time.time() - start) / (i + 1) * len(data_loader) * (epochs - epoch)) / 60:.2f} min\")\n",
    "#                 if windowed_mode and window_mode == \"biased_loss\":\n",
    "#                     ax2.clear()\n",
    "#                     ax2.bar(range(min_window, max_window + 1), window_counts.cpu().numpy())\n",
    "#                     ax2.set_ylabel(\"Counts\")\n",
    "#                     ax2.set_title(\"Counts of Each Window Length Used\")\n",
    "\n",
    "#                     moving_avg_losses = torch.tensor([np.mean(loss_deque) if len(loss_deque) > 0 else initial_value for loss_deque in loss_deques], device=device).cpu().numpy()\n",
    "#                     ax3.clear()\n",
    "#                     ax3.bar(range(min_window, max_window + 1), moving_avg_losses)\n",
    "#                     ax3.set_xlabel(\"Window Length\")\n",
    "#                     ax3.set_ylabel(\"Moving Average Loss\")\n",
    "#                     ax3.set_title(\"Moving Average Loss for Each Window Length\")\n",
    "\n",
    "#                 display(fig)\n",
    "#                 clear_output(wait=True)\n",
    "\n",
    "#         end = time.time()\n",
    "\n",
    "#         # Validation\n",
    "#         if epoch % validation_frequency == 0:\n",
    "#             loss_list_validation = []\n",
    "#             for i, batch in enumerate(data_loader_validation):\n",
    "#                 batch = batch.to(device)\n",
    "#                 batch = batch_embedder(batch)\n",
    "#                 if i % validation_prp == 0:\n",
    "#                     predicted_noise, noise, noise_mask = model(batch)\n",
    "#                     loss = loss_func(predicted_noise, noise, noise_mask)\n",
    "#                     loss_list_validation.append(loss.item())\n",
    "\n",
    "#             val_loss = np.mean(loss_list_validation)\n",
    "\n",
    "#         if verbose:\n",
    "#             print(f\"Epoch {epoch} completed in {end - start} seconds, Loss: {epoch_loss}\")\n",
    "#             print(f\"Validation Loss: {val_loss}\")\n",
    "\n",
    "\n",
    "#     return model, loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import random\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torch.utils.data import Sampler\n",
    "# from collections import defaultdict\n",
    "# import torch\n",
    "# from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "# class CustomTimeSeriesDataset(Dataset):\n",
    "#     def __init__(\n",
    "#         self, data_tensor, sequence_lengths, min_seq_length=None, max_seq_length=None\n",
    "#     ):\n",
    "#         # Store the initial sequences and lengths\n",
    "#         self.data_tensor = data_tensor\n",
    "#         self.sequence_lengths = sequence_lengths\n",
    "#         self.min_seq_length = min_seq_length\n",
    "#         self.max_seq_length = max_seq_length\n",
    "\n",
    "#         # Filter sequences based on min and max sequence length\n",
    "#         if self.min_seq_length is not None:\n",
    "#             valid_indices = [\n",
    "#                 i\n",
    "#                 for i, length in enumerate(self.sequence_lengths)\n",
    "#                 if length >= self.min_seq_length\n",
    "#             ]\n",
    "#         else:\n",
    "#             valid_indices = list(range(len(self.sequence_lengths)))\n",
    "\n",
    "#         if self.max_seq_length is not None:\n",
    "#             valid_indices = [\n",
    "#                 i\n",
    "#                 for i in valid_indices\n",
    "#                 if self.sequence_lengths[i] <= self.max_seq_length\n",
    "#             ]\n",
    "\n",
    "#         self.data_tensor = self.data_tensor[valid_indices]\n",
    "#         self.sequence_lengths = [self.sequence_lengths[i]\n",
    "#                                  for i in valid_indices]\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.sequence_lengths)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         seq_length = int(self.sequence_lengths[idx])\n",
    "#         return self.data_tensor[idx, :seq_length, :], seq_length\n",
    "\n",
    "\n",
    "# # class CustomTimeSeriesDataset(Dataset):\n",
    "# #     def __init__(self, data_tensor, sequence_lengths, min_seq_length=None, max_seq_length=None):\n",
    "# #         self.data_tensor = data_tensor\n",
    "# #         self.sequence_lengths = sequence_lengths\n",
    "\n",
    "# #     def __len__(self):\n",
    "# #         return len(self.sequence_lengths)\n",
    "\n",
    "# #     def __getitem__(self, idx):\n",
    "# #         seq_length = int(self.sequence_lengths[idx])\n",
    "# #         return self.data_tensor[idx, :seq_length, :], seq_length\n",
    "\n",
    "\n",
    "# class LengthBatchSampler(Sampler):\n",
    "#     def __init__(self, dataset, batch_size):\n",
    "#         self.sequence_lengths = dataset.sequence_lengths\n",
    "#         self.batch_size = batch_size\n",
    "#         self.batches = self._create_batches()\n",
    "\n",
    "#     def _create_batches(self):\n",
    "#         length_to_indices = defaultdict(list)\n",
    "#         for idx, length in enumerate(self.sequence_lengths):\n",
    "#             length_to_indices[length].append(idx)\n",
    "\n",
    "#         batches = []\n",
    "#         for length, indices in length_to_indices.items():\n",
    "#             # Split indices into batches of the specified batch size\n",
    "#             for i in range(0, len(indices), self.batch_size):\n",
    "#                 batches.append(indices[i: i + self.batch_size])\n",
    "#         return batches\n",
    "\n",
    "#     def __iter__(self):\n",
    "#         for batch in self.batches:\n",
    "#             yield batch\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.batches)\n",
    "\n",
    "\n",
    "# def collate_fn(batch):\n",
    "#     data, lengths = zip(*batch)\n",
    "#     data = torch.stack(data)\n",
    "#     return data\n",
    "\n",
    "\n",
    "# def create_dataloader(\n",
    "#     data_tensor, sequence_lengths, batch_size, min_seq_length=None, max_seq_length=None\n",
    "# ):\n",
    "#     dataset = CustomTimeSeriesDataset(\n",
    "#         data_tensor, sequence_lengths, min_seq_length, max_seq_length\n",
    "#     )\n",
    "#     sampler = LengthBatchSampler(dataset, batch_size)\n",
    "#     dataloader = DataLoader(\n",
    "#         dataset, batch_sampler=sampler, collate_fn=collate_fn)\n",
    "#     return dataloader\n",
    "\n",
    "\n",
    "# class BalancedLengthBatchSampler(Sampler):\n",
    "#     def __init__(self, dataset, batch_size, balance_factor=1.0):\n",
    "#         self.sequence_lengths = dataset.sequence_lengths\n",
    "#         self.batch_size = batch_size\n",
    "#         self.balance_factor = balance_factor\n",
    "#         self.batches = self._create_balanced_batches()\n",
    "\n",
    "#     def _create_balanced_batches(self):\n",
    "#         # Group indices by sequence length\n",
    "#         length_to_indices = defaultdict(list)\n",
    "#         for idx, length in enumerate(self.sequence_lengths):\n",
    "#             length_to_indices[length].append(idx)\n",
    "\n",
    "#         # Calculate the maximum count of indices for balancing\n",
    "#         max_count = max(len(indices) for indices in length_to_indices.values())\n",
    "\n",
    "#         # Balance the distribution of sequence lengths by oversampling shorter sequences\n",
    "#         balanced_batches = []\n",
    "#         for length, indices in length_to_indices.items():\n",
    "#             count = len(indices)\n",
    "#             if count < max_count:\n",
    "#                 repeat_factor = int(self.balance_factor * (max_count / count))\n",
    "#                 # Correctly oversample the list elements\n",
    "#                 oversampled_indices = indices * repeat_factor\n",
    "#                 # oversampled_indices = oversampled_indices[:max_count]\n",
    "#             else:\n",
    "#                 oversampled_indices = indices\n",
    "\n",
    "#             # Shuffle the indices of this particular length\n",
    "#             random.shuffle(oversampled_indices)\n",
    "\n",
    "#             # Create batches for this length\n",
    "#             for i in range(0, len(oversampled_indices), self.batch_size):\n",
    "#                 batch = oversampled_indices[i: i + self.batch_size]\n",
    "#                 if len(batch) == self.batch_size:\n",
    "#                     balanced_batches.append(batch)\n",
    "\n",
    "#         # Shuffle the list of balanced batches to ensure random order\n",
    "#         random.shuffle(balanced_batches)\n",
    "\n",
    "#         return balanced_batches\n",
    "\n",
    "#     def __iter__(self):\n",
    "#         for batch in self.batches:\n",
    "#             yield batch\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.batches)\n",
    "\n",
    "\n",
    "# def create_balanced_dataloader(\n",
    "#     data_tensor,\n",
    "#     sequence_lengths,\n",
    "#     batch_size,\n",
    "#     balance_factor=1.0,\n",
    "#     min_seq_length=None,\n",
    "#     max_seq_length=None,\n",
    "# ):\n",
    "#     dataset = CustomTimeSeriesDataset(\n",
    "#         data_tensor, sequence_lengths, min_seq_length, max_seq_length\n",
    "#     )\n",
    "#     sampler = BalancedLengthBatchSampler(dataset, batch_size, balance_factor)\n",
    "#     dataloader = DataLoader(\n",
    "#         dataset, batch_sampler=sampler, collate_fn=collate_fn)\n",
    "#     return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = create_balanced_dataloader(\n",
    "#     training_data_tensor,\n",
    "#     training_data_sequence_lengths,\n",
    "#     batch_size=100,\n",
    "#     balance_factor=1.0,\n",
    "#     min_seq_length=2,\n",
    "#     max_seq_length=None,\n",
    "# )\n",
    "\n",
    "# val_loader = create_balanced_dataloader(\n",
    "#     validation_data_tensor,\n",
    "#     validation_data_sequence_lengths,\n",
    "#     batch_size=100,\n",
    "#     balance_factor=1.0,\n",
    "#     min_seq_length=2,\n",
    "#     max_seq_length=None,\n",
    "# )\n",
    "\n",
    "# # check the size of the train, val, and test sets\n",
    "# print(len(train_loader))\n",
    "# print(len(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_data_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical_indices_sizes = {\n",
    "#     # 'time_step' : [0, 1],\n",
    "#     \"chemo_application_prev\": [0, 1],\n",
    "#     \"radio_application_prev\": [1, 1],\n",
    "#     \"patient_type_tile\": [2, 1],\n",
    "# }\n",
    "\n",
    "# numerical_indices = {\"cancer_volume\": 3}\n",
    "\n",
    "# numerical_indices = {\n",
    "#     'chemo_application_prev': 0,\n",
    "#     'radio_application_prev': 1,\n",
    "#     'patient_type_tile': 2,\n",
    "#     'cancer_volume': 3\n",
    "# }\n",
    "\n",
    "\n",
    "# training_data_tensor_embedded = data_embedder(training_data_tensor)\n",
    "# validation_data_tensor_embedded = data_embedder(validation_data_tensor)\n",
    "# test_data_factuals_tensor_embedded = data_embedder(test_data_factuals_tensor)\n",
    "# test_data_counterfactuals_tensor_embedded = data_embedder(test_data_counterfactuals_tensor)\n",
    "# test_data_seq_tensor_embedded = data_embedder(test_data_seq_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diffusion_imputer = diffusion_imputation(\n",
    "#     emb_dim=128,\n",
    "#     # strategy=\"forecasting_last_n_time\",\n",
    "#     # strategy=\"random\",\n",
    "#     # missing_prp=0.5,\n",
    "#     # strategy='selected_features',\n",
    "#     strategy=\"selected_features_last_n_time\",\n",
    "#     last_n_time=1,\n",
    "#     features_to_impute=[0, 1, 2, 3],\n",
    "#     # excluded_features = [i for i in range(6)], #[2],#[0,1,2,3,5], #for the embedded stock names which we don't need to predict\n",
    "#     # strategy='selected_features_and_selected_features_after_time',\n",
    "#     # features_to_impute_completely=[2],\n",
    "#     # features_to_impute_after_time=[3],\n",
    "#     num_residual_layers=2,\n",
    "#     diffusion_steps=50,\n",
    "#     diffusion_beta_schedule=\"quadratic\",\n",
    "#     num_heads=8,\n",
    "#     kernel_size=(1, 1),\n",
    "#     ff_dim=512,\n",
    "#     num_cells=1,\n",
    "#     dropout=0,\n",
    "#     # csdi, csdi_moded_transformer, rsa, rsa_moded_transformer, moded_transformer_alone, rsa_csdi\n",
    "#     method=\"rsa_moded_transformer\",\n",
    "#     device=\"cuda\",\n",
    "# )\n",
    "\n",
    "# # data_embedder = DataEmbedder(\n",
    "# #     categorical_indices_sizes, numerical_indices, training_data_tensor\n",
    "# # )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
