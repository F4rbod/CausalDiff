{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import enum\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "import zipfile\n",
    "import sys\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import copy\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0.dev20240808+cu121\n",
      "12.1\n"
     ]
    }
   ],
   "source": [
    "# show pytorch version and cuda version\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## collecting and preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/work/postresearch/Shared/Researchers/Farbod/cancer/code/CausalDiff'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change working directory\n",
    "os.chdir(\"/work/postresearch/Shared/Researchers/Farbod/cancer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cancer data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = 0\n",
    "# read from pickle file\n",
    "training_data = pd.read_pickle(f\"training_data_tau_{tau}.pkl\")\n",
    "validation_data = pd.read_pickle(f\"validation_data_tau_{tau}.pkl\")\n",
    "\n",
    "test_data_factuals = pd.read_pickle(f\"test_data_factuals_tau_{tau}.pkl\")\n",
    "#:return: simulated data dict with number of rows equal to num_patients * seq_length * num_treatments\n",
    "test_data_counterfactuals = pd.read_pickle(f\"test_data_counterfactuals_tau_{tau}.pkl\")\n",
    "#:return: simulated data dict with number of rows equal to num_patients * seq_length * 2 * projection_horizon\n",
    "test_data_seq = pd.read_pickle(f\"test_data_seq_tau_{tau}.pkl\")\n",
    "means = pd.read_pickle(f\"means_tau_{tau}.pkl\")\n",
    "stds = pd.read_pickle(f\"stds_tau_{tau}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "means[\"chemo_application\"] = 0\n",
    "means[\"radio_application\"] = 0\n",
    "stds[\"chemo_application\"] = 1\n",
    "stds[\"radio_application\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cancer_volume        9.796984\n",
       "chemo_dosage         4.789335\n",
       "radio_dosage         0.979020\n",
       "patient_types        2.000600\n",
       "chemo_application    0.000000\n",
       "radio_application    0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "cancer_volume        63.363042\n",
       "chemo_dosage          2.948792\n",
       "radio_dosage          0.999780\n",
       "patient_types         0.819268\n",
       "chemo_application     1.000000\n",
       "radio_application     1.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means\n",
    "stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the data\n",
    "for key in training_data.keys():\n",
    "    if key in means.keys():\n",
    "        training_data[key] = (training_data[key] - means[key]) / stds[key]\n",
    "        validation_data[key] = (validation_data[key] - means[key]) / stds[key]\n",
    "        test_data_factuals[key] = (test_data_factuals[key] - means[key]) / stds[key]\n",
    "\n",
    "for key in test_data_counterfactuals.keys():\n",
    "    if key in means.keys():\n",
    "        test_data_counterfactuals[key] = (\n",
    "            test_data_counterfactuals[key] - means[key]\n",
    "        ) / stds[key]\n",
    "        test_data_seq[key] = (test_data_seq[key] - means[key]) / stds[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tile_patient_types(data: dict):\n",
    "    data[\"patient_type_tile\"] = np.tile(\n",
    "        data[\"patient_types\"][:, None], (1, data[\"cancer_volume\"].shape[1])\n",
    "    )\n",
    "    return data\n",
    "\n",
    "\n",
    "training_data = tile_patient_types(training_data)\n",
    "validation_data = tile_patient_types(validation_data)\n",
    "test_data_factuals = tile_patient_types(test_data_factuals)\n",
    "test_data_counterfactuals = tile_patient_types(test_data_counterfactuals)\n",
    "test_data_seq = tile_patient_types(test_data_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon = 1\n",
    "offset = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_types = training_data[\"patient_type_tile\"]\n",
    "cancer_volume = training_data[\"cancer_volume\"]\n",
    "\n",
    "current_covariates = np.concatenate(\n",
    "    [cancer_volume[:, :-offset, np.newaxis], patient_types[:, :-offset, np.newaxis]],\n",
    "    axis=-1,\n",
    ")\n",
    "outputs = cancer_volume[:, horizon:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a column that indicates the application of chemo and radio at time t-1\n",
    "def add_previous_treatment(data: dict):\n",
    "    data[\"chemo_application_prev\"] = np.roll(data[\"chemo_application\"], offset, axis=1)\n",
    "    data[\"radio_application_prev\"] = np.roll(data[\"radio_application\"], offset, axis=1)\n",
    "    data[\"chemo_application_prev\"][:, 0] = 0\n",
    "    data[\"radio_application_prev\"][:, 0] = 0\n",
    "    return data\n",
    "\n",
    "\n",
    "training_data = add_previous_treatment(training_data)\n",
    "validation_data = add_previous_treatment(validation_data)\n",
    "test_data_factuals = add_previous_treatment(test_data_factuals)\n",
    "test_data_counterfactuals = add_previous_treatment(test_data_counterfactuals)\n",
    "test_data_seq = add_previous_treatment(test_data_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "chemo_application = training_data[\"chemo_application\"]\n",
    "radio_application = training_data[\"radio_application\"]\n",
    "\n",
    "treatments = np.concatenate(\n",
    "    [\n",
    "        chemo_application[:, :-offset, np.newaxis],\n",
    "        radio_application[:, :-offset, np.newaxis],\n",
    "    ],\n",
    "    axis=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence lengths is reported once per patient, so we tile it to match the shape of the data\n",
    "def add_sequence_lengths(data: dict):\n",
    "    data[\"time_step\"] = np.tile(\n",
    "        np.arange(data[\"cancer_volume\"].shape[1]), (data[\"cancer_volume\"].shape[0], 1)\n",
    "    )\n",
    "    data[\"sequence_length_tile\"] = np.tile(\n",
    "        data[\"sequence_lengths\"][:, None], (1, data[\"cancer_volume\"].shape[1])\n",
    "    )\n",
    "    data[\"distance_from_end\"] = data[\"sequence_length_tile\"] - data[\"time_step\"]\n",
    "    # distance_from_end_scaled is a value between 0 and 1 that indicates how far we are from the end of the sequence for each patient\n",
    "    data[\"distance_from_end_scaled\"] = (\n",
    "        data[\"distance_from_end\"] / data[\"sequence_length_tile\"]\n",
    "    )\n",
    "    data[\"active_entries\"] = data[\"distance_from_end\"] > 0\n",
    "    return data\n",
    "\n",
    "\n",
    "training_data = add_sequence_lengths(training_data)\n",
    "validation_data = add_sequence_lengths(validation_data)\n",
    "test_data_factuals = add_sequence_lengths(test_data_factuals)\n",
    "test_data_counterfactuals = add_sequence_lengths(test_data_counterfactuals)\n",
    "test_data_seq = add_sequence_lengths(test_data_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_entries = np.zeros(outputs.shape)\n",
    "\n",
    "for i in range(training_data[\"sequence_lengths\"].shape[0]):\n",
    "    sequence_length = int(training_data[\"sequence_lengths\"][i])\n",
    "    active_entries[i, :sequence_length, :] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_unscaled_data(data: dict, means=means, stds=stds):\n",
    "    temp_data = copy.deepcopy(data)\n",
    "    for key in data.keys():\n",
    "        if key in means.keys():\n",
    "            var_name = key + \"_unscaled\"\n",
    "            temp_data[var_name] = data[key] * stds[key] + means[key]\n",
    "    return temp_data\n",
    "\n",
    "\n",
    "training_data = add_unscaled_data(training_data)\n",
    "validation_data = add_unscaled_data(validation_data)\n",
    "test_data_factuals = add_unscaled_data(test_data_factuals)\n",
    "test_data_counterfactuals = add_unscaled_data(test_data_counterfactuals)\n",
    "test_data_seq = add_unscaled_data(test_data_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "unscaled_outputs = outputs * stds[\"cancer_volume\"] + means[\"cancer_volume\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_outputs = current_covariates[:, :, :1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "treatments = np.concatenate(\n",
    "    [\n",
    "        training_data[\"chemo_application\"][:, :-offset, np.newaxis],\n",
    "        training_data[\"radio_application\"][:, :-offset, np.newaxis],\n",
    "    ],\n",
    "    axis=-1,\n",
    ")\n",
    "\n",
    "current_treatments = treatments\n",
    "prev_treatments = treatments[:, :-1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_init_treatment = np.zeros(\n",
    "    shape=[current_covariates.shape[0], 1, prev_treatments.shape[-1]]\n",
    ")\n",
    "prev_treatments = np.concatenate([zero_init_treatment, prev_treatments], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_means = means[[\"cancer_volume\"]].values.flatten()[0]\n",
    "output_stds = stds[[\"cancer_volume\"]].values.flatten()[0]\n",
    "\n",
    "scaling_params = {\n",
    "    \"input_means\": means,\n",
    "    \"inputs_stds\": stds,\n",
    "    \"output_means\": means[[\"cancer_volume\"]].values.flatten()[0],\n",
    "    \"output_stds\": stds[[\"cancer_volume\"]].values.flatten()[0],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_data_sequence_lengths = training_data['sequence_lengths']\n",
    "# validation_data_sequence_lengths = validation_data['sequence_lengths']\n",
    "# test_data_factuals_sequence_lengths = test_data_factuals['sequence_lengths']\n",
    "# test_data_counterfactuals_sequence_lengths = test_data_counterfactuals['sequence_lengths']\n",
    "# test_data_seq_sequence_lengths = test_data_seq['sequence_lengths']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed_keys = [\n",
    "#     'chemo_application_prev', 'radio_application_prev', 'patient_type_tile', 'cancer_volume'\n",
    "# ]\n",
    "# #\n",
    "# training_data = {key: training_data[key] for key in needed_keys}\n",
    "# validation_data = {key: validation_data[key] for key in needed_keys}\n",
    "# test_data_factuals = {key: test_data_factuals[key] for key in needed_keys}\n",
    "# test_data_counterfactuals = {key: test_data_counterfactuals[key] for key in needed_keys}\n",
    "# test_data_seq = {key: test_data_seq[key] for key in needed_keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #convert tensors where the first dimension is the number of patients the second dimension is time and the third is the features\n",
    "# #the keys are the features\n",
    "# #within the features, the first dimension is the number of patients, the second is time\n",
    "\n",
    "# #convert the keys to a dimension in the tensor\n",
    "# def dictionary_to_tensor(data_dict):\n",
    "#     # Extract keys and values from the dictionary\n",
    "#     keys = list(data_dict.keys())\n",
    "#     values = [data_dict[key] for key in keys]\n",
    "\n",
    "#     # Check consistency in dimensions\n",
    "#     num_patients = values[0].shape[0]\n",
    "#     time_steps = values[0].shape[1]\n",
    "\n",
    "#     for table in values:\n",
    "#         assert table.shape[0] == num_patients, \"Number of patients mismatch.\"\n",
    "#         assert table.shape[1] == time_steps, \"Number of time steps mismatch.\"\n",
    "\n",
    "#     # Stack tables along the new feature dimension\n",
    "#     tensor = np.stack(values, axis=-1)\n",
    "#     tensor = torch.tensor(tensor, dtype=torch.float32)\n",
    "\n",
    "#     return tensor, keys\n",
    "\n",
    "# training_data_tensor, keys = dictionary_to_tensor(training_data)\n",
    "# validation_data_tensor, keys = dictionary_to_tensor(validation_data)\n",
    "# test_data_factuals_tensor, keys = dictionary_to_tensor(test_data_factuals)\n",
    "# test_data_counterfactuals_tensor, keys = dictionary_to_tensor(test_data_counterfactuals)\n",
    "# test_data_seq_tensor, keys = dictionary_to_tensor(test_data_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data for CT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class DataPreparer:\n",
    "    def __init__(self, data, means, stds, horizon=1, offset=1):\n",
    "        self.data = data\n",
    "        self.means = means\n",
    "        self.stds = stds\n",
    "        self.horizon = horizon\n",
    "        self.offset = offset\n",
    "        self.training_data_ct = {}\n",
    "        self.scaling_params = {}\n",
    "\n",
    "    def prepare_data(self):\n",
    "        training_data = self.data\n",
    "        patient_types = training_data[\"patient_type_tile\"]\n",
    "        cancer_volume = training_data[\"cancer_volume\"]\n",
    "        input_means = self.means\n",
    "        input_stds = self.stds\n",
    "        output_means = self.means[[\"cancer_volume\"]].values.flatten()[0]\n",
    "        output_stds = self.stds[[\"cancer_volume\"]].values.flatten()[0]\n",
    "\n",
    "        chemo_application = training_data[\"chemo_application\"]\n",
    "        radio_application = training_data[\"radio_application\"]\n",
    "        sequence_lengths = training_data[\"sequence_lengths\"]\n",
    "\n",
    "        self.training_data_ct[\"current_covariates\"] = np.concatenate(\n",
    "            [\n",
    "                training_data[\"cancer_volume\"][:, : -self.horizon, np.newaxis],\n",
    "                training_data[\"patient_type_tile\"][:, : -self.horizon, np.newaxis],\n",
    "            ],\n",
    "            axis=-1,\n",
    "        )\n",
    "        self.training_data_ct[\"outputs\"] = training_data[\"cancer_volume\"][\n",
    "            :, self.horizon :, np.newaxis\n",
    "        ]\n",
    "        self.training_data_ct[\"active_entries\"] = training_data[\"active_entries\"][\n",
    "            :, : -self.offset, np.newaxis\n",
    "        ]\n",
    "        self.training_data_ct[\"unscaled_outputs\"] = training_data[\n",
    "            \"cancer_volume_unscaled\"\n",
    "        ][:, self.horizon :, np.newaxis]\n",
    "        self.training_data_ct[\"prev_outputs\"] = np.concatenate(\n",
    "            [\n",
    "                training_data[\"cancer_volume\"][:, : -self.horizon, np.newaxis],\n",
    "                training_data[\"patient_type_tile\"][:, : -self.horizon, np.newaxis],\n",
    "            ],\n",
    "            axis=-1,\n",
    "        )[:, :, :1]\n",
    "        self.training_data_ct[\"static_features\"] = np.concatenate(\n",
    "            [\n",
    "                training_data[\"cancer_volume\"][:, : -self.horizon, np.newaxis],\n",
    "                training_data[\"patient_type_tile\"][:, : -self.horizon, np.newaxis],\n",
    "            ],\n",
    "            axis=-1,\n",
    "        )[:, 0, 1:]\n",
    "        self.training_data_ct[\"current_treatments\"] = np.concatenate(\n",
    "            [\n",
    "                training_data[\"chemo_application\"][:, : -self.offset, np.newaxis],\n",
    "                training_data[\"radio_application\"][:, : -self.offset, np.newaxis],\n",
    "            ],\n",
    "            axis=-1,\n",
    "        )\n",
    "        self.training_data_ct[\"prev_treatments\"] = np.concatenate(\n",
    "            [\n",
    "                training_data[\"chemo_application_prev\"][:, : -self.offset, np.newaxis],\n",
    "                training_data[\"radio_application_prev\"][:, : -self.offset, np.newaxis],\n",
    "            ],\n",
    "            axis=-1,\n",
    "        )\n",
    "        self.training_data_ct[\"sequence_lengths\"] = training_data[\"sequence_lengths\"]\n",
    "        self.training_data_ct[\"patient_types\"] = patient_types\n",
    "        self.training_data_ct[\"cancer_volume\"] = cancer_volume\n",
    "        self.training_data_ct[\"chemo_application\"] = chemo_application\n",
    "        self.training_data_ct[\"radio_application\"] = radio_application\n",
    "        self.training_data_ct[\"sequence_lengths\"] = sequence_lengths\n",
    "\n",
    "        self.scaling_params = {\n",
    "            \"input_means\": input_means,\n",
    "            \"inputs_stds\": input_stds,\n",
    "            \"output_means\": output_means,\n",
    "            \"output_stds\": output_stds,\n",
    "        }\n",
    "\n",
    "        result = {\"data\": self.training_data_ct, \"scaling_params\": self.scaling_params}\n",
    "\n",
    "        self.prepared_data = result\n",
    "\n",
    "        return result\n",
    "\n",
    "    def explode_trajectories(self, data, scaling_params, projection_horizon):\n",
    "\n",
    "        outputs = data[\"outputs\"]\n",
    "        prev_outputs = data[\"prev_outputs\"]\n",
    "        sequence_lengths = data[\"sequence_lengths\"]\n",
    "        active_entries = data[\"active_entries\"]\n",
    "        current_treatments = data[\"current_treatments\"]\n",
    "        previous_treatments = data[\"prev_treatments\"]\n",
    "        static_features = data[\"static_features\"]\n",
    "        # if 'stabilized_weights' in data:\n",
    "        #     stabilized_weights = data['stabilized_weights']\n",
    "\n",
    "        num_patients, max_seq_length, num_features = outputs.shape\n",
    "        num_seq2seq_rows = num_patients * max_seq_length\n",
    "\n",
    "        seq2seq_previous_treatments = np.zeros(\n",
    "            (num_seq2seq_rows, max_seq_length, previous_treatments.shape[-1])\n",
    "        )\n",
    "        seq2seq_current_treatments = np.zeros(\n",
    "            (num_seq2seq_rows, max_seq_length, current_treatments.shape[-1])\n",
    "        )\n",
    "        seq2seq_static_features = np.zeros(\n",
    "            (num_seq2seq_rows, static_features.shape[-1])\n",
    "        )\n",
    "        seq2seq_outputs = np.zeros(\n",
    "            (num_seq2seq_rows, max_seq_length, outputs.shape[-1])\n",
    "        )\n",
    "        seq2seq_prev_outputs = np.zeros(\n",
    "            (num_seq2seq_rows, max_seq_length, prev_outputs.shape[-1])\n",
    "        )\n",
    "        # seq2seq_vitals = np.zeros((num_seq2seq_rows, max_seq_length, vitals.shape[-1]))\n",
    "        # seq2seq_next_vitals = np.zeros((num_seq2seq_rows, max_seq_length - 1, next_vitals.shape[-1]))\n",
    "        seq2seq_active_entries = np.zeros(\n",
    "            (num_seq2seq_rows, max_seq_length, active_entries.shape[-1])\n",
    "        )\n",
    "        seq2seq_sequence_lengths = np.zeros(num_seq2seq_rows)\n",
    "        # if 'stabilized_weights' in data:\n",
    "        #     seq2seq_stabilized_weights = np.zeros(\n",
    "        #         (num_seq2seq_rows, max_seq_length))\n",
    "\n",
    "        total_seq2seq_rows = 0  # we use this to shorten any trajectories later\n",
    "\n",
    "        for i in range(num_patients):\n",
    "            sequence_length = int(sequence_lengths[i])\n",
    "\n",
    "            for t in range(\n",
    "                projection_horizon, sequence_length\n",
    "            ):  # shift outputs back by 1\n",
    "                seq2seq_active_entries[total_seq2seq_rows, : (t + 1), :] = (\n",
    "                    active_entries[i, : (t + 1), :]\n",
    "                )\n",
    "                # if 'stabilized_weights' in data:\n",
    "                #     seq2seq_stabilized_weights[total_seq2seq_rows, :(\n",
    "                #         t + 1)] = stabilized_weights[i, :(t + 1)]\n",
    "                seq2seq_previous_treatments[total_seq2seq_rows, : (t + 1), :] = (\n",
    "                    previous_treatments[i, : (t + 1), :]\n",
    "                )\n",
    "                seq2seq_current_treatments[total_seq2seq_rows, : (t + 1), :] = (\n",
    "                    current_treatments[i, : (t + 1), :]\n",
    "                )\n",
    "                seq2seq_outputs[total_seq2seq_rows, : (t + 1), :] = outputs[\n",
    "                    i, : (t + 1), :\n",
    "                ]\n",
    "                seq2seq_prev_outputs[total_seq2seq_rows, : (t + 1), :] = prev_outputs[\n",
    "                    i, : (t + 1), :\n",
    "                ]\n",
    "                # seq2seq_vitals[total_seq2seq_rows, :(t + 1), :] = vitals[i, :(t + 1), :]\n",
    "                # seq2seq_next_vitals[total_seq2seq_rows, :min(t + 1, sequence_length - 1), :] = \\\n",
    "                #     next_vitals[i, :min(t + 1, sequence_length - 1), :]\n",
    "                seq2seq_sequence_lengths[total_seq2seq_rows] = t + 1\n",
    "                seq2seq_static_features[total_seq2seq_rows] = static_features[i]\n",
    "\n",
    "                total_seq2seq_rows += 1\n",
    "\n",
    "        # Filter everything shorter\n",
    "        seq2seq_previous_treatments = seq2seq_previous_treatments[\n",
    "            :total_seq2seq_rows, :, :\n",
    "        ]\n",
    "        seq2seq_current_treatments = seq2seq_current_treatments[\n",
    "            :total_seq2seq_rows, :, :\n",
    "        ]\n",
    "        seq2seq_static_features = seq2seq_static_features[:total_seq2seq_rows, :]\n",
    "        seq2seq_outputs = seq2seq_outputs[:total_seq2seq_rows, :, :]\n",
    "        seq2seq_prev_outputs = seq2seq_prev_outputs[:total_seq2seq_rows, :, :]\n",
    "        # seq2seq_vitals = seq2seq_vitals[:total_seq2seq_rows, :, :]\n",
    "        # seq2seq_next_vitals = seq2seq_next_vitals[:total_seq2seq_rows, :, :]\n",
    "        seq2seq_active_entries = seq2seq_active_entries[:total_seq2seq_rows, :, :]\n",
    "        seq2seq_sequence_lengths = seq2seq_sequence_lengths[:total_seq2seq_rows]\n",
    "\n",
    "        # if 'stabilized_weights' in data:\n",
    "        #     seq2seq_stabilized_weights = seq2seq_stabilized_weights[:total_seq2seq_rows]\n",
    "\n",
    "        new_data = {\n",
    "            \"prev_treatments\": seq2seq_previous_treatments,\n",
    "            \"current_treatments\": seq2seq_current_treatments,\n",
    "            \"static_features\": seq2seq_static_features,\n",
    "            \"prev_outputs\": seq2seq_prev_outputs,\n",
    "            \"outputs\": seq2seq_outputs,\n",
    "            # 'vitals': seq2seq_vitals,\n",
    "            # 'next_vitals': seq2seq_next_vitals,\n",
    "            \"unscaled_outputs\": seq2seq_outputs * scaling_params[\"output_stds\"]\n",
    "            + scaling_params[\"output_means\"],\n",
    "            \"sequence_lengths\": seq2seq_sequence_lengths,\n",
    "            \"active_entries\": seq2seq_active_entries,\n",
    "        }\n",
    "        # if 'stabilized_weights' in data:\n",
    "        #     new_data['stabilized_weights'] = seq2seq_stabilized_weights\n",
    "\n",
    "        return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preparer = DataPreparer(training_data, means, stds)\n",
    "result = data_preparer.prepare_data()\n",
    "\n",
    "training_data_ct = result[\"data\"]\n",
    "scaling_params = result[\"scaling_params\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['prev_treatments', 'current_treatments', 'static_features', 'prev_outputs', 'outputs', 'unscaled_outputs', 'sequence_lengths', 'active_entries'])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_ct_exploded = data_preparer.explode_trajectories(\n",
    "    training_data_ct, scaling_params, horizon\n",
    ")\n",
    "\n",
    "training_data_ct_exploded.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_propensity_scores(exploded_dataset, model_type, lag_features, dim_treatments):\n",
    "\n",
    "    dataset = exploded_dataset\n",
    "    active_entries = dataset['active_entries']\n",
    "    prev_treatments = dataset['prev_treatments']\n",
    "    inputs = (prev_treatments * active_entries).sum(1)\n",
    "    \n",
    "    classifier = model_type\n",
    "\n",
    "    propensity_scores = np.stack(classifier.predict_proba(inputs), 1)[:, :, 1]\n",
    "    propensity_scores = propensity_scores.reshape(dataset['active_entries'].shape[0],\n",
    "                                                    dataset['active_entries'].shape[1] - lag_features,\n",
    "                                                    dim_treatments)\n",
    "    propensity_scores = np.concatenate([0.5 * np.ones((propensity_scores.shape[0], lag_features, dim_treatments)),\n",
    "                                        propensity_scores], axis=1)\n",
    "    return propensity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSM:\n",
    "    \"\"\"\n",
    "    Pytorch-Lightning implementation of Marginal Structural Models (MSMs) (https://pubmed.ncbi.nlm.nih.gov/10955408/)\n",
    "    \"\"\"\n",
    "    model_type = None  # Will be defined in subclasses\n",
    "    possible_model_types = {'msm_regressor',\n",
    "                            'propensity_treatment', 'propensity_history'}\n",
    "    tuning_criterion = None\n",
    "\n",
    "    def __init__(self,\n",
    "                 args: DictConfig,\n",
    "                 dataset_collection: Union[RealDatasetCollection,\n",
    "                                           SyntheticDatasetCollection] = None,\n",
    "                 autoregressive: bool = None,\n",
    "                 has_vitals: bool = None,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            args: DictConfig of model hyperparameters\n",
    "            dataset_collection: Dataset collection\n",
    "            autoregressive: Flag of including previous outcomes to modelling\n",
    "            has_vitals: Flag of vitals in dataset\n",
    "            **kwargs: Other arguments\n",
    "        \"\"\"\n",
    "        super().__init__(args, dataset_collection, autoregressive, has_vitals)\n",
    "        self.lag_features = args.model.lag_features\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        if self.dataset_collection is not None and not self.dataset_collection.processed_data_multi:\n",
    "            # Only binary multilabel regime possible\n",
    "            assert self.hparams.dataset.treatment_mode == 'multilabel'\n",
    "            self.dataset_collection.process_data_multi()\n",
    "\n",
    "    def get_exploded_dataset(self, dataset: Dataset, min_length: int, only_active_entries=True, max_length=None) -> Dataset:\n",
    "        exploded_dataset = deepcopy(dataset)\n",
    "        if max_length is None:\n",
    "            max_length = max(exploded_dataset.data['sequence_lengths'][:])\n",
    "        if not only_active_entries:\n",
    "            exploded_dataset.data['active_entries'][:, :, :] = 1.0\n",
    "            exploded_dataset.data['sequence_lengths'][:] = max_length\n",
    "        exploded_dataset.explode_trajectories(min_length)\n",
    "        return exploded_dataset\n",
    "\n",
    "    def get_propensity_scores(self, dataset: Dataset) -> np.array:\n",
    "        logger.info(f'Propensity scores for {dataset.subset_name}.')\n",
    "        exploded_dataset = self.get_exploded_dataset(\n",
    "            dataset, min_length=self.lag_features, only_active_entries=False)\n",
    "\n",
    "        inputs = self.get_inputs(exploded_dataset)\n",
    "        classifier = getattr(self, self.model_type)\n",
    "\n",
    "        propensity_scores = np.stack(\n",
    "            classifier.predict_proba(inputs), 1)[:, :, 1]\n",
    "        propensity_scores = propensity_scores.reshape(dataset.data['active_entries'].shape[0],\n",
    "                                                      dataset.data['active_entries'].shape[1] -\n",
    "                                                      self.lag_features,\n",
    "                                                      self.dim_treatments)\n",
    "        propensity_scores = np.concatenate([0.5 * np.ones((propensity_scores.shape[0], self.lag_features, self.dim_treatments)),\n",
    "                                            propensity_scores], axis=1)\n",
    "        return propensity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "This LogisticRegression instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LogisticRegression\n\u001b[1;32m      4\u001b[0m model_type \u001b[38;5;241m=\u001b[39m LogisticRegression(max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m propensity_scores \u001b[38;5;241m=\u001b[39m \u001b[43mget_propensity_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_data_ct_exploded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[43], line 10\u001b[0m, in \u001b[0;36mget_propensity_scores\u001b[0;34m(exploded_dataset, model_type, lag_features, dim_treatments)\u001b[0m\n\u001b[1;32m      6\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (prev_treatments \u001b[38;5;241m*\u001b[39m active_entries)\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      8\u001b[0m classifier \u001b[38;5;241m=\u001b[39m model_type\n\u001b[0;32m---> 10\u001b[0m propensity_scores \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack(\u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m1\u001b[39m)[:, :, \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     11\u001b[0m propensity_scores \u001b[38;5;241m=\u001b[39m propensity_scores\u001b[38;5;241m.\u001b[39mreshape(dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactive_entries\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m     12\u001b[0m                                                 dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactive_entries\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m lag_features,\n\u001b[1;32m     13\u001b[0m                                                 dim_treatments)\n\u001b[1;32m     14\u001b[0m propensity_scores \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mones((propensity_scores\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], lag_features, dim_treatments)),\n\u001b[1;32m     15\u001b[0m                                     propensity_scores], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/micromamba/envs/pytorch_env/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1367\u001b[0m, in \u001b[0;36mLogisticRegression.predict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1341\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_proba\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m   1342\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1343\u001b[0m \u001b[38;5;124;03m    Probability estimates.\u001b[39;00m\n\u001b[1;32m   1344\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1365\u001b[0m \u001b[38;5;124;03m        where classes are ordered as they are in ``self.classes_``.\u001b[39;00m\n\u001b[1;32m   1366\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1367\u001b[0m     \u001b[43mcheck_is_fitted\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1369\u001b[0m     ovr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmulti_class \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124movr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   1370\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmulti_class \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1371\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1374\u001b[0m         )\n\u001b[1;32m   1375\u001b[0m     )\n\u001b[1;32m   1376\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ovr:\n",
      "File \u001b[0;32m~/micromamba/envs/pytorch_env/lib/python3.10/site-packages/sklearn/utils/validation.py:1622\u001b[0m, in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m   1619\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not an estimator instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (estimator))\n\u001b[1;32m   1621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_fitted(estimator, attributes, all_or_any):\n\u001b[0;32m-> 1622\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NotFittedError(msg \u001b[38;5;241m%\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(estimator)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m})\n",
      "\u001b[0;31mNotFittedError\u001b[0m: This LogisticRegression instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model_type = LogisticRegression(max_iter=1000)\n",
    "\n",
    "propensity_scores = get_propensity_scores(training_data_ct_exploded, model_type, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
