{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import enum\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "import zipfile\n",
    "import sys\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import copy\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "torch.set_default_dtype(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0.dev20240808+cu121\n",
      "12.1\n",
      "Number of available GPUs: 1\n",
      "GPU 0: NVIDIA A100-SXM4-80GB\n"
     ]
    }
   ],
   "source": [
    "# show pytorch version and cuda version\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of available GPUs: {num_gpus}\")\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    for i in range(num_gpus):\n",
    "        gpu_name = torch.cuda.get_device_name(i)\n",
    "        print(f\"GPU {i}: {gpu_name}\")\n",
    "else:\n",
    "    print(\"CUDA is not available. No GPUs detected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting and preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/work/postresearch/Shared/Researchers/Farbod/cancer/code/CausalDiff'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change working directory\n",
    "os.chdir(\"/work/postresearch/Shared/Researchers/Farbod/cancer/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cancer data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tau=0\n",
    "# #read from pickle file\n",
    "# training_data = pd.read_pickle(f'training_data_tau_{tau}.pkl')\n",
    "# validation_data = pd.read_pickle(f'validation_data_tau_{tau}.pkl')\n",
    "\n",
    "# test_data_factuals = pd.read_pickle(f'test_data_factuals_tau_{tau}.pkl')\n",
    "# #:return: simulated data dict with number of rows equal to num_patients * seq_length * num_treatments\n",
    "# test_data_counterfactuals = pd.read_pickle(f'test_data_counterfactuals_tau_{tau}.pkl')\n",
    "# #:return: simulated data dict with number of rows equal to num_patients * seq_length * 2 * projection_horizon\n",
    "# test_data_seq = pd.read_pickle(f'test_data_seq_tau_{tau}.pkl')\n",
    "# means = pd.read_pickle(f'means_tau_{tau}.pkl')\n",
    "# stds = pd.read_pickle(f'stds_tau_{tau}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# Set the file path\n",
    "tau = 0\n",
    "pickle_file_path = f\"data/new_cancer_sim_{tau}_{tau}.p\"\n",
    "\n",
    "# Read data from the pickle file\n",
    "with open(pickle_file_path, \"rb\") as file:\n",
    "    pickle_map = pickle.load(file)\n",
    "\n",
    "# Extract the relevant data from the pickle map\n",
    "chemo_coeff = pickle_map[\"chemo_coeff\"]\n",
    "radio_coeff = pickle_map[\"radio_coeff\"]\n",
    "num_time_steps = pickle_map[\"num_time_steps\"]\n",
    "training_data = pickle_map[\"training_data\"]\n",
    "validation_data = pickle_map[\"validation_data\"]\n",
    "test_data_counterfactuals = pickle_map[\"test_data\"]\n",
    "test_data_factuals = pickle_map[\"test_data_factuals\"]\n",
    "test_data_seq = pickle_map[\"test_data_seq\"]\n",
    "scaling_data = pickle_map[\"scaling_data\"]\n",
    "means = pickle_map[\"scaling_data\"][0]\n",
    "stds = pickle_map[\"scaling_data\"][1]\n",
    "window_size = pickle_map[\"window_size\"]\n",
    "\n",
    "# Optional: Print the loaded data for verification\n",
    "print(\"Chemo Coeff:\", chemo_coeff)\n",
    "print(\"Radio Coeff:\", radio_coeff)\n",
    "print(\"Number of Time Steps:\", num_time_steps)\n",
    "print(\"Training Data:\", training_data)\n",
    "print(\"Validation Data:\", validation_data)\n",
    "print(\"Test Data Counterfactuals:\", test_data_counterfactuals)\n",
    "print(\"Test Data Factuals:\", test_data_factuals)\n",
    "print(\"Test Data Sequence:\", test_data_seq)\n",
    "print(\"Scaling Data:\", scaling_data)\n",
    "print(\"Means:\", means)\n",
    "print(\"Stds:\", stds)\n",
    "print(\"Window Size:\", window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #remove patient_types from means\n",
    "# means = means[:-1]\n",
    "# stds = stds[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the data\n",
    "for key in training_data.keys():\n",
    "    if key in means.keys():\n",
    "        training_data[key] = (training_data[key] - means[key]) / stds[key]\n",
    "        validation_data[key] = (validation_data[key] - means[key]) / stds[key]\n",
    "        test_data_factuals[key] = (test_data_factuals[key] - means[key]) / stds[key]\n",
    "\n",
    "for key in test_data_counterfactuals.keys():\n",
    "    if key in means.keys():\n",
    "        test_data_counterfactuals[key] = (\n",
    "            test_data_counterfactuals[key] - means[key]\n",
    "        ) / stds[key]\n",
    "        test_data_seq[key] = (test_data_seq[key] - means[key]) / stds[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_treatments(data: dict, patient: int):\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"N(t)\": data[\"cancer_volume\"][patient],\n",
    "            \"C(t)\": data[\"chemo_dosage\"][patient],\n",
    "            \"d(t)\": data[\"radio_dosage\"][patient],\n",
    "        }\n",
    "    )\n",
    "    df = df[[\"N(t)\", \"C(t)\", \"d(t)\"]]\n",
    "    df.plot(secondary_y=[\"C(t)\", \"d(t)\"])\n",
    "    plt.xlabel(\"$t$\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_treatments(test_data_factuals, 130)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_counterfactuals.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_treatments(data: dict, patient: int):\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"N(t)\": data[\"cancer_volume\"][patient],\n",
    "            \"C(t)\": data[\"chemo_application\"][patient],\n",
    "            \"d(t)\": data[\"radio_application\"][patient],\n",
    "        }\n",
    "    )\n",
    "    df = df[[\"N(t)\", \"C(t)\", \"d(t)\"]]\n",
    "    df.plot(secondary_y=[\"C(t)\", \"d(t)\"])\n",
    "    plt.xlabel(\"$t$\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_treatments(test_data_counterfactuals, 34000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a column that indicates the application of chemo and radio at time t-1\n",
    "\n",
    "\n",
    "def add_previous_treatment(data: dict):\n",
    "    data[\"chemo_application_prev\"] = np.roll(data[\"chemo_application\"], 1, axis=1)\n",
    "    data[\"radio_application_prev\"] = np.roll(data[\"radio_application\"], 1, axis=1)\n",
    "    data[\"chemo_application_prev\"][:, 0] = 0\n",
    "    data[\"radio_application_prev\"][:, 0] = 0\n",
    "    return data\n",
    "\n",
    "\n",
    "training_data = add_previous_treatment(training_data)\n",
    "validation_data = add_previous_treatment(validation_data)\n",
    "test_data_factuals = add_previous_treatment(test_data_factuals)\n",
    "test_data_counterfactuals = add_previous_treatment(test_data_counterfactuals)\n",
    "test_data_seq = add_previous_treatment(test_data_seq)\n",
    "\n",
    "\n",
    "def tile_patient_types(data: dict):\n",
    "    data[\"patient_type_tile\"] = np.tile(\n",
    "        data[\"patient_types\"][:, None], (1, data[\"cancer_volume\"].shape[1])\n",
    "    )\n",
    "    return data\n",
    "\n",
    "\n",
    "training_data = tile_patient_types(training_data)\n",
    "validation_data = tile_patient_types(validation_data)\n",
    "test_data_factuals = tile_patient_types(test_data_factuals)\n",
    "test_data_counterfactuals = tile_patient_types(test_data_counterfactuals)\n",
    "test_data_seq = tile_patient_types(test_data_seq)\n",
    "\n",
    "\n",
    "def add_time_steps(\n",
    "    data: dict,\n",
    "):  # this should be the same shape as the cancer_volume (patients, time_steps)\n",
    "    data[\"time_step\"] = np.tile(\n",
    "        np.arange(data[\"cancer_volume\"].shape[1]), (data[\"cancer_volume\"].shape[0], 1)\n",
    "    )\n",
    "    return data\n",
    "\n",
    "\n",
    "# sequence lengths is reported once per patient, so we tile it to match the shape of the data\n",
    "def add_sequence_lengths(data: dict):\n",
    "    data[\"sequence_length_tile\"] = np.tile(\n",
    "        data[\"sequence_lengths\"][:, None], (1, data[\"cancer_volume\"].shape[1])\n",
    "    )\n",
    "    data[\"distance_from_end\"] = data[\"sequence_length_tile\"] - data[\"time_step\"]\n",
    "    # distance_from_end_scaled is a value between 0 and 1 that indicates how far we are from the end of the sequence for each patient\n",
    "    data[\"distance_from_end_scaled\"] = (\n",
    "        data[\"distance_from_end\"] / data[\"sequence_length_tile\"]\n",
    "    )\n",
    "    return data\n",
    "\n",
    "\n",
    "training_data = add_time_steps(training_data)\n",
    "validation_data = add_time_steps(validation_data)\n",
    "test_data_factuals = add_time_steps(test_data_factuals)\n",
    "test_data_counterfactuals = add_time_steps(test_data_counterfactuals)\n",
    "test_data_seq = add_time_steps(test_data_seq)\n",
    "\n",
    "training_data = add_sequence_lengths(training_data)\n",
    "validation_data = add_sequence_lengths(validation_data)\n",
    "test_data_factuals = add_sequence_lengths(test_data_factuals)\n",
    "test_data_counterfactuals = add_sequence_lengths(test_data_counterfactuals)\n",
    "test_data_seq = add_sequence_lengths(test_data_seq)\n",
    "\n",
    "\n",
    "# if distance from end is less than 0, we set all the other columns to nan\n",
    "def mask_data(data: dict):\n",
    "    for key in data.keys():\n",
    "        if data[key].shape == data[\"distance_from_end\"].shape:\n",
    "            data[key] = np.where(data[\"distance_from_end\"] < 0, np.nan, data[key])\n",
    "    return data\n",
    "\n",
    "\n",
    "training_data = mask_data(training_data)\n",
    "validation_data = mask_data(validation_data)\n",
    "test_data_factuals = mask_data(test_data_factuals)\n",
    "test_data_counterfactuals = mask_data(test_data_counterfactuals)\n",
    "test_data_seq = mask_data(test_data_seq)\n",
    "\n",
    "training_data = add_sequence_lengths(training_data)\n",
    "validation_data = add_sequence_lengths(validation_data)\n",
    "test_data_factuals = add_sequence_lengths(test_data_factuals)\n",
    "test_data_counterfactuals = add_sequence_lengths(test_data_counterfactuals)\n",
    "test_data_seq = add_sequence_lengths(test_data_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_data['time_step'].shape\n",
    "# training_data['sequence_length_tile'].shape\n",
    "training_data[\"chemo_application_prev\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_sequence_lengths = training_data[\"sequence_lengths\"]\n",
    "validation_data_sequence_lengths = validation_data[\"sequence_lengths\"]\n",
    "test_data_factuals_sequence_lengths = test_data_factuals[\"sequence_lengths\"]\n",
    "test_data_counterfactuals_sequence_lengths = test_data_counterfactuals[\n",
    "    \"sequence_lengths\"\n",
    "]\n",
    "test_data_seq_sequence_lengths = test_data_seq[\"sequence_lengths\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count of the unique values of the sequence lengths\n",
    "np.unique(training_data_sequence_lengths, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(test_data_counterfactuals_sequence_lengths, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "needed_keys = [\n",
    "    \"chemo_application_prev\",\n",
    "    \"radio_application_prev\",\n",
    "    \"patient_type_tile\",\n",
    "    \"cancer_volume\",\n",
    "]\n",
    "#\n",
    "training_data = {key: training_data[key] for key in needed_keys}\n",
    "validation_data = {key: validation_data[key] for key in needed_keys}\n",
    "test_data_factuals = {key: test_data_factuals[key] for key in needed_keys}\n",
    "test_data_counterfactuals = {key: test_data_counterfactuals[key] for key in needed_keys}\n",
    "test_data_seq = {key: test_data_seq[key] for key in needed_keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert tensors where the first dimension is the number of patients the second dimension is time and the third is the features\n",
    "# the keys are the features\n",
    "# within the features, the first dimension is the number of patients, the second is time\n",
    "\n",
    "\n",
    "# convert the keys to a dimension in the tensor\n",
    "def dictionary_to_tensor(data_dict):\n",
    "    # Extract keys and values from the dictionary\n",
    "    keys = list(data_dict.keys())\n",
    "    values = [data_dict[key] for key in keys]\n",
    "\n",
    "    # Check consistency in dimensions\n",
    "    num_patients = values[0].shape[0]\n",
    "    time_steps = values[0].shape[1]\n",
    "\n",
    "    for table in values:\n",
    "        assert table.shape[0] == num_patients, \"Number of patients mismatch.\"\n",
    "        assert table.shape[1] == time_steps, \"Number of time steps mismatch.\"\n",
    "\n",
    "    # Stack tables along the new feature dimension\n",
    "    tensor = np.stack(values, axis=-1)\n",
    "    tensor = torch.tensor(tensor, dtype=torch.float32)\n",
    "\n",
    "    return tensor, keys\n",
    "\n",
    "\n",
    "training_data_tensor, keys = dictionary_to_tensor(training_data)\n",
    "validation_data_tensor, keys = dictionary_to_tensor(validation_data)\n",
    "test_data_factuals_tensor, keys = dictionary_to_tensor(test_data_factuals)\n",
    "test_data_counterfactuals_tensor, keys = dictionary_to_tensor(test_data_counterfactuals)\n",
    "test_data_seq_tensor, keys = dictionary_to_tensor(test_data_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_tensor.shape\n",
    "test_data_counterfactuals_tensor.shape\n",
    "test_data_seq_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_counterfactuals_sequence_lengths.shape\n",
    "test_data_seq_sequence_lengths.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data embedding before feeding into the model\n",
    "\n",
    "This will first one-hot encode all the categorical features and then embed them to n columns. The resulting columns will then be concatenated with the numerical features. The result will then be used to create the torch tensor for the model. The torch tensor will be shaped as (Cases, Time, Features).\n",
    "\n",
    "The input data will be a dataframe like this:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataEmbedder(nn.Module):\n",
    "    def __init__(self, categorical_indices_sizes, numerical_indices, dataset):\n",
    "        super(DataEmbedder, self).__init__()\n",
    "        # dictionary with feature name, and a list of index and size\n",
    "        self.categoricals = categorical_indices_sizes\n",
    "        self.numerics = numerical_indices  # dictionary with feature name and index\n",
    "        self.embeddings = nn.ModuleDict()\n",
    "        self.mapping_dicts = {}\n",
    "\n",
    "        # Initialize embeddings and mapping dictionaries\n",
    "        for key in self.categoricals:\n",
    "            unique_values = np.unique(dataset[:, :, self.categoricals[key][0]])\n",
    "            self.mapping_dicts[key] = {\n",
    "                name: idx for idx, name in enumerate(unique_values)\n",
    "            }\n",
    "            self.embeddings[key] = nn.Embedding(\n",
    "                num_embeddings=len(unique_values),\n",
    "                embedding_dim=self.categoricals[key][1],\n",
    "            )\n",
    "            print(\n",
    "                f\"Feature: {key}, Categories: {len(unique_values)}, Embedding Size: {self.categoricals[key][1]}\"\n",
    "            )\n",
    "\n",
    "    def forward(self, dataset):\n",
    "        # Apply embeddings to the categorical indices\n",
    "        if len(self.categoricals) == 0:\n",
    "            return dataset\n",
    "        else:\n",
    "            embedded_features = []\n",
    "            for key in self.categoricals:\n",
    "                # Map the categorical values to their corresponding indices\n",
    "                indices = dataset[:, :, self.categoricals[key][0]].cpu().numpy()\n",
    "                mapped_indices = np.vectorize(self.mapping_dicts[key].get)(indices)\n",
    "                mapped_indices = torch.tensor(\n",
    "                    mapped_indices, dtype=torch.long, device=dataset.device\n",
    "                )\n",
    "                # print(f\"Feature: {key}, Mapped Indices: {mapped_indices}\")\n",
    "                embedded_features.append(self.embeddings[key](mapped_indices))\n",
    "\n",
    "            embedded_features = torch.cat(embedded_features, dim=-1)\n",
    "\n",
    "            numeric_features = dataset[:, :, list(self.numerics.values())].float()\n",
    "\n",
    "            # Concatenate the embedded features with the numerical data\n",
    "            result = torch.cat([embedded_features, numeric_features], dim=-1)\n",
    "\n",
    "            feature_count_embedded = len(self.numerics) + sum(\n",
    "                [self.categoricals[key][1] for key in self.categoricals]\n",
    "            )\n",
    "\n",
    "            result = result.reshape(dataset.shape[0], -1, feature_count_embedded)\n",
    "\n",
    "            return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moded Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class moded_TimesSeriesAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    A module that computes multi-head attention given query, key, and value tensors for time series data of shape (b, t, f, e)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, num_heads: int):\n",
    "        \"\"\"\n",
    "        Constructor.\n",
    "\n",
    "        Inputs:\n",
    "        - input_dim: Dimension of the input query, key, and value. We assume they all have\n",
    "          the same dimensions. This is basically the dimension of the embedding.\n",
    "        - num_heads: Number of attention heads\n",
    "        \"\"\"\n",
    "        super(moded_TimesSeriesAttention, self).__init__()\n",
    "\n",
    "        assert embed_dim % num_heads == 0\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dim_per_head = embed_dim // num_heads\n",
    "\n",
    "        self.linear_query = nn.Linear(embed_dim, embed_dim)\n",
    "        self.linear_key = nn.Linear(embed_dim, embed_dim)\n",
    "        self.linear_value = nn.Linear(\n",
    "            embed_dim, embed_dim\n",
    "        )  # (self.num_heads * self.dim_per_head * self.dim_per_head))\n",
    "        self.output_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        self.softmax = nn.Softmax2d()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        query: torch.Tensor,\n",
    "        key: torch.Tensor,\n",
    "        value: torch.Tensor,\n",
    "        # mask: torch.Tensor = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Compute the attended feature representations.\n",
    "\n",
    "        Inputs:\n",
    "        - query: Tensor of the shape BxTxFXE, where B is the batch size, T is the time dimension, F is the feature dimension,\n",
    "        and E is the embedding dimension\n",
    "        - key: Tensor of the shape BxTxFXE\n",
    "        - value: Tensor of the shape BxTxFXE\n",
    "        # - mask: Tensor indicating where the attention should *not* be performed (to remove)\n",
    "        \"\"\"\n",
    "        b = query.shape[0]\n",
    "        t = query.shape[1]\n",
    "        f = query.shape[2]\n",
    "        e = query.shape[3]\n",
    "        d = self.dim_per_head\n",
    "        h = self.num_heads\n",
    "\n",
    "        query_linear = self.linear_query(query)\n",
    "        key_linear = self.linear_key(key)\n",
    "        value_linear = self.linear_value(value)\n",
    "\n",
    "        query_reshaped = query_linear.reshape(b, t, f, h, d)\n",
    "        key_reshaped = key_linear.reshape(b, t, f, h, d)\n",
    "        value_reshaped = value_linear.reshape(b, t, f, h, d)\n",
    "\n",
    "        query_reshaped = query_reshaped.permute(0, 3, 1, 2, 4)  # BxHxTxFxD\n",
    "        key_reshaped = key_reshaped.permute(0, 3, 1, 2, 4)  # BxHxTxFxD\n",
    "        value_reshaped = value_reshaped.permute(0, 3, 1, 2, 4)  # BxHxTxFxD\n",
    "\n",
    "        kq = torch.einsum(\"bhtfd,bhxyd->bhtfxy\", key_reshaped, query_reshaped)\n",
    "\n",
    "        dot_prod_scores = kq / math.sqrt(self.dim_per_head)\n",
    "\n",
    "        # softmax across last 2 features (use softmax2d) (review later)\n",
    "        dot_prod_scores = dot_prod_scores.reshape(b * h, t * f, t, f)\n",
    "        dot_prod_scores = self.softmax(dot_prod_scores)\n",
    "        dot_prod_scores = dot_prod_scores.reshape(b, h, t, f, t, f)\n",
    "\n",
    "        out = torch.einsum(\"bhtfxy,bhtfd->bhtfd\", dot_prod_scores, value_reshaped)\n",
    "        out = out.permute(0, 2, 3, 1, 4).reshape(b, t, f, e)\n",
    "        out = self.output_linear(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class moded_TransformerEncoderCell(nn.Module):\n",
    "    \"\"\"\n",
    "    A single cell (unit) for the Transformer encoder.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, num_heads: int, ff_dim: int, dropout: float):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - embed_dim: embedding dimension for each element in the time series data\n",
    "        - num_heads: Number of attention heads in a multi-head attention module\n",
    "        - ff_dim: The hidden dimension for a feedforward network\n",
    "        - dropout: Dropout ratio for the output of the multi-head attention and feedforward\n",
    "          modules.\n",
    "        \"\"\"\n",
    "        super(moded_TransformerEncoderCell, self).__init__()\n",
    "\n",
    "        self.time_series_attention = moded_TimesSeriesAttention(embed_dim, num_heads)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(embed_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.linear1 = nn.Linear(embed_dim, ff_dim)\n",
    "        self.linear2 = nn.Linear(ff_dim, embed_dim)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        # mask: torch.Tensor = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - x: Tensor of the shape BxTxFXE, where B is the batch size, T is the time dimension, F is the feature dimension,\n",
    "        and E is the embedding dimension\n",
    "        # - mask: Tensor for multi-head attention\n",
    "        \"\"\"\n",
    "\n",
    "        attention2 = self.time_series_attention(x, x, x)\n",
    "        attention = x + self.dropout1(attention2)\n",
    "        attention = self.layer_norm(attention)\n",
    "\n",
    "        attention2 = self.linear2(\n",
    "            self.dropout(self.activation(self.linear1(attention)))\n",
    "        )\n",
    "        attention = attention + self.dropout2(attention2)\n",
    "        attention = self.layer_norm(attention)\n",
    "\n",
    "        return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class moded_TransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A full encoder consisting of a set of TransformerEncoderCell.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int,\n",
    "        num_heads: int,\n",
    "        ff_dim: int,\n",
    "        num_cells: int,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - embed_dim: embedding dimension for each element in the time series data\n",
    "        - num_heads: Number of attention heads in a multi-head attention module\n",
    "        - ff_dim: The hidden dimension for a feedforward network\n",
    "        - num_cells: Number of time series attention cells in the encoder\n",
    "        - dropout: Dropout ratio for the output of the multi-head attention and feedforward\n",
    "          modules.\n",
    "        \"\"\"\n",
    "        super(moded_TransformerEncoder, self).__init__()\n",
    "\n",
    "        self.norm = None\n",
    "\n",
    "        self.encoder_modules = nn.ModuleList(\n",
    "            moded_TransformerEncoderCell(embed_dim, num_heads, ff_dim, dropout)\n",
    "            for _ in range(num_cells)\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        # mask: torch.Tensor = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - x: Tensor of the shape BxTxFXE, where B is the batch size, T is the time dimension, F is the feature dimension,\n",
    "        and E is the embedding dimension\n",
    "        # - mask: Tensor for multi-head attention (to remove)\n",
    "\n",
    "        Return:\n",
    "        - y: Tensor of the shape BxTxFXE\n",
    "        \"\"\"\n",
    "\n",
    "        # run encoder modules and add residual connections\n",
    "        for encoder_module in self.encoder_modules:\n",
    "            x = encoder_module(x)\n",
    "\n",
    "        y = x\n",
    "\n",
    "        return y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimesSeriesAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_in,\n",
    "        d_out,\n",
    "        nh=8,\n",
    "        dk=0,\n",
    "        dv=0,\n",
    "        dd=0,\n",
    "        kernel_size=(3, 7),\n",
    "        stride=(1, 1, 1),\n",
    "        kernel_type=\"VplusR\",  # ['V', 'R', 'VplusR']\n",
    "        feat_type=\"VplusR\",  # ['V', 'R', 'VplusR']\n",
    "    ):\n",
    "        super(TimesSeriesAttention, self).__init__()\n",
    "\n",
    "        self.d_in = d_in\n",
    "        self.d_out = d_out\n",
    "        self.nh = nh\n",
    "        self.dv = dv = d_out // nh if dv == 0 else dv\n",
    "        self.dk = dk = dv if dk == 0 else dk\n",
    "        self.dd = dd = dk if dd == 0 else dd\n",
    "\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.kernel_type = kernel_type\n",
    "        self.feat_type = feat_type\n",
    "\n",
    "        assert self.kernel_type in [\n",
    "            \"V\",\n",
    "            \"R\",\n",
    "            \"VplusR\",\n",
    "        ], \"Not implemented involution type: {}\".format(self.kernel_type)\n",
    "        assert self.feat_type in [\n",
    "            \"V\",\n",
    "            \"R\",\n",
    "            \"VplusR\",\n",
    "        ], \"Not implemented feature type: {}\".format(self.feat_type)\n",
    "\n",
    "        # print(\"d_in: {}, d_out: {}, nh: {}, dk: {}, dv: {}, dd:{}, kernel_size: {}, kernel_type: {}, feat_type: {}\"\n",
    "        #       .format(d_in, d_out, nh, dk, dv, self.dd, kernel_size, kernel_type, feat_type))\n",
    "\n",
    "        self.ksize = ksize = kernel_size[0] * kernel_size[1]\n",
    "        self.pad = pad = tuple(k // 2 for k in kernel_size)\n",
    "\n",
    "        # hidden dimension\n",
    "        d_hid = nh * dk + dv if self.kernel_type == \"V\" else nh * dk + dk + dv\n",
    "\n",
    "        # Linear projection\n",
    "        # self.projection = nn.Conv2d(d_in, d_hid, 1, bias=False)\n",
    "        self.projection_linear = nn.Sequential(\n",
    "            nn.Linear(d_in, d_hid, bias=False),\n",
    "            nn.SiLU(inplace=True),\n",
    "            nn.Linear(d_hid, d_hid, bias=False),\n",
    "        )\n",
    "\n",
    "        # Intervolution Kernel\n",
    "        if self.kernel_type == \"V\":\n",
    "            self.H2 = nn.Conv2d(1, dd, kernel_size, padding=self.pad, bias=False)\n",
    "        elif self.kernel_type == \"R\":\n",
    "            self.H1 = nn.Conv2d(\n",
    "                dk, dk * dd, kernel_size, padding=self.pad, groups=dk, bias=False\n",
    "            )\n",
    "            self.H2 = nn.Conv2d(1, dd, kernel_size, padding=self.pad, bias=False)\n",
    "        elif self.kernel_type == \"VplusR\":\n",
    "            self.P1 = nn.Parameter(\n",
    "                torch.randn(dk, dd).unsqueeze(0) * np.sqrt(1 / (ksize * dd)),\n",
    "                requires_grad=True,\n",
    "            )\n",
    "            self.H1 = nn.Conv2d(\n",
    "                dk, dk * dd, kernel_size, padding=self.pad, groups=dk, bias=False\n",
    "            )\n",
    "            self.H2 = nn.Conv2d(1, dd, kernel_size, padding=self.pad, bias=False)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        # Feature embedding layer\n",
    "        if self.feat_type == \"V\":\n",
    "            pass\n",
    "        elif self.feat_type == \"R\":\n",
    "            self.G = nn.Conv2d(1, dv, kernel_size, padding=self.pad, bias=False)\n",
    "        elif self.feat_type == \"VplusR\":\n",
    "            self.G = nn.Conv2d(1, dv, kernel_size, padding=self.pad, bias=False)\n",
    "            self.I = nn.Parameter(torch.eye(dk).unsqueeze(0), requires_grad=True)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        # Downsampling layer\n",
    "        if max(self.stride) > 1:\n",
    "            self.avgpool = nn.AvgPool2d(\n",
    "                kernel_size=(1, 3), stride=(1, 2), padding=(0, 1)\n",
    "            )\n",
    "\n",
    "    def L2norm(self, x, d=1):\n",
    "        eps = 1e-6\n",
    "        norm = x**2\n",
    "        norm = norm.sum(dim=d, keepdim=True) + eps\n",
    "        norm = norm ** (0.5)\n",
    "        return x / norm\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # print(x.shape)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        N, C, T, H = x.shape\n",
    "\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "\n",
    "        \"\"\"Linear projection\"\"\"\n",
    "        # x_proj = self.projection(x)\n",
    "        x_proj = self.projection_linear(x)\n",
    "        x_proj = x_proj.permute(0, 3, 1, 2)\n",
    "        # print(x_proj.shape)\n",
    "\n",
    "        if self.kernel_type != \"V\":\n",
    "            q, k, v = torch.split(x_proj, [self.nh * self.dk, self.dk, self.dv], dim=1)\n",
    "        else:\n",
    "            q, v = torch.split(x_proj, [self.nh * self.dk, self.dv], dim=1)\n",
    "\n",
    "        \"\"\"Normalization\"\"\"\n",
    "        q = rearrange(q, \"b (nh k) t h -> b nh k t h\", k=self.dk)\n",
    "        q = self.L2norm(q, d=2)\n",
    "        q = rearrange(q, \"b nh k t h -> (b t h) nh k\")\n",
    "\n",
    "        v = self.L2norm(v, d=1)\n",
    "\n",
    "        if self.kernel_type != \"V\":\n",
    "            k = self.L2norm(k, d=1)\n",
    "\n",
    "        \"\"\"\n",
    "        q = (b t h) nh k\n",
    "        k = b k t h\n",
    "        v = b v t h\n",
    "        \"\"\"\n",
    "\n",
    "        # Intervolution generation\n",
    "        # Basic kernel\n",
    "        if self.kernel_type == \"V\":\n",
    "            kernel = q\n",
    "        # Relational kernel\n",
    "        else:\n",
    "            K_H1 = self.H1(k)\n",
    "            K_H1 = rearrange(K_H1, \"b (k d) t h-> (b t h) k d\", k=self.dk)\n",
    "\n",
    "            if self.kernel_type == \"VplusR\":\n",
    "                K_H1 = K_H1 + self.P1\n",
    "\n",
    "            kernel = torch.einsum(\n",
    "                \"abc,abd->acd\", q.transpose(1, 2), K_H1\n",
    "            )  # (bth, nh, d)\n",
    "\n",
    "        # feature generation\n",
    "        # Appearance feature\n",
    "        v = rearrange(v, \"b (v 1) t h-> (b v) 1 t h\")\n",
    "\n",
    "        V = self.H2(v)  # (bv, d, t, h)\n",
    "        feature = rearrange(V, \"(b v) d t h -> (b t h) v d\", v=self.dv)\n",
    "\n",
    "        # Relational feature\n",
    "        if self.feat_type in [\"R\", \"VplusR\"]:\n",
    "            V_G = self.G(v)  # (bv, v2, t, h)\n",
    "            V_G = rearrange(V_G, \"(b v) v2 t h -> (b t h) v v2\", v=self.dv)\n",
    "\n",
    "            if self.feat_type == \"VplusR\":\n",
    "                V_G = V_G + self.I\n",
    "\n",
    "            feature = torch.einsum(\"abc,abd->acd\", V_G, feature)  # (bth, v2, d)\n",
    "\n",
    "        # kernel * feat\n",
    "        out = torch.einsum(\"abc,adc->adb\", kernel, feature)  # (bth, nh, v2)\n",
    "\n",
    "        out = rearrange(out, \"(b t h) nh v -> b (nh v) t h\", t=T, h=H)\n",
    "\n",
    "        if max(self.stride) > 1:\n",
    "            out = self.avgpool(out)\n",
    "\n",
    "        out = out.permute(0, 2, 3, 1)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderCell(nn.Module):\n",
    "    \"\"\"\n",
    "    A single cell (unit) for the Transformer encoder.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, embed_dim: int, num_heads: int, kernel_size, ff_dim: int, dropout: float\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - embed_dim: embedding dimension for each element in the time series data\n",
    "        - num_heads: Number of attention heads in a multi-head attention module\n",
    "        - ff_dim: The hidden dimension for a feedforward network\n",
    "        - dropout: Dropout ratio for the output of the multi-head attention and feedforward\n",
    "          modules.\n",
    "        \"\"\"\n",
    "        super(TransformerEncoderCell, self).__init__()\n",
    "\n",
    "        self.time_series_attention = TimesSeriesAttention(\n",
    "            embed_dim, embed_dim, nh=num_heads, kernel_size=kernel_size\n",
    "        )\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(embed_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.linear1 = nn.Linear(embed_dim, ff_dim)\n",
    "        self.linear2 = nn.Linear(ff_dim, embed_dim)\n",
    "\n",
    "    def forward(self, data: torch.Tensor, embeddings, mask: torch.Tensor = None):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - x: Tensor of the shape BxTxFXE, where B is the batch size, T is the time dimension, F is the feature dimension,\n",
    "        and E is the embedding dimension\n",
    "        - mask: Tensor for multi-head attention\n",
    "        \"\"\"\n",
    "\n",
    "        # attention2 = self.time_series_attention(x, x, x, mask)\n",
    "        attention2 = self.time_series_attention(data)\n",
    "        attention = data + self.dropout1(attention2)\n",
    "        attention = self.layer_norm(attention)\n",
    "\n",
    "        attention2 = self.linear2(\n",
    "            self.dropout(self.activation(self.linear1(attention)))\n",
    "        )\n",
    "        attention = attention + self.dropout2(attention2)\n",
    "        attention = self.layer_norm(attention)\n",
    "\n",
    "        return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A full encoder consisting of a set of TransformerEncoderCell.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int,\n",
    "        num_heads: int,\n",
    "        kernel_size,\n",
    "        ff_dim: int,\n",
    "        num_cells: int,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - embed_dim: embedding dimension for each element in the time series data\n",
    "        - num_heads: Number of attention heads in a multi-head attention module\n",
    "        - ff_dim: The hidden dimension for a feedforward network\n",
    "        - num_cells: Number of time series attention cells in the encoder\n",
    "        - dropout: Dropout ratio for the output of the multi-head attention and feedforward\n",
    "          modules.\n",
    "        \"\"\"\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "\n",
    "        self.norm = None\n",
    "\n",
    "        self.encoder_modules = nn.ModuleList(\n",
    "            TransformerEncoderCell(embed_dim, num_heads, kernel_size, ff_dim, dropout)\n",
    "            for _ in range(num_cells)\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor = None):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - x: Tensor of the shape BxTxFXE, where B is the batch size, T is the time dimension, F is the feature dimension,\n",
    "        and E is the embedding dimension\n",
    "        - mask: Tensor for multi-head attention\n",
    "\n",
    "        Return:\n",
    "        - y: Tensor of the shape BxTxFXE\n",
    "        \"\"\"\n",
    "\n",
    "        # run encoder modules and add residual connections\n",
    "        for encoder_module in self.encoder_modules:\n",
    "            x = encoder_module(x, mask)\n",
    "\n",
    "        y = x\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSDI transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_torch_trans(num_heads=8, num_cells=1, embed_dim=128, ff_dim=512, dropout=0.1):\n",
    "    encoder_layer = nn.TransformerEncoderLayer(\n",
    "        d_model=embed_dim,\n",
    "        nhead=num_heads,\n",
    "        dim_feedforward=ff_dim,\n",
    "        activation=\"gelu\",\n",
    "        dropout=dropout,\n",
    "    )\n",
    "    return nn.TransformerEncoder(encoder_layer, num_layers=num_cells)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DiffusionEmbedding(nn.Module):\n",
    "#     def __init__(self, num_steps, embedding_dim, projection_dim=None):\n",
    "#         super(DiffusionEmbedding, self).__init__()\n",
    "#         if projection_dim is None:\n",
    "#             projection_dim = embedding_dim\n",
    "#         self.register_buffer(\n",
    "#             \"embedding\",\n",
    "#             self._build_embedding(num_steps, embedding_dim / 2),\n",
    "#             persistent=False,\n",
    "#         )\n",
    "#         self.projection1 = nn.Linear(embedding_dim, projection_dim)\n",
    "#         self.projection2 = nn.Linear(projection_dim, embedding_dim)\n",
    "\n",
    "#     def forward(self, diffusion_step, data, device=\"cpu\"):\n",
    "#         x = self.embedding[diffusion_step]\n",
    "#         x = self.projection1(x)\n",
    "#         x = F.silu(x)\n",
    "#         x = self.projection2(x)\n",
    "#         x = F.silu(x)\n",
    "#         x = torch.zeros(data.shape).to(device) + x.unsqueeze(1).unsqueeze(1)\n",
    "#         return x\n",
    "\n",
    "#     def _build_embedding(self, num_steps, dim=64):\n",
    "#         steps = torch.arange(num_steps).unsqueeze(1)  # (T,1)\n",
    "#         frequencies = 10.0 ** (torch.arange(dim) / (dim - 1) * 4.0).unsqueeze(\n",
    "#             0\n",
    "#         )  # (1,dim)\n",
    "#         table = steps * frequencies  # (T,dim)\n",
    "#         table = torch.cat([torch.sin(table), torch.cos(table)], dim=1)  # (T,dim*2)\n",
    "#         return table\n",
    "\n",
    "\n",
    "# class TimeEmbedding(nn.Module):\n",
    "#     def __init__(self, embedding_dim, max_len=10000.0):\n",
    "#         super(TimeEmbedding, self).__init__()\n",
    "#         self.max_len = max_len\n",
    "#         self.learnable = nn.Sequential(\n",
    "#             nn.Linear(embedding_dim, embedding_dim),\n",
    "#             nn.SiLU(),\n",
    "#             nn.Linear(embedding_dim, embedding_dim),\n",
    "#         )\n",
    "\n",
    "#     def forward(self, data, device=\"cpu\"):\n",
    "\n",
    "#         b, l, f, e = data.shape\n",
    "#         pe = None\n",
    "#         pe_row = torch.arange(l)\n",
    "\n",
    "#         pe = pe_row.unsqueeze(0)\n",
    "#         pe = pe.unsqueeze(2)\n",
    "\n",
    "#         pe = pe.repeat((b, 1, e))\n",
    "#         pe = pe.float()\n",
    "\n",
    "#         pe[:, :, 0::2] = torch.sin(\n",
    "#             pe[:, :, 0::2] / (self.max_len ** (torch.arange(0, e, 2) / e))\n",
    "#         )\n",
    "#         pe[:, :, 1::2] = torch.cos(\n",
    "#             pe[:, :, 1::2] / (self.max_len ** (torch.arange(0, e, 2) / e))\n",
    "#         )\n",
    "\n",
    "#         pe = pe.to(device).unsqueeze(2).repeat((1, 1, f, 1))\n",
    "\n",
    "#         # pe = torch.arange(l).unsqueeze(0).unsqueeze(-1).unsqueeze(-1).to(device)\n",
    "#         # pe = torch.zeros(data.shape).to(device) + pe\n",
    "\n",
    "#         # div_term = 1 / torch.pow(\n",
    "#         #     self.max_len, torch.arange(0, f, 2) / f\n",
    "#         # ).unsqueeze(-1).to(device)\n",
    "\n",
    "#         # pe[:, :, 0::2] = torch.sin(pe[:, :, 0::2] * div_term)\n",
    "#         # pe[:, :, 1::2] = torch.cos(pe[:, :, 1::2] * div_term)\n",
    "\n",
    "#         return self.learnable(pe)\n",
    "\n",
    "\n",
    "# class FeatureEmbedding(nn.Module):\n",
    "#     def __init__(self, embedding_dim, max_len=10000.0):\n",
    "#         super(FeatureEmbedding, self).__init__()\n",
    "#         self.max_len = max_len\n",
    "#         self.learnable = nn.Sequential(\n",
    "#             nn.Linear(embedding_dim, embedding_dim),\n",
    "#             nn.SiLU(),\n",
    "#             nn.Linear(embedding_dim, embedding_dim),\n",
    "#         )\n",
    "\n",
    "#     def forward(self, data, device=\"cpu\"):\n",
    "#         b, l, f, e = data.shape\n",
    "\n",
    "#         pe = None\n",
    "#         pe_row = torch.arange(f)\n",
    "\n",
    "#         pe = pe_row.unsqueeze(0)\n",
    "#         pe = pe.unsqueeze(2)\n",
    "\n",
    "#         pe = pe.repeat((b, 1, e))\n",
    "#         pe = pe.float()\n",
    "\n",
    "#         pe[:, :, 0::2] = torch.sin(\n",
    "#             pe[:, :, 0::2] / (self.max_len ** (torch.arange(0, e, 2) / e))\n",
    "#         )\n",
    "#         pe[:, :, 1::2] = torch.cos(\n",
    "#             pe[:, :, 1::2] / (self.max_len ** (torch.arange(0, e, 2) / e))\n",
    "#         )\n",
    "\n",
    "#         pe = pe.to(device).unsqueeze(1).repeat((1, l, 1, 1))\n",
    "\n",
    "#         # pe = torch.arange(f).unsqueeze(0).unsqueeze(0).unsqueeze(-1).to(device)\n",
    "#         # pe = torch.zeros(data.shape).to(device) + pe\n",
    "\n",
    "#         # div_term = 1 / torch.pow(\n",
    "#         #     self.max_len, torch.arange(0, e, 2) / e\n",
    "#         # ).to(device)\n",
    "\n",
    "#         # pe[:, :, :, 0::2] = torch.sin(pe[:, :, :, 0::2] * div_term)\n",
    "#         # pe[:, :, :, 1::2] = torch.cos(pe[:, :, :, 1::2] * div_term)\n",
    "\n",
    "#         return self.learnable(pe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionEmbedding(nn.Module):\n",
    "    def __init__(self, num_steps, embedding_dim):\n",
    "        super(DiffusionEmbedding, self).__init__()\n",
    "\n",
    "        # Learnable embeddings for each diffusion step\n",
    "        self.embedding = nn.Embedding(num_steps, embedding_dim)\n",
    "\n",
    "        # Optional: You could still add a simple processing layer if desired\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, embedding_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(embedding_dim, embedding_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, diffusion_step, data, device=\"cpu\"):\n",
    "        b, t, f, e = data.shape\n",
    "\n",
    "        diffusion_step = diffusion_step.to(device)\n",
    "        # Retrieve the embedding for the given diffusion step\n",
    "        x = self.embedding(diffusion_step).to(device)  # Shape: (embedding_dim)\n",
    "\n",
    "        # Process the embedding if a projection is used\n",
    "        x = self.projection(x)\n",
    "\n",
    "        # Expand the embedding to match the data shape\n",
    "        x = x.view(b, 1, 1, -1).expand(b, t, f, -1)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class TimeEmbedding(nn.Module):\n",
    "    def __init__(self, max_steps, embedding_dim):\n",
    "        super(TimeEmbedding, self).__init__()\n",
    "\n",
    "        # Learnable embeddings for the time steps\n",
    "        self.embedding = nn.Embedding(max_steps, embedding_dim)\n",
    "\n",
    "        # Optional projection layer to process the embeddings further\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, embedding_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(embedding_dim, embedding_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, start_steps, data, device=\"cpu\"):\n",
    "        b, t, f, e = data.shape\n",
    "\n",
    "        # if start steps is just an integer value, we need to expand it to match the batch size\n",
    "        if isinstance(start_steps, int):\n",
    "            start_steps = torch.tensor([start_steps] * b, device=device)\n",
    "        else:\n",
    "            start_steps = start_steps.to(device=device)\n",
    "        # Generate time steps for each sequence based on the start_steps\n",
    "        time_indices = torch.arange(t, device=device).unsqueeze(\n",
    "            0\n",
    "        ) + start_steps.unsqueeze(1)\n",
    "        # time_indices will have shape (b, t)\n",
    "\n",
    "        # Embed the time indices\n",
    "        pe = self.embedding(time_indices)  # Shape: (b, t, embedding_dim)\n",
    "\n",
    "        # Expand to match the feature dimension\n",
    "        pe = pe.unsqueeze(2).expand(b, t, f, -1)  # Shape: (b, t, f, embedding_dim)\n",
    "\n",
    "        # Apply the projection layer if needed\n",
    "        pe = self.projection(pe)\n",
    "        pe = pe.to(device)\n",
    "\n",
    "        return pe\n",
    "\n",
    "\n",
    "class FeatureEmbedding(nn.Module):\n",
    "    def __init__(self, num_features, embedding_dim):\n",
    "        super(FeatureEmbedding, self).__init__()\n",
    "\n",
    "        # Learnable embeddings for the feature indices\n",
    "        self.embedding = nn.Embedding(num_features, embedding_dim)\n",
    "\n",
    "        # Optional projection layer to process the embeddings further\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, embedding_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(embedding_dim, embedding_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, data, device=\"cpu\"):\n",
    "        b, t, f, e = data.shape\n",
    "\n",
    "        # Generate feature indices (0 to f-1)\n",
    "        feature_indices = torch.arange(f, device=device)\n",
    "\n",
    "        # Embed the feature indices\n",
    "        pe = self.embedding(feature_indices)  # Shape: (f, embedding_dim)\n",
    "\n",
    "        # Expand to match the batch and time dimensions\n",
    "        pe = (\n",
    "            pe.unsqueeze(0).unsqueeze(0).expand(b, t, f, -1)\n",
    "        )  # Shape: (b, t, f, embedding_dim)\n",
    "\n",
    "        # Apply the projection layer if needed\n",
    "        pe = self.projection(pe)\n",
    "        pe = pe.to(device)\n",
    "\n",
    "        return pe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Residual block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Conv1d_with_init(in_channels, out_channels, kernel_size):\n",
    "    layer = nn.Conv1d(in_channels, out_channels, kernel_size)\n",
    "    nn.init.kaiming_normal_(layer.weight)\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ResidualBlock(nn.Module):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         num_heads=8,\n",
    "#         num_cells=1,\n",
    "#         kernel_size=(3, 7),\n",
    "#         embed_dim=128,\n",
    "#         ff_dim=512,\n",
    "#         dropout=0.1,\n",
    "#         method=\"rsa\",\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.method = method\n",
    "\n",
    "#         self.embedding_add = nn.Sequential(\n",
    "#             nn.Linear(embed_dim * 4, embed_dim * 4),\n",
    "#             nn.SiLU(),\n",
    "#             nn.Linear(embed_dim * 4, embed_dim * 2),\n",
    "#             nn.SiLU(),\n",
    "#             nn.Linear(embed_dim * 2, embed_dim),\n",
    "#         )\n",
    "\n",
    "#         self.layer_norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "#         self.mid_projection = Conv1d_with_init(embed_dim, 2 * embed_dim, 1)\n",
    "#         # nn.Linear(embed_dim, embed_dim*2)\n",
    "#         self.output_projection = Conv1d_with_init(embed_dim, 2 * embed_dim, 1)\n",
    "#         # self.output_projection = nn.Linear(embed_dim, embed_dim*2)\n",
    "\n",
    "#         if method == \"rsa\":\n",
    "#             self.feature_and_time_transformer = TransformerEncoder(\n",
    "#                 embed_dim=embed_dim,\n",
    "#                 num_heads=num_heads,\n",
    "#                 kernel_size=kernel_size,\n",
    "#                 ff_dim=ff_dim,\n",
    "#                 num_cells=num_cells,\n",
    "#                 dropout=dropout,\n",
    "#             )\n",
    "#             self.linear_time_and_feature = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "#         elif method == \"csdi\":\n",
    "#             self.time_layer = get_torch_trans(\n",
    "#                 num_heads=num_heads,\n",
    "#                 num_cells=num_cells,\n",
    "#                 embed_dim=embed_dim,\n",
    "#                 ff_dim=ff_dim,\n",
    "#                 dropout=dropout,\n",
    "#             )\n",
    "#             self.feature_layer = get_torch_trans(\n",
    "#                 num_heads=num_heads,\n",
    "#                 num_cells=num_cells,\n",
    "#                 embed_dim=embed_dim,\n",
    "#                 ff_dim=ff_dim,\n",
    "#                 dropout=dropout,\n",
    "#             )\n",
    "#             self.linear_time = nn.Linear(embed_dim, embed_dim)\n",
    "#             self.linear_feature = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "#         elif method == \"csdi_moded_transformer\":\n",
    "#             self.time_layer = get_torch_trans(\n",
    "#                 num_heads=num_heads,\n",
    "#                 num_cells=num_cells,\n",
    "#                 embed_dim=embed_dim,\n",
    "#                 ff_dim=ff_dim,\n",
    "#                 dropout=dropout,\n",
    "#             )\n",
    "#             self.feature_layer = get_torch_trans(\n",
    "#                 num_heads=num_heads,\n",
    "#                 num_cells=num_cells,\n",
    "#                 embed_dim=embed_dim,\n",
    "#                 ff_dim=ff_dim,\n",
    "#                 dropout=dropout,\n",
    "#             )\n",
    "#             self.linear_time = nn.Linear(embed_dim, embed_dim)\n",
    "#             self.linear_feature = nn.Linear(embed_dim, embed_dim)\n",
    "#             self.feature_and_time_transformer = moded_TransformerEncoder(\n",
    "#                 embed_dim=embed_dim,\n",
    "#                 num_heads=num_heads,\n",
    "#                 ff_dim=ff_dim,\n",
    "#                 num_cells=num_cells,\n",
    "#                 dropout=dropout,\n",
    "#             )\n",
    "#             self.linear_time_and_feature = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "#         elif method == \"rsa_csdi\":\n",
    "#             self.time_layer = get_torch_trans(\n",
    "#                 num_heads=num_heads,\n",
    "#                 num_cells=num_cells,\n",
    "#                 embed_dim=embed_dim,\n",
    "#                 ff_dim=ff_dim,\n",
    "#                 dropout=dropout,\n",
    "#             )\n",
    "#             self.feature_layer = get_torch_trans(\n",
    "#                 num_heads=num_heads,\n",
    "#                 num_cells=num_cells,\n",
    "#                 embed_dim=embed_dim,\n",
    "#                 ff_dim=ff_dim,\n",
    "#                 dropout=dropout,\n",
    "#             )\n",
    "#             self.linear_time = nn.Linear(embed_dim, embed_dim)\n",
    "#             self.linear_feature = nn.Linear(embed_dim, embed_dim)\n",
    "#             self.feature_and_time_transformer = TransformerEncoder(\n",
    "#                 embed_dim=embed_dim,\n",
    "#                 num_heads=num_heads,\n",
    "#                 kernel_size=kernel_size,\n",
    "#                 ff_dim=ff_dim,\n",
    "#                 num_cells=num_cells,\n",
    "#                 dropout=dropout,\n",
    "#             )\n",
    "#             self.linear_time_and_feature = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "#         elif method == \"rsa_moded_transformer\":\n",
    "#             self.feature_and_time_transformer = TransformerEncoder(\n",
    "#                 embed_dim=embed_dim,\n",
    "#                 num_heads=num_heads,\n",
    "#                 kernel_size=kernel_size,\n",
    "#                 ff_dim=ff_dim,\n",
    "#                 num_cells=num_cells,\n",
    "#                 dropout=dropout,\n",
    "#             )\n",
    "#             self.linear_time_and_feature = nn.Linear(embed_dim, embed_dim)\n",
    "#             self.moded_feature_and_time_transformer = moded_TransformerEncoder(\n",
    "#                 embed_dim=embed_dim,\n",
    "#                 num_heads=num_heads,\n",
    "#                 ff_dim=ff_dim,\n",
    "#                 num_cells=num_cells,\n",
    "#                 dropout=dropout,\n",
    "#             )\n",
    "#             self.moded_linear_time_and_feature = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "#         elif method == \"moded_transformer_alone\":\n",
    "#             self.moded_feature_and_time_transformer = moded_TransformerEncoder(\n",
    "#                 embed_dim=embed_dim,\n",
    "#                 num_heads=num_heads,\n",
    "#                 ff_dim=ff_dim,\n",
    "#                 num_cells=num_cells,\n",
    "#                 dropout=dropout,\n",
    "#             )\n",
    "#             self.moded_linear_time_and_feature = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "#         else:\n",
    "#             raise NotImplementedError\n",
    "\n",
    "#     def forward_time(self, y, base_shape):\n",
    "#         b, t, f, e = base_shape\n",
    "#         y = y.permute(0, 2, 1, 3).reshape(b * f, t, e)\n",
    "#         y = self.time_layer(y.permute(1, 0, 2)).permute(1, 0, 2)\n",
    "#         y = y.reshape(b, f, t, e).permute(0, 2, 1, 3)\n",
    "#         return y\n",
    "\n",
    "#     def forward_feature(self, y, base_shape):\n",
    "#         b, t, f, e = base_shape\n",
    "#         y = y.reshape(b * t, f, e)\n",
    "#         y = self.feature_layer(y.permute(1, 0, 2)).permute(1, 0, 2)\n",
    "#         y = y.reshape(b, t, f, e)\n",
    "#         return y\n",
    "\n",
    "#     def forward(self, noised_data, diffusion_emb, time_emb, feature_emb):\n",
    "\n",
    "#         b, t, f, e = noised_data.shape\n",
    "#         base_shape = noised_data.shape\n",
    "\n",
    "#         y = torch.stack((noised_data, diffusion_emb, time_emb, feature_emb), dim=-1)\n",
    "#         y = y.reshape(b, t, f, -1)\n",
    "#         y = self.embedding_add(y)\n",
    "#         y_resid = y\n",
    "\n",
    "#         if self.method == \"rsa\":\n",
    "#             y = self.feature_and_time_transformer(y)\n",
    "#             y = y.squeeze(-1)\n",
    "#             y = self.linear_time_and_feature(y)\n",
    "\n",
    "#         elif self.method == \"csdi\":\n",
    "#             y = self.forward_time(y, base_shape)\n",
    "#             y = self.linear_time(y)\n",
    "#             y = (y + y_resid) / math.sqrt(2.0)\n",
    "#             y = self.layer_norm(y)\n",
    "#             y = self.forward_feature(y, base_shape)\n",
    "#             y = self.linear_feature(y)\n",
    "\n",
    "#         elif self.method == \"csdi_moded_transformer\":\n",
    "#             y = self.forward_time(y, base_shape)\n",
    "#             y = self.linear_time(y)\n",
    "#             y = (y + y_resid) / math.sqrt(2.0)\n",
    "#             y = self.layer_norm(y)\n",
    "#             y = self.forward_feature(y, base_shape)\n",
    "#             y = self.linear_feature(y)\n",
    "#             y = (y + y_resid) / math.sqrt(2.0)\n",
    "#             y_resid = y\n",
    "#             y = self.layer_norm(y)\n",
    "#             y = self.feature_and_time_transformer(y)\n",
    "#             y = self.linear_time_and_feature(y)\n",
    "\n",
    "#         elif self.method == \"rsa_csdi\":\n",
    "#             y = self.forward_time(y, base_shape)\n",
    "#             y = self.linear_time(y)\n",
    "#             y = (y + y_resid) / math.sqrt(2.0)\n",
    "#             y = self.layer_norm(y)\n",
    "#             y = self.forward_feature(y, base_shape)\n",
    "#             y = self.linear_feature(y)\n",
    "#             y = (y + y_resid) / math.sqrt(2.0)\n",
    "#             y_resid = y\n",
    "#             y = self.layer_norm(y)\n",
    "#             y = self.feature_and_time_transformer(y)\n",
    "#             y = y.squeeze(-1)\n",
    "#             y = self.linear_time_and_feature(y)\n",
    "\n",
    "#         elif self.method == \"rsa_moded_transformer\":\n",
    "#             y = self.feature_and_time_transformer(y)\n",
    "#             y = y.squeeze(-1)\n",
    "#             y = self.linear_time_and_feature(y)\n",
    "#             y = (y + y_resid) / math.sqrt(2.0)\n",
    "#             y = self.layer_norm(y)\n",
    "#             y = self.moded_feature_and_time_transformer(y)\n",
    "#             y = self.moded_linear_time_and_feature(y)\n",
    "\n",
    "#         elif self.method == \"moded_transformer_alone\":\n",
    "#             y = self.moded_feature_and_time_transformer(y)\n",
    "#             y = self.moded_linear_time_and_feature(y)\n",
    "\n",
    "#         y = (y + y_resid) / math.sqrt(2.0)\n",
    "#         y = self.layer_norm(y)\n",
    "#         y = y.permute(0, 3, 1, 2).reshape(b, e, t * f)\n",
    "#         y = self.mid_projection(y)\n",
    "#         # y = y.permute(0, 3, 2, 1).reshape(b, 2*e, t*f)\n",
    "\n",
    "#         gate, filter = torch.chunk(y, 2, dim=1)\n",
    "#         y = torch.sigmoid(gate) * torch.tanh(filter)  # (b,e,f*t)\n",
    "#         # y = y.permute(0, 2, 1)\n",
    "#         y = self.output_projection(y)\n",
    "#         # y = y.permute(0, 2, 1)\n",
    "\n",
    "#         residual, skip = torch.chunk(y, 2, dim=1)\n",
    "#         residual = residual.permute(0, 2, 1)\n",
    "#         skip = skip.permute(0, 2, 1)\n",
    "#         residual = residual.reshape(base_shape)\n",
    "#         skip = skip.reshape(base_shape)\n",
    "#         return (noised_data + residual) / math.sqrt(2.0), skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_heads=8,\n",
    "        num_cells=1,\n",
    "        kernel_size=(3, 7),\n",
    "        embed_dim=128,\n",
    "        ff_dim=512,\n",
    "        dropout=0.1,\n",
    "        method=\"rsa\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.method = method\n",
    "\n",
    "        # self.embedding_add = nn.Sequential(\n",
    "        #     nn.Linear(embed_dim * 4, embed_dim * 4),\n",
    "        #     nn.SiLU(),\n",
    "        #     nn.Linear(embed_dim * 4, embed_dim * 2),\n",
    "        #     nn.SiLU(),\n",
    "        #     nn.Linear(embed_dim * 2, embed_dim),\n",
    "        # )\n",
    "\n",
    "        # use conv1d instead of linear\n",
    "        # self.embedding_add = Conv1d_with_init(3, 1, 1)\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        # self.mid_projection = Conv1d_with_init(embed_dim, 2 * embed_dim, 1)\n",
    "        # nn.Linear(embed_dim, embed_dim*2)\n",
    "        # self.output_projection = Conv1d_with_init(embed_dim, 2 * embed_dim, 1)\n",
    "        # self.output_projection = nn.Linear(embed_dim, embed_dim*2)\n",
    "\n",
    "        self.output_projection = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim), nn.SiLU(), nn.Linear(embed_dim, embed_dim)\n",
    "        )\n",
    "\n",
    "        if method == \"rsa\":\n",
    "            self.feature_and_time_transformer = TransformerEncoder(\n",
    "                embed_dim=embed_dim,\n",
    "                num_heads=num_heads,\n",
    "                kernel_size=kernel_size,\n",
    "                ff_dim=ff_dim,\n",
    "                num_cells=num_cells,\n",
    "                dropout=dropout,\n",
    "            )\n",
    "            self.linear_time_and_feature = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        elif method == \"csdi\":\n",
    "            self.time_layer = get_torch_trans(\n",
    "                num_heads=num_heads,\n",
    "                num_cells=num_cells,\n",
    "                embed_dim=embed_dim,\n",
    "                ff_dim=ff_dim,\n",
    "                dropout=dropout,\n",
    "            )\n",
    "            self.feature_layer = get_torch_trans(\n",
    "                num_heads=num_heads,\n",
    "                num_cells=num_cells,\n",
    "                embed_dim=embed_dim,\n",
    "                ff_dim=ff_dim,\n",
    "                dropout=dropout,\n",
    "            )\n",
    "            self.linear_time = nn.Linear(embed_dim, embed_dim)\n",
    "            self.linear_feature = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        elif method == \"csdi_moded_transformer\":\n",
    "            self.time_layer = get_torch_trans(\n",
    "                num_heads=num_heads,\n",
    "                num_cells=num_cells,\n",
    "                embed_dim=embed_dim,\n",
    "                ff_dim=ff_dim,\n",
    "                dropout=dropout,\n",
    "            )\n",
    "            self.feature_layer = get_torch_trans(\n",
    "                num_heads=num_heads,\n",
    "                num_cells=num_cells,\n",
    "                embed_dim=embed_dim,\n",
    "                ff_dim=ff_dim,\n",
    "                dropout=dropout,\n",
    "            )\n",
    "            self.linear_time = nn.Linear(embed_dim, embed_dim)\n",
    "            self.linear_feature = nn.Linear(embed_dim, embed_dim)\n",
    "            self.feature_and_time_transformer = moded_TransformerEncoder(\n",
    "                embed_dim=embed_dim,\n",
    "                num_heads=num_heads,\n",
    "                ff_dim=ff_dim,\n",
    "                num_cells=num_cells,\n",
    "                dropout=dropout,\n",
    "            )\n",
    "            self.linear_time_and_feature = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        elif method == \"rsa_csdi\":\n",
    "            self.time_layer = get_torch_trans(\n",
    "                num_heads=num_heads,\n",
    "                num_cells=num_cells,\n",
    "                embed_dim=embed_dim,\n",
    "                ff_dim=ff_dim,\n",
    "                dropout=dropout,\n",
    "            )\n",
    "            self.feature_layer = get_torch_trans(\n",
    "                num_heads=num_heads,\n",
    "                num_cells=num_cells,\n",
    "                embed_dim=embed_dim,\n",
    "                ff_dim=ff_dim,\n",
    "                dropout=dropout,\n",
    "            )\n",
    "            self.linear_time = nn.Linear(embed_dim, embed_dim)\n",
    "            self.linear_feature = nn.Linear(embed_dim, embed_dim)\n",
    "            self.feature_and_time_transformer = TransformerEncoder(\n",
    "                embed_dim=embed_dim,\n",
    "                num_heads=num_heads,\n",
    "                kernel_size=kernel_size,\n",
    "                ff_dim=ff_dim,\n",
    "                num_cells=num_cells,\n",
    "                dropout=dropout,\n",
    "            )\n",
    "            self.linear_time_and_feature = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        elif method == \"rsa_moded_transformer\":\n",
    "            self.feature_and_time_transformer = TransformerEncoder(\n",
    "                embed_dim=embed_dim,\n",
    "                num_heads=num_heads,\n",
    "                kernel_size=kernel_size,\n",
    "                ff_dim=ff_dim,\n",
    "                num_cells=num_cells,\n",
    "                dropout=dropout,\n",
    "            )\n",
    "            self.linear_time_and_feature = nn.Linear(embed_dim, embed_dim)\n",
    "            self.moded_feature_and_time_transformer = moded_TransformerEncoder(\n",
    "                embed_dim=embed_dim,\n",
    "                num_heads=num_heads,\n",
    "                ff_dim=ff_dim,\n",
    "                num_cells=num_cells,\n",
    "                dropout=dropout,\n",
    "            )\n",
    "            self.moded_linear_time_and_feature = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        elif method == \"moded_transformer_alone\":\n",
    "            self.moded_feature_and_time_transformer = moded_TransformerEncoder(\n",
    "                embed_dim=embed_dim,\n",
    "                num_heads=num_heads,\n",
    "                ff_dim=ff_dim,\n",
    "                num_cells=num_cells,\n",
    "                dropout=dropout,\n",
    "            )\n",
    "            self.moded_linear_time_and_feature = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def forward_time(self, y, base_shape):\n",
    "        b, t, f, e = base_shape\n",
    "        y = y.permute(0, 2, 1, 3).reshape(b * f, t, -1)\n",
    "        y = self.time_layer(y.permute(1, 0, 2)).permute(1, 0, 2)\n",
    "        y = y.reshape(b, f, t, -1).permute(0, 2, 1, 3)\n",
    "        return y\n",
    "\n",
    "    def forward_feature(self, y, base_shape):\n",
    "        b, t, f, e = base_shape\n",
    "        y = y.reshape(b * t, f, e)\n",
    "        y = self.feature_layer(y.permute(1, 0, 2)).permute(1, 0, 2)\n",
    "        y = y.reshape(b, t, f, e)\n",
    "        return y\n",
    "\n",
    "    def forward(self, noised_data, diffusion_emb, time_emb, feature_emb):\n",
    "        b, t, f, e = noised_data.shape\n",
    "        base_shape = noised_data.shape\n",
    "        # y = torch.stack((noised_data, diffusion_emb, time_emb, feature_emb), dim=-1)\n",
    "        # embeddings = torch.stack((diffusion_emb, time_emb, feature_emb), dim=-1)\n",
    "        # embeddings = embeddings.reshape(b, 3, -1)\n",
    "        # print(noised_data.shape)\n",
    "        # embeddings = self.embedding_add(embeddings)\n",
    "        # embeddings = embeddings.reshape(b, t, f, 1).squeeze(-1)\n",
    "        embeddings = diffusion_emb + time_emb + feature_emb\n",
    "        y = noised_data + embeddings\n",
    "        # make embeddings into the same shape as noised_data\n",
    "        # y = y.reshape(b, 4, -1)\n",
    "        # y = self.embedding_add(y)\n",
    "        # y = y.reshape(b, t, f, e)\n",
    "        y_resid = y\n",
    "\n",
    "        if self.method == \"rsa\":\n",
    "            y = self.feature_and_time_transformer(y)\n",
    "            y = y.squeeze(-1)\n",
    "            y = self.linear_time_and_feature(y)\n",
    "\n",
    "        elif self.method == \"csdi\":\n",
    "            y = self.forward_time(y, base_shape)\n",
    "            y = self.linear_time(y)\n",
    "            y = (y + y_resid) / math.sqrt(2.0)\n",
    "            y = self.layer_norm(y)\n",
    "            y = self.forward_feature(y, base_shape)\n",
    "            y = self.linear_feature(y)\n",
    "\n",
    "        elif self.method == \"csdi_moded_transformer\":\n",
    "            y = self.forward_time(y, base_shape)\n",
    "            y = self.linear_time(y)\n",
    "            y = (y + y_resid) / math.sqrt(2.0)\n",
    "            y = self.layer_norm(y)\n",
    "            y = self.forward_feature(y, base_shape)\n",
    "            y = self.linear_feature(y)\n",
    "            y = (y + y_resid) / math.sqrt(2.0)\n",
    "            y_resid = y\n",
    "            y = self.layer_norm(y)\n",
    "            y = self.feature_and_time_transformer(y)\n",
    "            y = self.linear_time_and_feature(y)\n",
    "\n",
    "        elif self.method == \"rsa_csdi\":\n",
    "            y = self.forward_time(y, base_shape)\n",
    "            y = self.linear_time(y)\n",
    "            y = (y + y_resid) / math.sqrt(2.0)\n",
    "            y = self.layer_norm(y)\n",
    "            y = self.forward_feature(y, base_shape)\n",
    "            y = self.linear_feature(y)\n",
    "            y = (y + y_resid) / math.sqrt(2.0)\n",
    "            y_resid = y\n",
    "            y = self.layer_norm(y)\n",
    "            y = self.feature_and_time_transformer(y)\n",
    "            y = y.squeeze(-1)\n",
    "            y = self.linear_time_and_feature(y)\n",
    "\n",
    "        elif self.method == \"rsa_moded_transformer\":\n",
    "            y = self.feature_and_time_transformer(y)\n",
    "            y = y.squeeze(-1)\n",
    "            y = self.linear_time_and_feature(y)\n",
    "            y = (y + y_resid) / math.sqrt(2.0)\n",
    "            y = self.layer_norm(y)\n",
    "            y = self.moded_feature_and_time_transformer(y)\n",
    "            y = self.moded_linear_time_and_feature(y)\n",
    "\n",
    "        elif self.method == \"moded_transformer_alone\":\n",
    "            y = self.moded_feature_and_time_transformer(y)\n",
    "            y = self.moded_linear_time_and_feature(y)\n",
    "\n",
    "        # y = (y + y_resid) / math.sqrt(2.0)\n",
    "        # y = self.layer_norm(y)\n",
    "        y = self.output_projection(y)\n",
    "        # y = y.permute(0, 3, 1, 2).reshape(b, e, t * f)\n",
    "        # y = self.mid_projection(y)\n",
    "        # y = y.permute(0, 3, 2, 1).reshape(b, 2*e, t*f)\n",
    "\n",
    "        # gate, filter = torch.chunk(y, 2, dim=1)\n",
    "        # y = torch.sigmoid(gate) * torch.tanh(filter)  # (b,e,f*t)\n",
    "        # y = y.permute(0, 2, 1)\n",
    "        # y = self.output_projection(y)\n",
    "        # y = y.permute(0, 2, 1)\n",
    "\n",
    "        # residual, skip = torch.chunk(y, 2, dim=1)\n",
    "        # residual = residual.permute(0, 2, 1)\n",
    "        # skip = skip.permute(0, 2, 1)\n",
    "        # residual = residual.reshape(base_shape)\n",
    "        # skip = skip.reshape(base_shape)\n",
    "        # return (noised_data + residual) / math.sqrt(2.0), skip\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ModelLoop(nn.Module):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         embed_dim=128,\n",
    "#         diffusion_steps=1000,\n",
    "#         max_time_steps=10000,\n",
    "#         feature_count=10,\n",
    "#         num_heads=8,\n",
    "#         kernel_size=(3, 7),\n",
    "#         num_cells=1,\n",
    "#         num_residual_layers=4,\n",
    "#         ff_dim=512,\n",
    "#         dropout=0.1,\n",
    "#         method=\"rsa\",\n",
    "#         device=\"cpu\",\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.device = device\n",
    "#         self.emb_dim = embed_dim\n",
    "\n",
    "#         self.data_embedding_linear = nn.Sequential(\n",
    "#             nn.Linear(1, self.emb_dim), nn.SiLU(), nn.Linear(self.emb_dim, self.emb_dim)\n",
    "#         )\n",
    "#         self.x_embedding = nn.Sequential(\n",
    "#             nn.Linear(2, self.emb_dim), nn.SiLU(), nn.Linear(self.emb_dim, self.emb_dim)\n",
    "#         )\n",
    "\n",
    "#         # self.data_embedding_linear = Conv1d_with_init(1, self.emb_dim, 1)\n",
    "#         # self.x_embedding = Conv1d_with_init(2, self.emb_dim, 1)\n",
    "\n",
    "#         # self.output = Conv1d_with_init(self.emb_dim, 1, 1)\n",
    "#         # self.output_final = Conv1d_with_init(self.emb_dim, 1, 1)\n",
    "\n",
    "#         self.output = nn.Sequential(\n",
    "#             nn.Linear(self.emb_dim, self.emb_dim),\n",
    "#             nn.SiLU(),\n",
    "#             nn.Linear(self.emb_dim, 1),\n",
    "#         )\n",
    "\n",
    "#         self.output_final = nn.Sequential(\n",
    "#             nn.Linear(self.emb_dim, self.emb_dim),\n",
    "#             nn.SiLU(),\n",
    "#             nn.Linear(self.emb_dim, 1),\n",
    "#         )\n",
    "\n",
    "#         # self.x_add = nn.Sequential(\n",
    "#         #     nn.Linear(embed_dim*num_residual_layers, embed_dim*num_residual_layers),\n",
    "#         #     nn.SiLU(),\n",
    "#         #     nn.Linear(embed_dim*num_residual_layers, embed_dim)\n",
    "#         # )\n",
    "\n",
    "#         self.diffusion_embedding = DiffusionEmbedding(diffusion_steps, embed_dim)\n",
    "#         self.time_embedding = TimeEmbedding(max_time_steps, embed_dim)\n",
    "#         self.feature_embedding = FeatureEmbedding(feature_count, embed_dim)\n",
    "\n",
    "#         self.residual_layers = nn.ModuleList(\n",
    "#             ResidualBlock(\n",
    "#                 num_heads=num_heads,\n",
    "#                 num_cells=num_cells,\n",
    "#                 kernel_size=kernel_size,\n",
    "#                 embed_dim=embed_dim,\n",
    "#                 ff_dim=ff_dim,\n",
    "#                 dropout=dropout,\n",
    "#                 method=method,\n",
    "#             )\n",
    "#             for _ in range(num_residual_layers)\n",
    "#         )\n",
    "\n",
    "#         # self.output = nn.Sequential(\n",
    "#         #     nn.Linear(self.emb_dim, self.emb_dim),\n",
    "#         #     nn.SiLU(),\n",
    "#         #     nn.Linear(self.emb_dim, 1)\n",
    "#         # )\n",
    "\n",
    "#         # self.output_final = nn.Sequential(\n",
    "#         #     nn.Linear(self.emb_dim, self.emb_dim),\n",
    "#         #     nn.SiLU(),\n",
    "#         #     nn.Linear(self.emb_dim, 1)\n",
    "#         # )\n",
    "\n",
    "#     def forward(self, noised_data, diffusion_t, start_time=0):\n",
    "\n",
    "#         b, t, f = noised_data.shape\n",
    "\n",
    "#         # noised_data_reshaped = noised_data.permute(0, 3, 1, 2).reshape(b, 1, t * f)\n",
    "#         noised_data_reshaped = noised_data.unsqueeze(-1)\n",
    "#         noised_data_embedded = (\n",
    "#             self.data_embedding_linear(noised_data_reshaped)\n",
    "#             # .permute(0, 2, 1)\n",
    "#             # .reshape(b, t, f, self.emb_dim)\n",
    "#         )\n",
    "\n",
    "#         diffusion_embedding = self.diffusion_embedding(\n",
    "#             diffusion_t, noised_data_embedded, device=self.device\n",
    "#         )\n",
    "\n",
    "#         time_embedding = self.time_embedding(\n",
    "#             start_time, noised_data_embedded, device=self.device\n",
    "#         )\n",
    "\n",
    "#         feature_embedding = self.feature_embedding(\n",
    "#             noised_data_embedded, device=self.device\n",
    "#         )\n",
    "\n",
    "#         x = noised_data_embedded\n",
    "#         skip = []\n",
    "#         for layer in self.residual_layers:\n",
    "#             x, skip_connection = layer(\n",
    "#                 x, diffusion_embedding, time_embedding, feature_embedding\n",
    "#             )\n",
    "#             skip.append(skip_connection)\n",
    "#             # x = x.permute(0, 3, 1, 2).reshape(b, self.emb_dim, t * f)\n",
    "#             # x = self.output(x).permute(0, 2, 1).reshape(b, t, f)\n",
    "#             x = self.output(x).squeeze(-1)\n",
    "#             x = torch.stack((x, noised_data), dim=-1)\n",
    "#             # x = x * noise_mask + noised_data * (1 - noise_mask) #check later\n",
    "#             # x = x.permute(0, 3, 1, 2).reshape(b, 2, t * f)\n",
    "#             # x = self.x_embedding(x).permute(0, 2, 1).reshape(b, t, f, self.emb_dim)\n",
    "#             x = self.x_embedding(x)\n",
    "\n",
    "#         x = torch.sum(torch.stack(skip, dim=-1), dim=-1) / math.sqrt(\n",
    "#             len(self.residual_layers)\n",
    "#         )\n",
    "#         # x = torch.stack(skip, dim = -1).reshape(b, t, f, -1)\n",
    "#         # x = self.x_add(x)\n",
    "#         # x = x.permute(0, 3, 1, 2).reshape(b, self.emb_dim, t * f)\n",
    "#         # x = self.output_final(x).permute(0, 2, 1).reshape(b, t, f, 1).squeeze(-1)\n",
    "#         x = self.output_final(x).squeeze(-1)\n",
    "\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelLoop(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim=128,\n",
    "        diffusion_steps=1000,\n",
    "        max_time_steps=10000,\n",
    "        feature_count=10,\n",
    "        num_heads=8,\n",
    "        kernel_size=(3, 7),\n",
    "        num_cells=1,\n",
    "        num_residual_layers=4,\n",
    "        ff_dim=512,\n",
    "        dropout=0.1,\n",
    "        method=\"rsa\",\n",
    "        device=\"cpu\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self.emb_dim = embed_dim\n",
    "\n",
    "        self.data_embedding_linear = nn.Sequential(\n",
    "            nn.Linear(1, self.emb_dim), nn.SiLU(), nn.Linear(self.emb_dim, self.emb_dim)\n",
    "        )\n",
    "        self.x_embedding = nn.Sequential(\n",
    "            nn.Linear(2, self.emb_dim), nn.SiLU(), nn.Linear(self.emb_dim, self.emb_dim)\n",
    "        )\n",
    "\n",
    "        # self.data_embedding_linear = Conv1d_with_init(1, self.emb_dim, 1)\n",
    "        # self.x_embedding = Conv1d_with_init(2, self.emb_dim, 1)\n",
    "\n",
    "        # self.output = Conv1d_with_init(self.emb_dim, 1, 1)\n",
    "        # self.output_final = Conv1d_with_init(self.emb_dim, 1, 1)\n",
    "\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(self.emb_dim, self.emb_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(self.emb_dim, 1),\n",
    "        )\n",
    "\n",
    "        self.output_final = nn.Sequential(\n",
    "            nn.Linear(self.emb_dim, self.emb_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(self.emb_dim, 1),\n",
    "        )\n",
    "\n",
    "        # self.x_add = nn.Sequential(\n",
    "        #     nn.Linear(embed_dim*num_residual_layers, embed_dim*num_residual_layers),\n",
    "        #     nn.SiLU(),\n",
    "        #     nn.Linear(embed_dim*num_residual_layers, embed_dim)\n",
    "        # )\n",
    "\n",
    "        self.diffusion_embedding = DiffusionEmbedding(diffusion_steps, embed_dim)\n",
    "        self.time_embedding = TimeEmbedding(max_time_steps, embed_dim)\n",
    "        self.feature_embedding = FeatureEmbedding(feature_count, embed_dim)\n",
    "\n",
    "        self.residual_layers = nn.ModuleList(\n",
    "            ResidualBlock(\n",
    "                num_heads=num_heads,\n",
    "                num_cells=num_cells,\n",
    "                kernel_size=kernel_size,\n",
    "                embed_dim=embed_dim,\n",
    "                ff_dim=ff_dim,\n",
    "                dropout=dropout,\n",
    "                method=method,\n",
    "            )\n",
    "            for _ in range(num_residual_layers)\n",
    "        )\n",
    "\n",
    "        # self.output = nn.Sequential(\n",
    "        #     nn.Linear(self.emb_dim, self.emb_dim),\n",
    "        #     nn.SiLU(),\n",
    "        #     nn.Linear(self.emb_dim, 1)\n",
    "        # )\n",
    "\n",
    "        # self.output_final = nn.Sequential(\n",
    "        #     nn.Linear(self.emb_dim, self.emb_dim),\n",
    "        #     nn.SiLU(),\n",
    "        #     nn.Linear(self.emb_dim, 1)\n",
    "        # )\n",
    "\n",
    "    def forward(self, noised_data, diffusion_t, start_time=0):\n",
    "\n",
    "        # print(noised_data.shape)\n",
    "\n",
    "        b, t, f = noised_data.shape\n",
    "\n",
    "        # noised_data_reshaped = noised_data.permute(0, 3, 1, 2).reshape(b, 1, t * f)\n",
    "        noised_data_reshaped = noised_data.unsqueeze(-1)\n",
    "        noised_data_embedded = (\n",
    "            self.data_embedding_linear(noised_data_reshaped)\n",
    "            # .permute(0, 2, 1)\n",
    "            # .reshape(b, t, f, self.emb_dim)\n",
    "        )\n",
    "\n",
    "        diffusion_embedding = self.diffusion_embedding(\n",
    "            diffusion_t, noised_data_embedded, device=self.device\n",
    "        )\n",
    "\n",
    "        time_embedding = self.time_embedding(\n",
    "            start_time, noised_data_embedded, device=self.device\n",
    "        )\n",
    "\n",
    "        feature_embedding = self.feature_embedding(\n",
    "            noised_data_embedded, device=self.device\n",
    "        )\n",
    "\n",
    "        x = noised_data_embedded\n",
    "        skip = []\n",
    "        for layer in self.residual_layers:\n",
    "            # x, skip_connection = layer(\n",
    "            #     x, diffusion_embedding, time_embedding, feature_embedding\n",
    "            # )\n",
    "            x = layer(x, diffusion_embedding, time_embedding, feature_embedding)\n",
    "            # skip.append(skip_connection)\n",
    "            # x = x.permute(0, 3, 1, 2).reshape(b, self.emb_dim, t * f)\n",
    "            # x = self.output(x).permute(0, 2, 1).reshape(b, t, f)\n",
    "            # x = self.output(x).squeeze(-1)\n",
    "            # x = torch.stack((x, noised_data), dim=-1)\n",
    "            # x = x * noise_mask + noised_data * (1 - noise_mask) #check later\n",
    "            # x = x.permute(0, 3, 1, 2).reshape(b, 2, t * f)\n",
    "            # x = self.x_embedding(x).permute(0, 2, 1).reshape(b, t, f, self.emb_dim)\n",
    "            # x = self.x_embedding(x)\n",
    "\n",
    "        # x = torch.sum(torch.stack(skip, dim=-1), dim=-1) / math.sqrt(\n",
    "        #     len(self.residual_layers)\n",
    "        # )\n",
    "        # x = torch.stack(skip, dim = -1).reshape(b, t, f, -1)\n",
    "        # x = self.x_add(x)\n",
    "        # x = x.permute(0, 3, 1, 2).reshape(b, self.emb_dim, t * f)\n",
    "        # x = self.output_final(x).permute(0, 2, 1).reshape(b, t, f, 1).squeeze(-1)\n",
    "        x = self.output_final(x).squeeze(-1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beta Schedules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_named_beta_schedule(schedule_name, num_diffusion_timesteps):\n",
    "    \"\"\"\n",
    "    Get a pre-defined beta schedule for the given name.\n",
    "\n",
    "    The beta schedule library consists of beta schedules which remain similar\n",
    "    in the limit of num_diffusion_timesteps.\n",
    "    Beta schedules may be added, but should not be removed or changed once\n",
    "    they are committed to maintain backwards compatibility.\n",
    "    \"\"\"\n",
    "    if schedule_name == \"linear\":\n",
    "        # Linear schedule from Ho et al, extended to work for any number of\n",
    "        # diffusion steps.\n",
    "        scale = 1000 / num_diffusion_timesteps\n",
    "        beta_start = scale * 0.0001\n",
    "        beta_end = scale * 0.02\n",
    "        return torch.linspace(beta_start, beta_end, num_diffusion_timesteps)\n",
    "    elif schedule_name == \"cosine\":\n",
    "        return betas_for_alpha_bar(\n",
    "            num_diffusion_timesteps,\n",
    "            lambda t: math.cos((t + 0.008) / 1.008 * math.pi / 2) ** 2,\n",
    "        )\n",
    "    elif schedule_name == \"quadratic\":\n",
    "        scale = 50 / num_diffusion_timesteps\n",
    "        beta_start = scale * 0.0001\n",
    "        beta_end = scale * 0.5\n",
    "        return (\n",
    "            torch.linspace(beta_start**0.5, beta_end**0.5, num_diffusion_timesteps) ** 2\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError(f\"unknown beta schedule: {schedule_name}\")\n",
    "\n",
    "\n",
    "def betas_for_alpha_bar(num_diffusion_timesteps, alpha_bar, max_beta=0.999):\n",
    "    \"\"\"\n",
    "    Create a beta schedule that discretizes the given alpha_t_bar function,\n",
    "    which defines the cumulative product of (1-beta) over time from t = [0,1].\n",
    "\n",
    "    :param num_diffusion_timesteps: the number of betas to produce.\n",
    "    :param alpha_bar: a lambda that takes an argument t from 0 to 1 and\n",
    "                      produces the cumulative product of (1-beta) up to that\n",
    "                      part of the diffusion process.\n",
    "    :param max_beta: the maximum beta to use; use values lower than 1 to\n",
    "                     prevent singularities.\n",
    "    \"\"\"\n",
    "    betas = []\n",
    "    for i in range(num_diffusion_timesteps):\n",
    "        t1 = i / num_diffusion_timesteps\n",
    "        t2 = (i + 1) / num_diffusion_timesteps\n",
    "        betas.append(min(1 - alpha_bar(t2) / alpha_bar(t1), max_beta))\n",
    "    return torch.tensor(betas)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class diffusion_imputation(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        emb_dim,\n",
    "        excluded_features=None,\n",
    "        # vocab_size,\n",
    "        # pad_idx= None,\n",
    "        strategy=\"random\",\n",
    "        num_residual_layers=4,\n",
    "        features_to_impute=None,\n",
    "        features_to_impute_completely=None,\n",
    "        features_to_impute_after_time=None,\n",
    "        last_n_time=1,\n",
    "        missing_prp=0.1,\n",
    "        diffusion_steps=1000,\n",
    "        diffusion_beta_schedule=\"quadratic\",\n",
    "        num_heads=8,\n",
    "        kernel_size=(3, 7),\n",
    "        ff_dim=512,\n",
    "        num_cells=2,\n",
    "        dropout=0.1,\n",
    "        method=\"rsa\",\n",
    "        device=\"cpu\",\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self.emb_dim = emb_dim\n",
    "        self.strategy = strategy\n",
    "        self.features_to_impute = features_to_impute\n",
    "        self.missing_prp = missing_prp\n",
    "        self.diffusion_steps = diffusion_steps\n",
    "        self.last_n_time = last_n_time\n",
    "        self.exclude_features = excluded_features\n",
    "        self.features_to_impute_completely = features_to_impute_completely\n",
    "        self.features_to_impute_after_time = features_to_impute_after_time\n",
    "\n",
    "        # set device to cuda if available\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = \"cuda\"\n",
    "\n",
    "        self.model_loop = ModelLoop(\n",
    "            embed_dim=self.emb_dim,\n",
    "            diffusion_steps=diffusion_steps,\n",
    "            num_heads=num_heads,\n",
    "            kernel_size=kernel_size,\n",
    "            ff_dim=ff_dim,\n",
    "            num_cells=num_cells,\n",
    "            dropout=dropout,\n",
    "            num_residual_layers=num_residual_layers,\n",
    "            method=method,\n",
    "            device=self.device,\n",
    "        )\n",
    "\n",
    "        self.beta = get_named_beta_schedule(diffusion_beta_schedule, diffusion_steps)\n",
    "\n",
    "        # self.beta = torch.linspace(0.0001, 0.5, diffusion_steps)\n",
    "\n",
    "        # self.beta = torch.linspace(\n",
    "        #         0.0001 ** 0.5, 0.5 ** 0.5, diffusion_steps\n",
    "        #     ) ** 2\n",
    "\n",
    "        self.alpha_hat = 1 - self.beta\n",
    "        self.alpha = torch.cumprod(self.alpha_hat, dim=0)\n",
    "        self.alpha_torch = torch.tensor(self.alpha).float()\n",
    "\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def get_mask(self, data, strategy=\"random\"):\n",
    "\n",
    "        b = data.shape[0]\n",
    "        t = data.shape[1]\n",
    "        f = data.shape[2]\n",
    "\n",
    "        if strategy == \"forecasting\":\n",
    "            forecasted_time = torch.randint(2, t, (b, 1, 1))\n",
    "            mask = torch.zeros_like(data)\n",
    "            for i in range(b):\n",
    "                mask[i, forecasted_time[i] :, :] = 1\n",
    "\n",
    "        if strategy == \"forecasting_last_n_time\":\n",
    "            mask = torch.zeros_like(data)\n",
    "            mask[:, -self.last_n_time, :] = 1\n",
    "\n",
    "        if strategy == \"death_prediction\":\n",
    "            mask = torch.zeros_like(data)\n",
    "            # death is the last 7 columns of the data\n",
    "            mask[:, :, -1] = 1\n",
    "\n",
    "        if strategy == \"random_features\":\n",
    "            selected_features = torch.randint(0, f, (b, 1, 1))\n",
    "            mask = torch.zeros_like(data)\n",
    "            mask[:, :, selected_features] = 1\n",
    "\n",
    "        if strategy == \"selected_features\":\n",
    "            mask = torch.zeros_like(data)\n",
    "            mask[:, :, self.features_to_impute] = 1\n",
    "\n",
    "        if strategy == \"selected_features_after_time\":\n",
    "            selected_time = torch.randint(1, t, (b, 1, 1))\n",
    "            mask = torch.zeros_like(data)\n",
    "            mask[:, selected_time:, self.features_to_impute] = 1\n",
    "\n",
    "        if strategy == \"selected_features_last_n_time\":\n",
    "            mask = torch.zeros_like(data)\n",
    "            mask[:, -self.last_n_time :, self.features_to_impute] = 1\n",
    "\n",
    "        if strategy == \"random\":\n",
    "            mask = torch.rand(size=(b, t, f))\n",
    "            mask = mask < self.missing_prp\n",
    "            mask = mask.float()\n",
    "\n",
    "        if strategy == \"selected_features_and_selected_features_after_time\":\n",
    "            mask = torch.zeros_like(data)\n",
    "            mask[:, :, self.features_to_impute_completely] = 1\n",
    "            mask[:, -self.last_n_time :, self.features_to_impute_after_time] = 1\n",
    "\n",
    "        if self.exclude_features is not None:\n",
    "            mask[:, :, self.exclude_features] = 0\n",
    "\n",
    "        return mask\n",
    "\n",
    "    def loss_func(self, predicted_noise, noise, noise_mask):\n",
    "        # noise = torch.nan_to_num(noise, nan=0.0)\n",
    "        # predicted_noise = torch.nan_to_num(predicted_noise, nan=0.0)\n",
    "        residual = noise - predicted_noise\n",
    "        num_obs = torch.sum(noise_mask)\n",
    "        loss = (residual**2).sum() / num_obs\n",
    "        return loss\n",
    "\n",
    "    def forward(self, data, start_time=0):\n",
    "\n",
    "        b, t, f = data.shape\n",
    "        noise_mask = self.get_mask(data, self.strategy).to(self.device)\n",
    "        noise = torch.randn((b, t, f)).to(self.device)\n",
    "        noise = noise_mask * noise\n",
    "\n",
    "        diffusion_t = torch.randint(0, self.diffusion_steps, (b, 1)).squeeze(1)\n",
    "        alpha = self.alpha_torch[diffusion_t].unsqueeze(1).unsqueeze(2).to(self.device)\n",
    "\n",
    "        noised_data = data * noise_mask\n",
    "        noised_data = noised_data * (alpha**0.5) + noise * ((1 - alpha) ** 0.5)\n",
    "        conditional_data = data * (1 - noise_mask)\n",
    "        noised_data = noised_data + conditional_data\n",
    "        noised_data = noised_data.float()\n",
    "\n",
    "        predicted_noise = self.model_loop(noised_data, diffusion_t, start_time)\n",
    "        predicted_noise = predicted_noise * noise_mask\n",
    "\n",
    "        return (predicted_noise, noise, noise_mask)\n",
    "\n",
    "    # def eval_with_grad(self, data, start_time = 0, scale=1):\n",
    "\n",
    "    #     # with torch.no_grad():\n",
    "    #     imputation_mask = self.get_mask(data, self.strategy).to(self.device)\n",
    "    #     conditional_data = data * (1 - imputation_mask)\n",
    "    #     random_noise = torch.randn_like(data) * imputation_mask * scale\n",
    "    #     data_2 = (conditional_data + random_noise).unsqueeze(3)\n",
    "\n",
    "    #     b, ti, f, e = data_2.shape\n",
    "    #     imputed_samples = torch.zeros((b, ti, f)).to(self.device)\n",
    "    #     x = conditional_data + random_noise\n",
    "\n",
    "    #     for t in range(self.diffusion_steps - 1, -1, -1):\n",
    "\n",
    "    #         x = x.unsqueeze(3).float()\n",
    "    #         predicted_noise = self.model_loop(\n",
    "    #             x, torch.tensor([t]).to(self.device), start_time\n",
    "    #         )\n",
    "    #         predicted_noise = predicted_noise * imputation_mask\n",
    "\n",
    "    #         coeff1 = 1 / self.alpha_hat[t] ** 0.5\n",
    "    #         coeff2 = (1 - self.alpha_hat[t]) / (1 - self.alpha[t]) ** 0.5\n",
    "\n",
    "    #         x = x.squeeze(3)\n",
    "    #         x = coeff1 * (x - coeff2 * predicted_noise)\n",
    "\n",
    "    #         if t > 0:\n",
    "    #             noise = torch.randn_like(x)\n",
    "    #             sigma = (\n",
    "    #                 (1.0 - self.alpha[t - 1]) / (1.0 - self.alpha[t]) * self.beta[t]\n",
    "    #             ) ** 0.5\n",
    "    #             x += sigma * noise\n",
    "\n",
    "    #         x = data_2.squeeze(3) * (1 - imputation_mask) + x * imputation_mask\n",
    "\n",
    "    #         imputed_samples = x\n",
    "\n",
    "    #     return (imputed_samples, data, imputation_mask)\n",
    "\n",
    "    def eval(\n",
    "        self,\n",
    "        data,\n",
    "        imputation_mask,\n",
    "        mean,\n",
    "        std,\n",
    "        start_time=0,\n",
    "        scale=1,\n",
    "        verbose=True,\n",
    "        show_max_diff=False,\n",
    "        show_rmse=False,\n",
    "    ):\n",
    "\n",
    "        conditional_data = data * (1 - imputation_mask)\n",
    "        random_noise = torch.randn_like(data) * imputation_mask * scale\n",
    "        data_2 = conditional_data + random_noise  # .unsqueeze(3)\n",
    "\n",
    "        b, ti, f = data_2.shape\n",
    "        print(f\"b = {b}\")\n",
    "        imputed_samples = torch.zeros((b, ti, f)).to(self.device)\n",
    "        x = conditional_data + random_noise\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for t in range(self.diffusion_steps - 1, -1, -1):\n",
    "\n",
    "                x = x  # .unsqueeze(3).float()\n",
    "                predicted_noise = self.model_loop(\n",
    "                    x, torch.full((b,), t).to(self.device), start_time\n",
    "                )\n",
    "                predicted_noise = predicted_noise * imputation_mask\n",
    "\n",
    "                coeff1 = 1 / self.alpha_hat[t] ** 0.5\n",
    "                coeff2 = (1 - self.alpha_hat[t]) / (1 - self.alpha[t]) ** 0.5\n",
    "\n",
    "                x = x  # .squeeze(3)\n",
    "                x = coeff1 * (x - coeff2 * predicted_noise)\n",
    "\n",
    "                if t > 0:\n",
    "                    noise = torch.randn_like(x)\n",
    "                    sigma = (\n",
    "                        (1.0 - self.alpha[t - 1]) / (1.0 - self.alpha[t]) * self.beta[t]\n",
    "                    ) ** 0.5\n",
    "                    x += sigma * noise\n",
    "\n",
    "                # x = data_2.squeeze(3) * (1 - imputation_mask) + x * imputation_mask\n",
    "                x = data_2 * (1 - imputation_mask) + x * imputation_mask\n",
    "\n",
    "            imputed_samples = x.detach()\n",
    "\n",
    "        if show_max_diff == True:\n",
    "            # show the data at torch.max(torch.abs(data[imputation_mask !=0] - imputed_samples[imputation_mask !=0]))\n",
    "            print(\n",
    "                \"max difference = \",\n",
    "                torch.max(\n",
    "                    torch.abs(\n",
    "                        data[imputation_mask != 0]\n",
    "                        - imputed_samples[imputation_mask != 0]\n",
    "                    )\n",
    "                ).item(),\n",
    "            )\n",
    "            print(\n",
    "                \"data at max difference = \",\n",
    "                data[imputation_mask != 0][\n",
    "                    torch.argmax(\n",
    "                        torch.abs(\n",
    "                            data[imputation_mask != 0]\n",
    "                            - imputed_samples[imputation_mask != 0]\n",
    "                        )\n",
    "                    )\n",
    "                ].item(),\n",
    "            )\n",
    "            print(\n",
    "                \"imputed at max difference = \",\n",
    "                imputed_samples[imputation_mask != 0][\n",
    "                    torch.argmax(\n",
    "                        torch.abs(\n",
    "                            data[imputation_mask != 0]\n",
    "                            - imputed_samples[imputation_mask != 0]\n",
    "                        )\n",
    "                    )\n",
    "                ].item(),\n",
    "            )\n",
    "\n",
    "        mae = torch.mean(\n",
    "            torch.abs(\n",
    "                data[imputation_mask != 0] - imputed_samples[imputation_mask != 0]\n",
    "            )\n",
    "        ).item()\n",
    "        if verbose == True:\n",
    "            print(\"mae = \", mae)\n",
    "\n",
    "        if show_rmse == True:\n",
    "            # descale the data\n",
    "            imputed_samples_copy = imputed_samples.detach().clone()\n",
    "            imputed_samples_copy = imputed_samples_copy * std + mean\n",
    "            data_copy = data.detach().clone()\n",
    "            data_copy = data_copy * std + mean\n",
    "            rmse = torch.sqrt(\n",
    "                torch.mean(\n",
    "                    (\n",
    "                        data_copy[imputation_mask != 0]\n",
    "                        - imputed_samples_copy[imputation_mask != 0]\n",
    "                    )\n",
    "                    ** 2\n",
    "                )\n",
    "            ).item()\n",
    "            rmse = rmse / 1150 * 100\n",
    "            print(\"rmse = \", rmse)\n",
    "        # data_to_print = data[imputation_mask !=0]\n",
    "        # imputed_samples_to_print = imputed_samples[imputation_mask !=0]\n",
    "        # print(\"data:\", data_to_print)\n",
    "        # print(\"imputed:\", imputed_samples_to_print)\n",
    "        # print(\"absolute difference in the first 100 : \", torch.abs(data_to_print - imputed_samples_to_print)[:100])\n",
    "        # print(\"mae = \", torch.mean(torch.abs(data_to_print - imputed_samples_to_print)).item())\n",
    "\n",
    "        return (imputed_samples, data, imputation_mask, mae)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "%matplotlib inline\n",
    "from IPython.display import display, clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import statistics\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "\n",
    "def train(model, data_loader, data_loader_validation, epochs, lr, loss_func, batch_embedder = None, gradient_clip = 1.0,\n",
    "          windowed_mode=False, window_mode=\"uniform\", window_start_mode=\"random\", min_window=50, max_window=100, neg_bin_p=0.95, train_on_all_every=4,\n",
    "          annealing_mode = False, annealing_window=5, annealing_multiplier=1.25, annealing_ratio = 0.5, annealing_minimum = 1e-6,\n",
    "          device=\"cuda\", num_gpus=1, verbose=False, plot_every=10,\n",
    "          validation_frequency=1, validation_prp=10, moving_avg_window=10):\n",
    "\n",
    "    # Check for GPU availability\n",
    "    available_gpus = torch.cuda.device_count()\n",
    "    if available_gpus < num_gpus:\n",
    "        print(f\"Requested {num_gpus} GPUs, but only {available_gpus} are available.\")\n",
    "        num_gpus = available_gpus\n",
    "    else:\n",
    "        print(f\"Using {num_gpus} GPUs for training.\")\n",
    "        #also print gpu model\n",
    "        print(f\"GPU model: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "    if num_gpus > 1:\n",
    "        model = torch.nn.DataParallel(model, device_ids=list(range(num_gpus)))\n",
    "        if batch_embedder is not None:\n",
    "            batch_embedder = torch.nn.DataParallel(batch_embedder, device_ids=list(range(num_gpus)))\n",
    "\n",
    "    if batch_embedder is not None:\n",
    "        batch_embedder = batch_embedder.to(device)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    if batch_embedder is not None:\n",
    "        optimizer = torch.optim.Adam(\n",
    "            chain(batch_embedder.parameters(), model.parameters()),\n",
    "            lr=lr\n",
    "        )\n",
    "    else:\n",
    "        optimizer = torch.optim.Adam(\n",
    "            model.parameters(),\n",
    "            lr=lr\n",
    "        )\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    if batch_embedder is not None:\n",
    "        batch_embedder.train()\n",
    "    \n",
    "    loss_list = []\n",
    "    initial_value = 1.0  # Initial value for equal probability\n",
    "    window_losses = torch.ones(max_window - min_window + 1, device=device) * initial_value  # Track losses for each window length\n",
    "    window_counts = torch.zeros(max_window - min_window + 1, device=device)  # Track counts for each window length\n",
    "    loss_deques = [deque(maxlen=moving_avg_window) for _ in range(max_window - min_window + 1)]  # Deques for moving average\n",
    "    if windowed_mode and window_mode == \"biased_loss\" or window_mode == \"balanced_counts\":\n",
    "        fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(10, 12))\n",
    "    else:\n",
    "        fig, ax1 = plt.subplots(1, 1, figsize=(10, 6))\n",
    "    epoch_loss_list = []\n",
    "    val_loss = 0\n",
    "\n",
    "    total_time_start = time.time()\n",
    "    scaler = GradScaler()\n",
    "    for epoch in range(epochs):\n",
    "        # Annealing for the learning rate\n",
    "        if annealing_mode and epoch > annealing_window:\n",
    "            if len(epoch_loss_list) > 0 and epoch_loss_list[-1] >= annealing_multiplier * (statistics.mean(epoch_loss_list[-annealing_window:])):\n",
    "                for g in optimizer.param_groups:\n",
    "                    if g['lr'] * annealing_ratio < annealing_minimum:\n",
    "                        g['lr'] = annealing_minimum\n",
    "                    else:\n",
    "                        g['lr'] *= annealing_ratio\n",
    "\n",
    "        start = time.time()\n",
    "        for i, batch in enumerate(data_loader):\n",
    "            \n",
    "            batch = batch.to(device, non_blocking=True)\n",
    "\n",
    "            batch_length = batch.shape[1]\n",
    "            window_length = 0\n",
    "            # Windowed mode logic\n",
    "            if windowed_mode:\n",
    "                cut_start = 0\n",
    "                if batch_length < min_window:\n",
    "                    continue\n",
    "                if window_start_mode == \"random\":\n",
    "                    cut_start = torch.randint(0, batch_length - window_length + 1, (1,)).item()\n",
    "                elif window_start_mode == \"fixed\":\n",
    "                    cut_start = 0\n",
    "                if window_mode == \"uniform\":\n",
    "                    while True:\n",
    "                        window_length = torch.randint(min_window, batch_length + 1, (1,)).item()\n",
    "                        cut_end = cut_start + window_length\n",
    "                        if min_window <= (cut_end - cut_start) <= batch_length:\n",
    "                            break\n",
    "                    batch = batch[:, cut_start:cut_end, :]\n",
    "\n",
    "                elif window_mode == \"negative_binomial\":\n",
    "                    total_count = 1\n",
    "                    probs = neg_bin_p\n",
    "                    distribution = torch.distributions.NegativeBinomial(total_count=total_count, probs=probs)\n",
    "                    while True:\n",
    "                        window_length = distribution.sample().item() + min_window\n",
    "                        cut_end = cut_start + window_length\n",
    "                        if min_window <= window_length <= batch_length:\n",
    "                            break\n",
    "                    batch = batch[:, cut_start:cut_end, :]\n",
    "\n",
    "                elif window_mode == \"biased_loss\":\n",
    "                    if torch.min(window_counts) < 2:\n",
    "                        # Use uniform distribution until each length has been used at least twice\n",
    "                        window_probs = torch.ones_like(window_losses) / len(window_losses)\n",
    "                    elif torch.sum(window_counts) % train_on_all_every == 0:\n",
    "                        window_probs = torch.ones_like(window_losses) / len(window_losses)\n",
    "                    else:\n",
    "                        # Update probabilities based on moving average of losses\n",
    "                        avg_losses = torch.tensor([np.mean(loss_deque) if len(loss_deque) > 0 else initial_value for loss_deque in loss_deques], device=device)\n",
    "                        window_probs = avg_losses / avg_losses.sum()\n",
    "                    while True:\n",
    "                        window_length = torch.multinomial(window_probs, 1).item() + min_window\n",
    "                        #check if the window length does work with the batch length\n",
    "                        if window_length > batch_length:\n",
    "                            continue\n",
    "                        cut_end = cut_start + window_length\n",
    "                        if min_window <= window_length <= batch_length:\n",
    "                            break\n",
    "                    batch = batch[:, cut_start:cut_end, :]\n",
    "                    window_counts[window_length - min_window] += 1  # Update window counts\n",
    "                \n",
    "                elif window_mode == \"balanced_counts\":\n",
    "                    if torch.min(window_counts) < 2:\n",
    "                        # Use uniform distribution until each length has been used at least twice\n",
    "                        window_probs = torch.ones_like(window_losses) / len(window_losses)\n",
    "                    else:\n",
    "                        window_probs = window_counts / window_counts.sum()\n",
    "                    while True:\n",
    "                        # window_length = torch.multinomial(window_probs, 1).item() + min_window\n",
    "                        #let's just use the window length with the least counts\n",
    "                        window_length = torch.argmin(window_counts) + min_window\n",
    "                        cut_end = cut_start + window_length\n",
    "                        if window_length < min_window:\n",
    "                            continue\n",
    "                        else:\n",
    "                            break\n",
    "\n",
    "            if window_mode == \"balanced_counts\":\n",
    "                if batch_length < window_length:\n",
    "                    continue\n",
    "                else:\n",
    "                    batch = batch[:, cut_start:cut_end, :]\n",
    "                    window_counts[window_length - min_window] += 1\n",
    "                \n",
    "            \n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with autocast(device_type='cuda'):\n",
    "                if batch_embedder is not None:\n",
    "                    batch = batch_embedder(batch)\n",
    "\n",
    "                if cut_start > 0:\n",
    "                    predicted_noise, noise, noise_mask = model(batch, start_time=cut_start)\n",
    "                else:\n",
    "                    predicted_noise, noise, noise_mask = model(batch)\n",
    "\n",
    "                loss = loss_func(predicted_noise, noise, noise_mask)\n",
    "            \n",
    "            # loss.backward()\n",
    "            scaler.scale(loss).backward()\n",
    "            # # Gradient clipping\n",
    "            if gradient_clip is not None:\n",
    "                scaler.unscale_(optimizer)\n",
    "                max_grad_norm = gradient_clip\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "                if batch_embedder is not None:\n",
    "                    torch.nn.utils.clip_grad_norm_(batch_embedder.parameters(), max_grad_norm)\n",
    "            # optimizer.step()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            loss_list.append(loss.item())\n",
    "\n",
    "            epoch_loss = sum(loss_list[-len(data_loader):]) / len(data_loader)\n",
    "            epoch_loss_list.append(epoch_loss)\n",
    "            \n",
    "            # Update window losses and moving average deque\n",
    "            if windowed_mode and window_mode == \"biased_loss\" or window_mode == \"balanced_counts\":\n",
    "                window_idx = window_length - min_window\n",
    "                window_losses[window_idx] += loss.item()\n",
    "                loss_deques[window_idx].append(loss.item())\n",
    "\n",
    "            # Dynamic plot update\n",
    "            if i % plot_every == 0:\n",
    "                ax1.clear()\n",
    "                ax1.set_ylim(0, 1)\n",
    "                ax1.plot(loss_list)\n",
    "                if len(loss_list) > 100:\n",
    "                    ax1.plot(np.convolve(loss_list, np.ones((100,))/100, mode='valid'))\n",
    "                    ax1.text(len(loss_list) - 1, np.convolve(loss_list, np.ones((100,))/100, mode='valid')[-1],\n",
    "                            str(round(np.convolve(loss_list, np.ones((100,))/100, mode='valid')[-1], 3)))\n",
    "                if len(epoch_loss_list) > 0:\n",
    "                    ax1.text(0.1, 0.9, f\"Epoch: {epoch} | Learning rate: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "                # ax1.text(0.1, 0.8, f\"Learning rate: {optimizer.param_groups[0]['lr']:.4e}\")\n",
    "                ax1.text(0.1, 0.8, f\"Loss: {epoch_loss_list[-1]:.3e} | Validation loss: {val_loss:.3e}\")\n",
    "                ax1.text(0.1, 0.7, f\"Time per step: {((time.time() - start) / (i + 1)):.2f} s | Time per epoch: {((time.time() - start) / (i + 1) * len(data_loader)):.2f} s\")\n",
    "                ax1.text(0.1, 0.6, f\"Time till finish (est.): {((time.time() - start) / (i + 1) * len(data_loader) * (epochs - epoch)) / 60:.2f} min\")\n",
    "                if windowed_mode and window_mode == \"biased_loss\" or window_mode == \"balanced_counts\":\n",
    "                    ax2.clear()\n",
    "                    ax2.bar(range(min_window, max_window + 1), window_counts.cpu().numpy())\n",
    "                    ax2.set_ylabel(\"Counts\")\n",
    "                    ax2.set_title(\"Counts of Each Window Length Used\")\n",
    "\n",
    "                    moving_avg_losses = torch.tensor([np.mean(loss_deque) if len(loss_deque) > 0 else initial_value for loss_deque in loss_deques], device=device).cpu().numpy()\n",
    "                    ax3.clear()\n",
    "                    ax3.bar(range(min_window, max_window + 1), moving_avg_losses)\n",
    "                    ax3.set_xlabel(\"Window Length\")\n",
    "                    ax3.set_ylabel(\"Moving Average Loss\")\n",
    "                    ax3.set_title(\"Moving Average Loss for Each Window Length\")\n",
    "\n",
    "                display(fig)\n",
    "                clear_output(wait=True)\n",
    "\n",
    "        end = time.time()\n",
    "\n",
    "        # Validation\n",
    "        if epoch % validation_frequency == 0:\n",
    "            loss_list_validation = []\n",
    "            for i, batch in enumerate(data_loader_validation):\n",
    "                batch = batch.to(device, non_blocking=True)\n",
    "                if batch_embedder is not None:\n",
    "                    batch = batch_embedder(batch)\n",
    "                if i % validation_prp == 0:\n",
    "                    with autocast(device_type='cuda'):\n",
    "                        predicted_noise, noise, noise_mask = model(batch)\n",
    "                        loss = loss_func(predicted_noise, noise, noise_mask)\n",
    "                        loss_list_validation.append(loss.item())\n",
    "\n",
    "            val_loss = np.mean(loss_list_validation)\n",
    "            \n",
    "            model.train()\n",
    "            if batch_embedder is not None:\n",
    "                batch_embedder.train()\n",
    "\n",
    "        total_time_end = time.time()\n",
    "        total_time = total_time_end - total_time_start\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Epoch {epoch} completed in {end - start} seconds, Loss: {epoch_loss}\")\n",
    "            print(f\"Validation Loss: {val_loss}\")\n",
    "    \n",
    "    print(f\"Took {total_time} seconds for {epoch} epochs.\")\n",
    "            \n",
    "\n",
    "\n",
    "    return model, loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Sampler\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class CustomTimeSeriesDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, data_tensor, sequence_lengths, min_seq_length=None, max_seq_length=None\n",
    "    ):\n",
    "        # Store the initial sequences and lengths\n",
    "        self.data_tensor = data_tensor\n",
    "        self.sequence_lengths = sequence_lengths\n",
    "        self.min_seq_length = min_seq_length\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "        # Filter sequences based on min and max sequence length\n",
    "        if self.min_seq_length is not None:\n",
    "            valid_indices = [\n",
    "                i\n",
    "                for i, length in enumerate(self.sequence_lengths)\n",
    "                if length >= self.min_seq_length\n",
    "            ]\n",
    "        else:\n",
    "            valid_indices = list(range(len(self.sequence_lengths)))\n",
    "\n",
    "        if self.max_seq_length is not None:\n",
    "            valid_indices = [\n",
    "                i\n",
    "                for i in valid_indices\n",
    "                if self.sequence_lengths[i] <= self.max_seq_length\n",
    "            ]\n",
    "\n",
    "        self.data_tensor = self.data_tensor[valid_indices]\n",
    "        self.sequence_lengths = [self.sequence_lengths[i] for i in valid_indices]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequence_lengths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq_length = int(self.sequence_lengths[idx])\n",
    "        return self.data_tensor[idx, :seq_length, :], seq_length\n",
    "\n",
    "\n",
    "# class CustomTimeSeriesDataset(Dataset):\n",
    "#     def __init__(self, data_tensor, sequence_lengths, min_seq_length=None, max_seq_length=None):\n",
    "#         self.data_tensor = data_tensor\n",
    "#         self.sequence_lengths = sequence_lengths\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.sequence_lengths)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         seq_length = int(self.sequence_lengths[idx])\n",
    "#         return self.data_tensor[idx, :seq_length, :], seq_length\n",
    "\n",
    "\n",
    "class LengthBatchSampler(Sampler):\n",
    "    def __init__(self, dataset, batch_size):\n",
    "        self.sequence_lengths = dataset.sequence_lengths\n",
    "        self.batch_size = batch_size\n",
    "        self.batches = self._create_batches()\n",
    "\n",
    "    def _create_batches(self):\n",
    "        length_to_indices = defaultdict(list)\n",
    "        for idx, length in enumerate(self.sequence_lengths):\n",
    "            length_to_indices[length].append(idx)\n",
    "\n",
    "        batches = []\n",
    "        for length, indices in length_to_indices.items():\n",
    "            # Split indices into batches of the specified batch size\n",
    "            for i in range(0, len(indices), self.batch_size):\n",
    "                batches.append(indices[i : i + self.batch_size])\n",
    "        return batches\n",
    "\n",
    "    def __iter__(self):\n",
    "        for batch in self.batches:\n",
    "            yield batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.batches)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    data, lengths = zip(*batch)\n",
    "    data = torch.stack(data)\n",
    "    return data\n",
    "\n",
    "\n",
    "def create_dataloader(\n",
    "    data_tensor, sequence_lengths, batch_size, min_seq_length=None, max_seq_length=None\n",
    "):\n",
    "    dataset = CustomTimeSeriesDataset(\n",
    "        data_tensor, sequence_lengths, min_seq_length, max_seq_length\n",
    "    )\n",
    "    sampler = LengthBatchSampler(dataset, batch_size)\n",
    "    dataloader = DataLoader(dataset, batch_sampler=sampler, collate_fn=collate_fn)\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "class BalancedLengthBatchSampler(Sampler):\n",
    "    def __init__(self, dataset, batch_size, balance_factor=1.0):\n",
    "        self.sequence_lengths = dataset.sequence_lengths\n",
    "        self.batch_size = batch_size\n",
    "        self.balance_factor = balance_factor\n",
    "        self.batches = self._create_balanced_batches()\n",
    "\n",
    "    def _create_balanced_batches(self):\n",
    "        # Group indices by sequence length\n",
    "        length_to_indices = defaultdict(list)\n",
    "        for idx, length in enumerate(self.sequence_lengths):\n",
    "            length_to_indices[length].append(idx)\n",
    "\n",
    "        # Calculate the maximum count of indices for balancing\n",
    "        max_count = max(len(indices) for indices in length_to_indices.values())\n",
    "\n",
    "        # Balance the distribution of sequence lengths by oversampling shorter sequences\n",
    "        balanced_batches = []\n",
    "        for length, indices in length_to_indices.items():\n",
    "            count = len(indices)\n",
    "            if count < max_count:\n",
    "                repeat_factor = int(self.balance_factor * (max_count / count))\n",
    "                # Correctly oversample the list elements\n",
    "                oversampled_indices = indices * repeat_factor\n",
    "                # oversampled_indices = oversampled_indices[:max_count]\n",
    "            else:\n",
    "                oversampled_indices = indices\n",
    "\n",
    "            # Shuffle the indices of this particular length\n",
    "            random.shuffle(oversampled_indices)\n",
    "\n",
    "            # Create batches for this length\n",
    "            for i in range(0, len(oversampled_indices), self.batch_size):\n",
    "                batch = oversampled_indices[i : i + self.batch_size]\n",
    "                if len(batch) == self.batch_size:\n",
    "                    balanced_batches.append(batch)\n",
    "\n",
    "        # Shuffle the list of balanced batches to ensure random order\n",
    "        random.shuffle(balanced_batches)\n",
    "\n",
    "        return balanced_batches\n",
    "\n",
    "    def __iter__(self):\n",
    "        for batch in self.batches:\n",
    "            yield batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.batches)\n",
    "\n",
    "\n",
    "def create_balanced_dataloader(\n",
    "    data_tensor,\n",
    "    sequence_lengths,\n",
    "    batch_size,\n",
    "    balance_factor=1.0,\n",
    "    min_seq_length=None,\n",
    "    max_seq_length=None,\n",
    "):\n",
    "    dataset = CustomTimeSeriesDataset(\n",
    "        data_tensor, sequence_lengths, min_seq_length, max_seq_length\n",
    "    )\n",
    "    sampler = BalancedLengthBatchSampler(dataset, batch_size, balance_factor)\n",
    "    dataloader = DataLoader(dataset, batch_sampler=sampler, collate_fn=collate_fn)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = create_balanced_dataloader(\n",
    "    training_data_tensor,\n",
    "    training_data_sequence_lengths,\n",
    "    batch_size=200,\n",
    "    balance_factor=1.0,\n",
    "    min_seq_length=2,\n",
    "    max_seq_length=None,\n",
    ")\n",
    "\n",
    "val_loader = create_balanced_dataloader(\n",
    "    validation_data_tensor,\n",
    "    validation_data_sequence_lengths,\n",
    "    batch_size=100,\n",
    "    balance_factor=1.0,\n",
    "    min_seq_length=2,\n",
    "    max_seq_length=None,\n",
    ")\n",
    "\n",
    "# check the size of the train, val, and test sets\n",
    "print(len(train_loader))\n",
    "print(len(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical_indices_sizes = {\n",
    "#     # 'time_step' : [0, 1],\n",
    "#     \"chemo_application_prev\": [0, 1],\n",
    "#     \"radio_application_prev\": [1, 1],\n",
    "#     \"patient_type_tile\": [2, 1],\n",
    "# }\n",
    "\n",
    "# numerical_indices = {\"cancer_volume\": 3}\n",
    "\n",
    "# numerical_indices = {\n",
    "#     'chemo_application_prev': 0,\n",
    "#     'radio_application_prev': 1,\n",
    "#     'patient_type_tile': 2,\n",
    "#     'cancer_volume': 3\n",
    "# }\n",
    "\n",
    "\n",
    "# training_data_tensor_embedded = data_embedder(training_data_tensor)\n",
    "# validation_data_tensor_embedded = data_embedder(validation_data_tensor)\n",
    "# test_data_factuals_tensor_embedded = data_embedder(test_data_factuals_tensor)\n",
    "# test_data_counterfactuals_tensor_embedded = data_embedder(test_data_counterfactuals_tensor)\n",
    "# test_data_seq_tensor_embedded = data_embedder(test_data_seq_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion_imputer = diffusion_imputation(\n",
    "    emb_dim=128,\n",
    "    # strategy=\"forecasting_last_n_time\",\n",
    "    strategy=\"random\",\n",
    "    missing_prp=0.3,\n",
    "    # strategy='selected_features',\n",
    "    # strategy=\"selected_features_last_n_time\",\n",
    "    # last_n_time=1,\n",
    "    # features_to_impute=[0, 1, 2, 3],\n",
    "    # excluded_features = [i for i in range(6)], #[2],#[0,1,2,3,5], #for the embedded stock names which we don't need to predict\n",
    "    # strategy='selected_features_and_selected_features_after_time',\n",
    "    # features_to_impute_completely=[2],\n",
    "    # features_to_impute_after_time=[3],\n",
    "    num_residual_layers=2,\n",
    "    diffusion_steps=50,\n",
    "    diffusion_beta_schedule=\"quadratic\",\n",
    "    num_heads=8,\n",
    "    kernel_size=(1, 1),\n",
    "    ff_dim=4096,\n",
    "    num_cells=1,\n",
    "    dropout=0,\n",
    "    # csdi, csdi_moded_transformer, rsa, rsa_moded_transformer, moded_transformer_alone, rsa_csdi\n",
    "    method=\"moded_transformer_alone\",\n",
    "    device=\"cuda\",\n",
    ")\n",
    "\n",
    "# data_embedder = DataEmbedder(\n",
    "#     categorical_indices_sizes, numerical_indices, training_data_tensor\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.deterministic = False\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\n",
    "    diffusion_imputer,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    num_gpus=1,\n",
    "    batch_embedder=None,\n",
    "    gradient_clip=None,\n",
    "    windowed_mode=True,\n",
    "    window_mode=\"balanced_counts\",\n",
    "    # window_mode=\"biased_loss\",\n",
    "    window_start_mode=\"fixed\",\n",
    "    # train_on_all_every=100,\n",
    "    min_window=2,\n",
    "    max_window=58,\n",
    "    device=\"cuda\",\n",
    "    epochs=50,\n",
    "    lr=1e-3,\n",
    "    annealing_mode=True,\n",
    "    annealing_window=2,\n",
    "    annealing_multiplier=1.0,\n",
    "    annealing_ratio=0.2,\n",
    "    annealing_minimum=1e-5,\n",
    "    loss_func=diffusion_imputer.loss_func,\n",
    "    validation_frequency=2,\n",
    "    validation_prp=1,\n",
    "    verbose=False,\n",
    "    plot_every=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_hybrid(\n",
    "#     diffusion_imputer,\n",
    "#     hybrid_model,\n",
    "#     train_loader,\n",
    "#     val_loader,\n",
    "#     batch_embedder = embedder,\n",
    "#     epochs = 20,\n",
    "#     lr = 0.001,\n",
    "#     annealing_window = 5,\n",
    "#     annealing_multiplier = 1,\n",
    "#     loss_func = diffusion_imputer.loss_func,\n",
    "#     hybrid_loss_func = hybrid_model.loss_func,\n",
    "#     hybrid_start_epoch = 0,\n",
    "#     hybrid_every_n_epoch = 5,\n",
    "#     validation_frequency=2,\n",
    "#     validation_prp=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save the entire model for further training\n",
    "torch.save(diffusion_imputer, \"diffusion_imputer_cancer_aug15_2.pt\")\n",
    "# torch.save(data_embedder, \"data_embedder_cancer_aug9.pt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def calculate_quantiles(imputed_samples_tensor, qlist):\n",
    "    quantiles_imp = []\n",
    "    for q in qlist:\n",
    "        quantiles = torch.quantile(\n",
    "            imputed_samples_tensor, q, dim=0, interpolation=\"midpoint\"\n",
    "        )\n",
    "        quantiles_imp.append(quantiles)\n",
    "    return torch.stack(quantiles_imp, dim=0)\n",
    "\n",
    "\n",
    "# Calculate RMSE with denormalization\n",
    "\n",
    "\n",
    "def calculate_rmse(\n",
    "    final_samples, training_mean, training_std, qlist=[0.05, 0.25, 0.50, 0.75, 0.95]\n",
    "):\n",
    "    all_data = []\n",
    "    all_imputed = []\n",
    "    imputed_samples_list = []\n",
    "    actual_data_list = []\n",
    "\n",
    "    # Extract one instance of the actual data and imputation mask\n",
    "    data_instance, imputation_mask_instance = None, None\n",
    "    sample_number = len(final_samples)\n",
    "\n",
    "    for sample in final_samples:\n",
    "        for imputed_samples, data, imputation_mask, _ in sample:\n",
    "            # Store imputed samples for quantile calculation\n",
    "            imputed_samples_list.append(imputed_samples[imputation_mask != 0].cpu())\n",
    "            actual_data_list.append(data[imputation_mask != 0].cpu())\n",
    "            # Flatten the tensors and filter non-zero mask areas\n",
    "            all_data.append(data[imputation_mask != 0].cpu())\n",
    "            all_imputed.append(imputed_samples[imputation_mask != 0].cpu())\n",
    "\n",
    "    # Concatenate all filtered elements\n",
    "    all_data = torch.cat(all_data)\n",
    "    all_imputed = torch.cat(all_imputed)\n",
    "\n",
    "    # Concatenate all imputed samples and reshape to original dimensions\n",
    "    imputed_samples_tensor = torch.cat(imputed_samples_list).reshape(sample_number, -1)\n",
    "\n",
    "    # Concatenate all actual data and reshape to original dimensions\n",
    "    actual_data_tensor = torch.cat(actual_data_list).reshape(sample_number, -1)\n",
    "    actual_data_tensor = actual_data_tensor[0]\n",
    "\n",
    "    # Denormalize the data\n",
    "    all_data = all_data * training_std + training_mean\n",
    "    all_imputed = all_imputed * training_std + training_mean\n",
    "\n",
    "    # Calculate RMSE for all imputed values\n",
    "    rmse = torch.sqrt(torch.mean((all_data - all_imputed) ** 2)).item()\n",
    "    rmse = rmse / 1150 * 100  # Adjust the RMSE as per your requirement\n",
    "\n",
    "    # Calculate quantiles\n",
    "    quantiles_imp = calculate_quantiles(imputed_samples_tensor, qlist)\n",
    "    medians = quantiles_imp[qlist.index(0.50)]  # Median is the 50th percentile\n",
    "\n",
    "    # Denormalize the medians\n",
    "    medians = medians * training_std + training_mean\n",
    "\n",
    "    # Use the median for the new RMSE calculation\n",
    "    actual_data_tensor = actual_data_tensor * training_std + training_mean\n",
    "\n",
    "    # Calculate RMSE with medians\n",
    "    rmse_median = torch.sqrt(torch.mean((actual_data_tensor - medians) ** 2)).item()\n",
    "    # Adjust the RMSE as per your requirement\n",
    "    rmse_median = rmse_median / 1150 * 100\n",
    "\n",
    "    return rmse, rmse_median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_multiple_evaluations(\n",
    "#     dataloader,\n",
    "#     imputer,\n",
    "#     training_mean,\n",
    "#     training_std,\n",
    "#     sample_number,\n",
    "#     batch_embedder=None,\n",
    "#     old_sample=[],\n",
    "#     min_sequence_len=2,\n",
    "#     max_sequence_len=None,\n",
    "#     scale=1,\n",
    "#     verbose=True,\n",
    "#     show_max_diff=False,\n",
    "#     show_rmse=False,\n",
    "# ):\n",
    "#     final_samples = old_sample\n",
    "#     max_seq_len = 0\n",
    "#     total_batches = len(dataloader) * sample_number\n",
    "#     completed_batches = 0\n",
    "#     sample_time = []\n",
    "#     average_sample_time = 0\n",
    "\n",
    "#     for i in range(sample_number):\n",
    "#         sample_start = time.time()\n",
    "#         # print a line to separate the samples\n",
    "#         print(\"-------------------------------------------------\")\n",
    "#         print(f\"Running sample {i + 1}/{sample_number}\")\n",
    "#         all_samples = []\n",
    "#         for batch_idx, data_batch in enumerate(dataloader):\n",
    "#             # Get the data from the batch (collate_fn returns a tuple)\n",
    "#             data_batch = data_batch.to(imputer.device)\n",
    "#             if batch_embedder is not None:\n",
    "#                 data_batch = batch_embedder(data_batch)\n",
    "#             seq_length = data_batch.shape[1]\n",
    "#             if seq_length < min_sequence_len:\n",
    "#                 # print(f\"Skipping batch {batch_idx + 1} as sequence length is less than {min_sequence_len}\")\n",
    "#                 completed_batches += 1\n",
    "#                 continue\n",
    "#             if max_sequence_len is not None and seq_length > max_sequence_len:\n",
    "#                 # print(f\"Skipping batch {batch_idx + 1} as sequence length is greater than {max_sequence_len}\")\n",
    "#                 completed_batches += 1\n",
    "#                 continue\n",
    "\n",
    "#             print(f\"sequence length: {seq_length}\")\n",
    "\n",
    "#             # Generate imputation masks for the current batch\n",
    "#             imputation_masks = imputer.get_mask(\n",
    "#                 data_batch, strategy=\"selected_features_last_n_time\"\n",
    "#             ).to(imputer.device)\n",
    "#             imputed_samples = imputer.eval(\n",
    "#                 data_batch,\n",
    "#                 imputation_masks,\n",
    "#                 start_time=0,\n",
    "#                 mean=training_mean,\n",
    "#                 std=training_std,\n",
    "#                 scale=scale,\n",
    "#                 verbose=verbose,\n",
    "#                 show_max_diff=show_max_diff,\n",
    "#                 show_rmse=show_rmse,\n",
    "#             )\n",
    "\n",
    "#             all_samples.append(imputed_samples)\n",
    "\n",
    "#             completed_batches += 1\n",
    "#             progress = completed_batches / total_batches\n",
    "#             print(f\"Overall Progress: {progress * 100:.2f}%\")\n",
    "#             print(\n",
    "#                 f\"Time to finish (est.): {average_sample_time * (sample_number - i - 1) / 60:.2f} min\"\n",
    "#             )\n",
    "\n",
    "#         sample_end = time.time()\n",
    "#         sample_time.append(sample_end - sample_start)\n",
    "#         average_sample_time = sum(sample_time) / len(sample_time)\n",
    "#         final_samples.append(all_samples)\n",
    "\n",
    "#         rmse, rmse_median = calculate_rmse(final_samples, training_mean, training_std)\n",
    "#         print(f\"RMSE: {rmse:.3f} | RMSE (Median): {rmse_median:.3f}\")\n",
    "\n",
    "#     return final_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from turtle import st\n",
    "\n",
    "\n",
    "# def run_multiple_evaluations(\n",
    "#     dataloader,\n",
    "#     imputer,\n",
    "#     training_mean,\n",
    "#     training_std,\n",
    "#     sample_number,\n",
    "#     data_embedder=None,\n",
    "#     old_sample=[],\n",
    "#     min_sequence_len=2,\n",
    "#     max_sequence_len=None,\n",
    "#     scale=1,\n",
    "#     num_gpus=1,\n",
    "#     verbose=True,\n",
    "#     show_max_diff=False,\n",
    "#     show_rmse=False,\n",
    "# ):\n",
    "#     final_samples = old_sample\n",
    "#     max_seq_len = 0\n",
    "#     total_batches = len(dataloader) * sample_number\n",
    "#     completed_batches = 0\n",
    "#     sample_time = []\n",
    "#     average_sample_time = 0\n",
    "\n",
    "#     # Limit the number of GPUs to available GPUs and the specified number\n",
    "#     num_gpus = min(num_gpus, torch.cuda.device_count())\n",
    "#     if num_gpus > 1:\n",
    "#         # Wrap the imputer with DataParallel\n",
    "#         imputer = torch.nn.DataParallel(imputer, device_ids=list(range(num_gpus)))\n",
    "\n",
    "#         if data_embedder is not None:\n",
    "#             data_embedder = torch.nn.DataParallel(\n",
    "#                 data_embedder, device_ids=list(range(num_gpus))\n",
    "#             )\n",
    "\n",
    "#     print(f\"Using {num_gpus} GPUs for evaluation.\")\n",
    "\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     imputer = imputer.to(device)\n",
    "#     if data_embedder is not None:\n",
    "#         data_embedder = data_embedder.to(device)\n",
    "\n",
    "#     for i in range(sample_number):\n",
    "#         sample_start = time.time()\n",
    "#         print(\"-------------------------------------------------\")\n",
    "#         print(f\"Running sample {i + 1}/{sample_number}\")\n",
    "#         all_samples = []\n",
    "\n",
    "#         for batch_idx, data_batch in enumerate(dataloader):\n",
    "#             # Get the data from the batch (collate_fn returns a tuple)\n",
    "#             data_batch = data_batch.to(device)\n",
    "#             if data_embedder is not None:\n",
    "#                 data_batch = data_embedder(data_batch)\n",
    "#             seq_length = data_batch.shape[1]\n",
    "#             if seq_length < min_sequence_len:\n",
    "#                 completed_batches += 1\n",
    "#                 continue\n",
    "#             if max_sequence_len is not None and seq_length > max_sequence_len:\n",
    "#                 completed_batches += 1\n",
    "#                 continue\n",
    "\n",
    "#             print(f\"sequence length: {seq_length}\")\n",
    "\n",
    "#             # Generate imputation masks for the current batch\n",
    "#             if num_gpus > 1:\n",
    "#                 imputation_masks = imputer.module.get_mask(\n",
    "#                     data_batch, strategy=\"selected_features_last_n_time\"\n",
    "#                 ).to(device)\n",
    "#             else:\n",
    "#                 imputation_masks = imputer.get_mask(\n",
    "#                     data_batch, strategy=\"selected_features_last_n_time\"\n",
    "#                 ).to(device)\n",
    "\n",
    "#             if num_gpus > 1:\n",
    "#                 imputed_samples = imputer.module.eval(\n",
    "#                     data_batch,\n",
    "#                     imputation_masks,\n",
    "#                     start_time=0,\n",
    "#                     mean=training_mean,\n",
    "#                     std=training_std,\n",
    "#                     scale=scale,\n",
    "#                     verbose=verbose,\n",
    "#                     show_max_diff=show_max_diff,\n",
    "#                     show_rmse=show_rmse,\n",
    "#                 )\n",
    "#             else:\n",
    "#                 imputed_samples = imputer.eval(\n",
    "#                     data_batch,\n",
    "#                     imputation_masks,\n",
    "#                     start_time=0,\n",
    "#                     mean=training_mean,\n",
    "#                     std=training_std,\n",
    "#                     scale=scale,\n",
    "#                     verbose=verbose,\n",
    "#                     show_max_diff=show_max_diff,\n",
    "#                     show_rmse=show_rmse,\n",
    "#                 )\n",
    "\n",
    "#             all_samples.append(imputed_samples)\n",
    "\n",
    "#             completed_batches += 1\n",
    "#             progress = completed_batches / total_batches\n",
    "#             print(f\"Overall Progress: {progress * 100:.2f}%\")\n",
    "#             print(\n",
    "#                 f\"Time to finish (est.): {average_sample_time * (sample_number - i - 1) / 60:.2f} min\"\n",
    "#             )\n",
    "\n",
    "#         sample_end = time.time()\n",
    "#         sample_time.append(sample_end - sample_start)\n",
    "#         average_sample_time = sum(sample_time) / len(sample_time)\n",
    "#         final_samples.append(all_samples)\n",
    "\n",
    "#         rmse, rmse_median = calculate_rmse(final_samples, training_mean, training_std)\n",
    "#         print(f\"RMSE: {rmse:.3f} | RMSE (Median): {rmse_median:.3f}\")\n",
    "\n",
    "#     return final_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "\n",
    "def run_multiple_evaluations_with_repeats(\n",
    "    dataloader,\n",
    "    imputer,\n",
    "    training_mean,\n",
    "    training_std,\n",
    "    sample_number,\n",
    "    batch_size,\n",
    "    data_embedder=None,\n",
    "    old_sample=[],\n",
    "    min_sequence_len=2,\n",
    "    max_sequence_len=None,\n",
    "    scale=1,\n",
    "    num_gpus=1,\n",
    "    repetition_factor=2,\n",
    "    verbose=True,\n",
    "    show_max_diff=False,\n",
    "    show_rmse=False,\n",
    "):\n",
    "    final_samples = old_sample\n",
    "    max_seq_len = 0\n",
    "\n",
    "    # Determine the number of full repetitions and remainder\n",
    "    full_repetitions = sample_number // repetition_factor\n",
    "    remainder = sample_number % repetition_factor\n",
    "\n",
    "    total_batches = len(dataloader) * (full_repetitions + (1 if remainder > 0 else 0))\n",
    "    completed_batches = 0\n",
    "    sample_time = []\n",
    "    average_sample_time = 0\n",
    "\n",
    "    # Limit the number of GPUs to available GPUs and the specified number\n",
    "    num_gpus = min(num_gpus, torch.cuda.device_count())\n",
    "    if num_gpus > 1:\n",
    "        imputer = torch.nn.DataParallel(imputer, device_ids=list(range(num_gpus)))\n",
    "\n",
    "        if data_embedder is not None:\n",
    "            data_embedder = torch.nn.DataParallel(\n",
    "                data_embedder, device_ids=list(range(num_gpus))\n",
    "            )\n",
    "\n",
    "    print(f\"Using {num_gpus} GPUs for evaluation.\")\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    imputer = imputer.to(device)\n",
    "    if data_embedder is not None:\n",
    "        data_embedder = data_embedder.to(device)\n",
    "\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    all_samples = []  # Collect all samples first\n",
    "\n",
    "    for i in range(full_repetitions):\n",
    "        sample_start = time.time()\n",
    "        print(\"-------------------------------------------------\")\n",
    "        print(f\"Running sample {i + 1}/{full_repetitions}\")\n",
    "\n",
    "        for batch_idx, data_batch in enumerate(dataloader):\n",
    "            data_batch = data_batch.to(device)\n",
    "            if data_embedder is not None:\n",
    "                data_batch = data_embedder(data_batch)\n",
    "            seq_length = data_batch.shape[1]\n",
    "            if seq_length < min_sequence_len:\n",
    "                completed_batches += 1\n",
    "                continue\n",
    "            if max_sequence_len is not None and seq_length > max_sequence_len:\n",
    "                completed_batches += 1\n",
    "                continue\n",
    "\n",
    "            print(f\"Sequence length: {seq_length}\")\n",
    "\n",
    "            # Duplicate the data according to the repetition factor\n",
    "            repeated_data_batch = torch.cat([data_batch] * repetition_factor, dim=0)\n",
    "\n",
    "            if num_gpus > 1:\n",
    "                with autocast(device_type=\"cuda\"):\n",
    "                    imputation_masks = imputer.module.get_mask(\n",
    "                        repeated_data_batch, strategy=\"selected_features_last_n_time\"\n",
    "                    ).to(device)\n",
    "\n",
    "                    imputed_samples_tuple = imputer.module.eval(\n",
    "                        repeated_data_batch,\n",
    "                        imputation_masks,\n",
    "                        start_time=0,\n",
    "                        mean=training_mean,\n",
    "                        std=training_std,\n",
    "                        scale=scale,\n",
    "                        verbose=verbose,\n",
    "                        show_max_diff=show_max_diff,\n",
    "                        show_rmse=show_rmse,\n",
    "                    )\n",
    "            else:\n",
    "                imputation_masks = imputer.get_mask(\n",
    "                    repeated_data_batch, strategy=\"selected_features_last_n_time\"\n",
    "                ).to(device)\n",
    "                imputed_samples_tuple = imputer.eval(\n",
    "                    repeated_data_batch,\n",
    "                    imputation_masks,\n",
    "                    start_time=0,\n",
    "                    mean=training_mean,\n",
    "                    std=training_std,\n",
    "                    scale=scale,\n",
    "                    verbose=verbose,\n",
    "                    show_max_diff=show_max_diff,\n",
    "                    show_rmse=show_rmse,\n",
    "                )\n",
    "\n",
    "            # Collect all the parts\n",
    "            for j in range(repetition_factor):\n",
    "                part_imputed_samples = imputed_samples_tuple[0][\n",
    "                    j * batch_size : (j + 1) * batch_size\n",
    "                ]\n",
    "                part_data = imputed_samples_tuple[1][\n",
    "                    j * batch_size : (j + 1) * batch_size\n",
    "                ]\n",
    "                part_imputation_mask = imputed_samples_tuple[2][\n",
    "                    j * batch_size : (j + 1) * batch_size\n",
    "                ]\n",
    "                part_mae = imputed_samples_tuple[3]\n",
    "\n",
    "                # Append the sliced parts as a tuple\n",
    "                all_samples.append(\n",
    "                    (part_imputed_samples, part_data, part_imputation_mask, part_mae)\n",
    "                )\n",
    "\n",
    "            completed_batches += 1\n",
    "            progress = completed_batches / total_batches\n",
    "            print(f\"Overall Progress: {progress * 100:.2f}%\")\n",
    "            print(\n",
    "                f\"Time to finish (est.): {average_sample_time * (full_repetitions - i - 1) / 60:.2f} min\"\n",
    "            )\n",
    "\n",
    "        sample_end = time.time()\n",
    "        sample_time.append(sample_end - sample_start)\n",
    "        average_sample_time = sum(sample_time) / len(sample_time)\n",
    "\n",
    "    # Handle the remainder\n",
    "    if remainder > 0:\n",
    "        sample_start = time.time()\n",
    "        print(\"-------------------------------------------------\")\n",
    "        print(f\"Running final sample for the remainder {remainder}/{repetition_factor}\")\n",
    "\n",
    "        for batch_idx, data_batch in enumerate(dataloader):\n",
    "            data_batch = data_batch.to(device)\n",
    "            if data_embedder is not None:\n",
    "                data_batch = data_embedder(data_batch)\n",
    "            seq_length = data_batch.shape[1]\n",
    "            if seq_length < min_sequence_len:\n",
    "                completed_batches += 1\n",
    "                continue\n",
    "            if max_sequence_len is not None and seq_length > max_sequence_len:\n",
    "                completed_batches += 1\n",
    "                continue\n",
    "\n",
    "            print(f\"Sequence length: {seq_length}\")\n",
    "\n",
    "            # Duplicate the data according to the remainder\n",
    "            repeated_data_batch = torch.cat([data_batch] * remainder, dim=0)\n",
    "\n",
    "            if num_gpus > 1:\n",
    "                with autocast(device_type=\"cuda\"):\n",
    "                    imputation_masks = imputer.module.get_mask(\n",
    "                        repeated_data_batch, strategy=\"selected_features_last_n_time\"\n",
    "                    ).to(device)\n",
    "\n",
    "                    imputed_samples_tuple = imputer.module.eval(\n",
    "                        repeated_data_batch,\n",
    "                        imputation_masks,\n",
    "                        start_time=0,\n",
    "                        mean=training_mean,\n",
    "                        std=training_std,\n",
    "                        scale=scale,\n",
    "                        verbose=verbose,\n",
    "                        show_max_diff=show_max_diff,\n",
    "                        show_rmse=show_rmse,\n",
    "                    )\n",
    "            else:\n",
    "                imputation_masks = imputer.get_mask(\n",
    "                    repeated_data_batch, strategy=\"selected_features_last_n_time\"\n",
    "                ).to(device)\n",
    "                imputed_samples_tuple = imputer.eval(\n",
    "                    repeated_data_batch,\n",
    "                    imputation_masks,\n",
    "                    start_time=0,\n",
    "                    mean=training_mean,\n",
    "                    std=training_std,\n",
    "                    scale=scale,\n",
    "                    verbose=verbose,\n",
    "                    show_max_diff=show_max_diff,\n",
    "                    show_rmse=show_rmse,\n",
    "                )\n",
    "\n",
    "            # Collect all the parts\n",
    "            for j in range(remainder):\n",
    "                part_imputed_samples = imputed_samples_tuple[0][\n",
    "                    j * batch_size : (j + 1) * batch_size\n",
    "                ]\n",
    "                part_data = imputed_samples_tuple[1][\n",
    "                    j * batch_size : (j + 1) * batch_size\n",
    "                ]\n",
    "                part_imputation_mask = imputed_samples_tuple[2][\n",
    "                    j * batch_size : (j + 1) * batch_size\n",
    "                ]\n",
    "                part_mae = imputed_samples_tuple[3]\n",
    "\n",
    "                # Append the sliced parts as a tuple\n",
    "                all_samples.append(\n",
    "                    (part_imputed_samples, part_data, part_imputation_mask, part_mae)\n",
    "                )\n",
    "\n",
    "            completed_batches += 1\n",
    "            progress = completed_batches / total_batches\n",
    "            print(f\"Overall Progress: {progress * 100:.2f}%\")\n",
    "            print(\n",
    "                f\"Time to finish (est.): {average_sample_time * (1 - i - 1) / 60:.2f} min\"\n",
    "            )\n",
    "\n",
    "        sample_end = time.time()\n",
    "        sample_time.append(sample_end - sample_start)\n",
    "        average_sample_time = sum(sample_time) / len(sample_time)\n",
    "\n",
    "    # De-interleave all_samples to match the required sample_number\n",
    "    final_samples = [all_samples[i::repetition_factor] for i in range(sample_number)]\n",
    "\n",
    "    # Calculate RMSE for the current set of final samples\n",
    "    rmse, rmse_median = calculate_rmse(final_samples, training_mean, training_std)\n",
    "    print(f\"RMSE: {rmse:.3f} | RMSE (Median): {rmse_median:.3f}\")\n",
    "\n",
    "    return final_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diffusion_imputer = torch.load(\"diffusion_imputer_cancer_aug15.pt\")\n",
    "# data_embedder = torch.load(\"data_embedder_cancer_jul23.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "test_loader = create_dataloader(\n",
    "    test_data_counterfactuals_tensor,\n",
    "    test_data_counterfactuals_sequence_lengths,\n",
    "    batch_size=batch_size,\n",
    "    min_seq_length=2,\n",
    "    max_seq_length=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion_imputer.features_to_impute = [3]\n",
    "diffusion_imputer.last_n_time = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_standard_deviation = stds[\"cancer_volume\"]\n",
    "training_mean = means[\"cancer_volume\"]\n",
    "\n",
    "training_standard_deviation\n",
    "training_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # change working directory\n",
    "# os.chdir(\"/work/postresearch/Shared/Researchers/Farbod/cancer/\")\n",
    "# sys.path.append(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "sample_number = 10\n",
    "# final_samples = run_multiple_evaluations(\n",
    "#     dataloader=test_loader,\n",
    "#     imputer=diffusion_imputer,\n",
    "#     batch_embedder=None,\n",
    "#     training_mean=training_mean,\n",
    "#     training_std=training_standard_deviation,\n",
    "#     old_sample=[],  # final_samples,\n",
    "#     # min_sequence_len=2,\n",
    "#     # max_sequence_len=2,\n",
    "#     sample_number=sample_number,\n",
    "#     scale=1,\n",
    "# )\n",
    "\n",
    "# final_samples = run_multiple_evaluations(\n",
    "#     dataloader=test_loader,\n",
    "#     imputer=diffusion_imputer,\n",
    "#     training_mean=training_mean,\n",
    "#     training_std=training_standard_deviation,\n",
    "#     sample_number=sample_number,\n",
    "#     data_embedder=None,\n",
    "#     old_sample=[],\n",
    "#     min_sequence_len=58,\n",
    "#     max_sequence_len=58,\n",
    "#     scale=1,\n",
    "#     num_gpus=2,\n",
    "#     verbose=True,\n",
    "#     show_max_diff=False,\n",
    "#     show_rmse=False,\n",
    "# )\n",
    "\n",
    "repetition_factor = 2\n",
    "final_samples = run_multiple_evaluations_with_repeats(\n",
    "    dataloader=test_loader,\n",
    "    imputer=diffusion_imputer,\n",
    "    training_mean=training_mean,\n",
    "    training_std=training_standard_deviation,\n",
    "    sample_number=sample_number,\n",
    "    repetition_factor=repetition_factor,\n",
    "    batch_size=batch_size,\n",
    "    data_embedder=None,\n",
    "    old_sample=[],\n",
    "    min_sequence_len=10,\n",
    "    max_sequence_len=10,\n",
    "    scale=1,\n",
    "    num_gpus=2,\n",
    "    verbose=True,\n",
    "    show_max_diff=False,\n",
    "    show_rmse=False,\n",
    ")\n",
    "# Set the start method to 'spawn' for the notebook environment\n",
    "\n",
    "# from custom_classes_and_functions import * #wrapper_run_multiple_evaluations, create_dataloader, load_diffusion_imputer, diffusion_imputation, ModelLoop, DiffusionEmbedding, TimeEmbedding\n",
    "\n",
    "# import numpy as np\n",
    "# os.environ['MKL_THREADING_LAYER'] = 'GNU'\n",
    "\n",
    "# import multiprocessing as mp\n",
    "# mp.set_start_method('spawn', force=True)\n",
    "\n",
    "# import os\n",
    "# import sys\n",
    "\n",
    "# # Ensure the current directory is in sys.path\n",
    "\n",
    "# # Define your dataloader\n",
    "# test_loader = create_dataloader(\n",
    "#     test_data_counterfactuals_tensor, test_data_counterfactuals_sequence_lengths, batch_size=10000, min_seq_length=2, max_seq_length=2)\n",
    "\n",
    "# diffusion_imputer = load_diffusion_imputer(\"diffusion_imputer_cancer_jul23.pt\")\n",
    "\n",
    "# # Assuming other necessary objects are defined (`diffusion_imputer`, `training_mean`, `training_standard_deviation`)\n",
    "# if __name__ == \"__main__\":\n",
    "#     diffusion_imputer =  diffusion_imputer # Initialize your imputer with appropriate parameters\n",
    "#     final_samples = wrapper_run_multiple_evaluations(\n",
    "#         dataloader=test_loader,\n",
    "#         imputer=diffusion_imputer,\n",
    "#         training_mean=training_mean,\n",
    "#         training_std=training_standard_deviation,\n",
    "#         sample_number=6,\n",
    "#         num_gpus=3\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the final samples\n",
    "torch.save(final_samples, \"final_samples_cancer_aug15.pt\")\n",
    "# final_samples = torch.load(\"final_samples_cancer_jul20.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final_samples[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate RMSE\n",
    "rmse, rmse_median = calculate_rmse(\n",
    "    final_samples, training_mean, training_standard_deviation\n",
    ")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"RMSE using median: {rmse_median}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def quantile_loss(target, forecast, q: float, eval_points) -> float:\n",
    "#     return 2 * torch.sum(\n",
    "#         torch.abs((forecast - target) * eval_points * ((target <= forecast) * 1.0 - q))\n",
    "#     )\n",
    "\n",
    "\n",
    "# def calc_denominator(target, eval_points):\n",
    "#     return torch.sum(torch.abs(target * eval_points))\n",
    "\n",
    "\n",
    "# def calc_quantile_CRPS(target, forecast, eval_points, mean_scaler, scaler):\n",
    "#     target = target * scaler + mean_scaler\n",
    "#     forecast = forecast * scaler + mean_scaler\n",
    "\n",
    "#     quantiles = np.arange(0.05, 1.0, 0.05)\n",
    "#     denom = calc_denominator(target, eval_points)\n",
    "#     CRPS = 0\n",
    "#     for i in range(len(quantiles)):\n",
    "#         q_pred = []\n",
    "#         for j in range(len(forecast)):\n",
    "#             q_pred.append(torch.quantile(forecast[j : j + 1], quantiles[i], dim=1))\n",
    "#         q_pred = torch.cat(q_pred, 0)\n",
    "#         q_loss = quantile_loss(target, q_pred, quantiles[i], eval_points)\n",
    "#         CRPS += q_loss / denom\n",
    "#     return CRPS.item() / len(quantiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_number = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_target = samples[0][1]  # input_data same for all samples (B, L, K)\n",
    "# all_generated_samples = torch.stack([samples[i][0] for i in range(sample_number)]).permute(1,0,2,3)  # (B, sample_num, L, K)\n",
    "# all_evalpoint = samples[0][2]  # mask same for all samples (B, L, K)\n",
    "# CRPS = calc_quantile_CRPS(all_target, all_generated_samples, all_evalpoint, training_mean, training_standard_deviation)\n",
    "# print(CRPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.core.interactiveshell import InteractiveShell\n",
    "# InteractiveShell.ast_node_interactivity = \"last_expr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L = test_data.shape[1]\n",
    "# K = test_data.shape[2]\n",
    "\n",
    "# dataind = 40  # Number of samples to visualize\n",
    "# start_time = 40\n",
    "# n_skip = 0  # Number of columns to skip\n",
    "\n",
    "# plt.rcParams[\"font.size\"] = 16\n",
    "# fig, axes = plt.subplots(nrows=dataind, ncols=K - n_skip, figsize=(24.0, 6 * dataind))\n",
    "\n",
    "# for i in range(dataind):\n",
    "#     for k in range(n_skip, K):  # Start from n_skip instead of 0\n",
    "#         df = pd.DataFrame({\n",
    "#             \"x\": np.arange(start_time, L),\n",
    "#             \"val\": denormed_data[i, start_time:, k],\n",
    "#             \"y\": eval_points[i, start_time:, k]\n",
    "#         })\n",
    "#         df = df[df.y != 0]\n",
    "#         df2 = pd.DataFrame({\n",
    "#             \"x\": np.arange(start_time, L),\n",
    "#             \"val\": denormed_data[i, start_time:, k],\n",
    "#             \"y\": given_points[i, start_time:, k]\n",
    "#         })\n",
    "#         df2 = df2[df2.y != 0]\n",
    "#         indices = df.x.astype(int).to_numpy()\n",
    "#         row = i\n",
    "#         col = k - n_skip  # Adjust column index for skipped columns\n",
    "\n",
    "#         axes[row][col].plot(range(start_time, L), quantiles_imp[2][i, start_time:, k], color='g', linestyle='solid', label='median')\n",
    "#         axes[row][col].fill_between(range(start_time, L), quantiles_imp[0][i, start_time:, k], quantiles_imp[4][i, start_time:, k], color='g', alpha=0.3)\n",
    "#         axes[row][col].plot(df.x, df.val, color='b', marker='o', linestyle='None')\n",
    "#         axes[row][col].plot(df.x, quantiles_imp[5][i, indices, k], color='r', linestyle='None', label='median', marker='x')\n",
    "#         axes[row][col].plot(df.x, quantiles_imp[0][i, indices, k], color='r', linestyle='None', marker=1)\n",
    "#         axes[row][col].plot(df.x, quantiles_imp[4][i, indices, k], color='r', linestyle='None', marker=1)\n",
    "\n",
    "#         if col == 0:  # Only label the first of the remaining columns\n",
    "#             axes[row][col].set_ylabel('Value')\n",
    "#         if row == dataind - 1:  # Only label the last row\n",
    "#             axes[row][col].set_xlabel('Time')\n",
    "\n",
    "# # Optional: Adjust the layout for better spacing\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
